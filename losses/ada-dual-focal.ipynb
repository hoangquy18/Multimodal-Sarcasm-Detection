{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4a4904",
   "metadata": {
    "papermill": {
     "duration": 0.027527,
     "end_time": "2024-10-10T01:36:07.502411",
     "exception": false,
     "start_time": "2024-10-10T01:36:07.474884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7ad383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:36:07.555768Z",
     "iopub.status.busy": "2024-10-10T01:36:07.554893Z",
     "iopub.status.idle": "2024-10-10T01:37:08.878261Z",
     "shell.execute_reply": "2024-10-10T01:37:08.877000Z"
    },
    "papermill": {
     "duration": 61.352721,
     "end_time": "2024-10-10T01:37:08.880842",
     "exception": false,
     "start_time": "2024-10-10T01:36:07.528121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting underthesea\r\n",
      "  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from underthesea) (8.1.7)\r\n",
      "Collecting python-crfsuite>=0.9.6 (from underthesea)\r\n",
      "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from underthesea) (3.2.4)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from underthesea) (4.66.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from underthesea) (2.32.3)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.4.2)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.2.2)\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from underthesea) (6.0.2)\r\n",
      "Collecting underthesea-core==1.0.4 (from underthesea)\r\n",
      "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->underthesea) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (2024.8.30)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.5.0)\r\n",
      "Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\r\n",
      "Successfully installed python-crfsuite-0.9.11 underthesea-6.8.4 underthesea-core-1.0.4\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\r\n",
      "Collecting einops\r\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: einops\r\n",
      "Successfully installed einops-0.8.0\r\n",
      "Collecting einops_exts\r\n",
      "  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\r\n",
      "Requirement already satisfied: einops>=0.4 in /opt/conda/lib/python3.10/site-packages (from einops_exts) (0.8.0)\r\n",
      "Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\r\n",
      "Installing collected packages: einops_exts\r\n",
      "Successfully installed einops_exts-0.0.4\r\n",
      "Collecting open_clip_torch\r\n",
      "  Downloading open_clip_torch-2.26.1-py3-none-any.whl.metadata (31 kB)\r\n",
      "Requirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2.4.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.19.0)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2024.5.15)\r\n",
      "Collecting ftfy (from open_clip_torch)\r\n",
      "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (4.66.4)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.25.1)\r\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (1.0.9)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (2024.6.1)\r\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.13)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm->open_clip_torch) (0.4.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (10.3.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->open_clip_torch) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2024.8.30)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\r\n",
      "Downloading open_clip_torch-2.26.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ftfy-6.2.3-py3-none-any.whl (43 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: ftfy, open_clip_torch\r\n",
      "Successfully installed ftfy-6.2.3 open_clip_torch-2.26.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install underthesea\n",
    "!pip install sentencepiece\n",
    "# !pip install transformers==4.44.2\n",
    "!pip install einops\n",
    "!pip install einops_exts\n",
    "# !pip install pandas\n",
    "# !pip install torch==2.4.0\n",
    "# !pip install torchvision==0.19.0 \n",
    "!pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69808755",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:08.942084Z",
     "iopub.status.busy": "2024-10-10T01:37:08.941287Z",
     "iopub.status.idle": "2024-10-10T01:37:18.480881Z",
     "shell.execute_reply": "2024-10-10T01:37:18.479900Z"
    },
    "papermill": {
     "duration": 9.573053,
     "end_time": "2024-10-10T01:37:18.483207",
     "exception": false,
     "start_time": "2024-10-10T01:37:08.910154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize,text_normalize\n",
    "import torch\n",
    "from transformers import AutoTokenizer,get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import re\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d626c3de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:18.542273Z",
     "iopub.status.busy": "2024-10-10T01:37:18.541767Z",
     "iopub.status.idle": "2024-10-10T01:37:31.703769Z",
     "shell.execute_reply": "2024-10-10T01:37:31.702936Z"
    },
    "papermill": {
     "duration": 13.194045,
     "end_time": "2024-10-10T01:37:31.706231",
     "exception": false,
     "start_time": "2024-10-10T01:37:18.512186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor,CLIPModel, CLIPProcessor, AutoTokenizer, AutoModel,RobertaModel, AutoImageProcessor\n",
    "from transformers import CLIPVisionModel, CLIPVisionConfig\n",
    "\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbbe0bf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:31.765828Z",
     "iopub.status.busy": "2024-10-10T01:37:31.765472Z",
     "iopub.status.idle": "2024-10-10T01:37:31.775739Z",
     "shell.execute_reply": "2024-10-10T01:37:31.775009Z"
    },
    "papermill": {
     "duration": 0.04169,
     "end_time": "2024-10-10T01:37:31.777660",
     "exception": false,
     "start_time": "2024-10-10T01:37:31.735970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sss = 18 + 19\n",
    "torch.manual_seed(sss)\n",
    "np.random.seed(sss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbafe65f",
   "metadata": {
    "papermill": {
     "duration": 0.028261,
     "end_time": "2024-10-10T01:37:31.834597",
     "exception": false,
     "start_time": "2024-10-10T01:37:31.806336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEXT PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb8e084",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:31.909822Z",
     "iopub.status.busy": "2024-10-10T01:37:31.909445Z",
     "iopub.status.idle": "2024-10-10T01:37:31.916168Z",
     "shell.execute_reply": "2024-10-10T01:37:31.915327Z"
    },
    "papermill": {
     "duration": 0.04543,
     "end_time": "2024-10-10T01:37:31.918012",
     "exception": false,
     "start_time": "2024-10-10T01:37:31.872582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_unicode(text):\n",
    "  char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
    "  charutf8 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
    "  char1252 = char1252.split('|')\n",
    "  charutf8 = charutf8.split('|')\n",
    "\n",
    "  dic = {}\n",
    "  for i in range(len(char1252)): dic[char1252[i]] = charutf8[i]\n",
    "  return re.sub(\n",
    "      r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "      lambda x: dic[x.group()], text\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6dc648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:31.977258Z",
     "iopub.status.busy": "2024-10-10T01:37:31.976910Z",
     "iopub.status.idle": "2024-10-10T01:37:32.006812Z",
     "shell.execute_reply": "2024-10-10T01:37:32.005905Z"
    },
    "papermill": {
     "duration": 0.061932,
     "end_time": "2024-10-10T01:37:32.008784",
     "exception": false,
     "start_time": "2024-10-10T01:37:31.946852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "class TextNormalize:\n",
    "    def __init__(self):\n",
    "        self.vowels_to_ids = {}\n",
    "        self.vowels_table = [\n",
    "            ['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a' ],\n",
    "            ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "            ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "            ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e' ],\n",
    "            ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "            ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i' ],\n",
    "            ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o' ],\n",
    "            ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'o'],\n",
    "            ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "            ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u' ],\n",
    "            ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "            ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y' ]\n",
    "        ]\n",
    "        pass\n",
    "\n",
    "    def createVowelsTable(self):\n",
    "        \"\"\"Create Vowels Table\"\"\"\n",
    "        for i in range(len(self.vowels_table)):\n",
    "            for j in range(len(self.vowels_table[i]) - 1):\n",
    "                self.vowels_to_ids[self.vowels_table[i][j]] = (i, j)\n",
    "\n",
    "    def IsValidVietnameseWord(self,word):\n",
    "        \"\"\"Nguyên âm chỉ có thể đứng chung với nguyên âm. Một từ không thể có 2 nguyên âm cách nhau bởi 1 phụ âm\"\"\"\n",
    "        chars = list(word)\n",
    "        #nguyen am\n",
    "        vowel_index = -1\n",
    "        for i in range(len(chars)):\n",
    "            idx_vowel_table = self.vowels_to_ids.get(chars[i],(-1,-1))[0]\n",
    "            if idx_vowel_table != -1:\n",
    "                if vowel_index == -1:\n",
    "                    vowel_index = i\n",
    "                else:\n",
    "                    if i - vowel_index != 1:\n",
    "                        return False\n",
    "                    vowel_index = i\n",
    "        return True\n",
    "\n",
    "    def WordStandardized(self,word):\n",
    "        \"\"\"Standardize Word\"\"\"\n",
    "        if not self.IsValidVietnameseWord(word):\n",
    "            return word\n",
    "\n",
    "        chars = list(word)\n",
    "        vowel_indexes = []\n",
    "\n",
    "        # tìm vị trí nguyên âm\n",
    "        qu_or_gi = False\n",
    "        thanh_dieu = 0\n",
    "        for i in range(len(chars)):\n",
    "            vowel_table_row, vowel_table_col = self.vowels_to_ids.get(chars[i],(-1,-1))\n",
    "            if vowel_table_row == -1 :\n",
    "                continue\n",
    "            # qu\n",
    "            if vowel_table_row == 9:\n",
    "                if i != 0 and chars[i-1] == 'q':\n",
    "                    chars[i] = 'u'\n",
    "                    qu_or_gi = True\n",
    "            # gi\n",
    "            elif vowel_table_row == 5:\n",
    "                if i != 0 and chars[i-1] == 'g':\n",
    "                    chars[i] = 'i'\n",
    "                    qu_or_gi = True\n",
    "\n",
    "            # có chứa thanh điệu\n",
    "            if vowel_table_col != 0:\n",
    "                thanh_dieu = vowel_table_col\n",
    "                chars[i] = self.vowels_table[vowel_table_row][0]\n",
    "\n",
    "            vowel_indexes.append(i)\n",
    "        # 1 nguyên âm\n",
    "        if len(vowel_indexes) == 1:\n",
    "            c = chars[vowel_indexes[0]]\n",
    "            chars[vowel_indexes[0]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
    "            return ''.join(chars)\n",
    "\n",
    "        for idx_vowel in vowel_indexes:\n",
    "            vowel_table_row, vowel_table_col = self.vowels_to_ids.get(chars[idx_vowel],(-1,-1))\n",
    "            #ê, ơ, ô\n",
    "            if vowel_table_row == 4 or vowel_table_row == 7 or vowel_table_row == 8:\n",
    "                c = chars[idx_vowel]\n",
    "                chars[idx_vowel] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
    "                return ''.join(chars)\n",
    "\n",
    "            # kiểm tra qu và gi, 2-3 nguyên âm thì nguyên âm thứ 2 chứa dấu\n",
    "            if qu_or_gi:\n",
    "                if len(vowel_indexes) == 2 or len(vowel_indexes) == 3:\n",
    "                    c = chars[vowel_indexes[1]]\n",
    "                    chars[vowel_indexes[1]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
    "                return ''.join(chars)\n",
    "\n",
    "            # 2 nguyên âm\n",
    "            if len(vowel_indexes) == 2:\n",
    "                # âm cuối là nguyên âm\n",
    "                if vowel_indexes[-1] == len(chars) - 1:\n",
    "                    c = chars[vowel_indexes[0]]\n",
    "                    chars[vowel_indexes[0]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
    "                else:\n",
    "                    c = chars[vowel_indexes[-1]]\n",
    "                    chars[vowel_indexes[-1]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
    "                return ''.join(chars)\n",
    "\n",
    "            elif len(vowel_indexes) == 3:\n",
    "                # âm cuối là nguyên âm\n",
    "                if vowel_indexes[-1] == len(chars) - 1:\n",
    "                    c = chars[vowel_indexes[1]]\n",
    "                    chars[vowel_indexes[1]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
    "                else:\n",
    "                    c = chars[vowel_indexes[-1]]\n",
    "                    chars[vowel_indexes[-1]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
    "                return ''.join(chars)\n",
    "\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def normalize(self,text):\n",
    "\n",
    "\n",
    "        #Chuyen sang viet thuong\n",
    "        text = text.lower()\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "\n",
    "        # Rút gọn từ kéo dài\n",
    "        text = re.sub(r'(\\W)\\1+',r'\\1',text)\n",
    "\n",
    "        # xóa các emoji dư thừa\n",
    "#         emoji_pattern = re.compile(\n",
    "#             \"[\"\n",
    "#             \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "#             \"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "#             \"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "#             \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "#             \"\\U00002500-\\U00002BEF\"  # Chinese char\n",
    "#             \"\\U00002702-\\U000027B0\"\n",
    "#             \"\\U00002702-\\U000027B0\"\n",
    "#             \"\\U000024C2-\\U0001F251\"\n",
    "#             \"\\U0001f926-\\U0001f937\"\n",
    "#             \"\\U00010000-\\U0010ffff\"\n",
    "#             \"]+\",\n",
    "#             flags=re.UNICODE,\n",
    "#         )\n",
    "#         text = emoji_pattern.sub(r'',text) # no emoji\n",
    "        # remove hastag\n",
    "        text = re.sub(\"(@[A-Za-z0-9]+)|(#[0-9A-Za-z]+)\",\"\", text)\n",
    "\n",
    "        # xóa space d\n",
    "        text = re.sub(r\"( )\\1+\",r'\\1',text)\n",
    "#         text = re.sub(r\"[:)^@!`~%;?(\\+\\-\\'\\\"]+\",r'',text)\n",
    "        text = text.replace(\"“\",\"\")\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = \" \".join(text.split())\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58378b7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:32.068774Z",
     "iopub.status.busy": "2024-10-10T01:37:32.067906Z",
     "iopub.status.idle": "2024-10-10T01:37:33.069082Z",
     "shell.execute_reply": "2024-10-10T01:37:33.067944Z"
    },
    "papermill": {
     "duration": 1.033953,
     "end_time": "2024-10-10T01:37:33.071486",
     "exception": false,
     "start_time": "2024-10-10T01:37:32.037533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04a80774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:33.131515Z",
     "iopub.status.busy": "2024-10-10T01:37:33.131144Z",
     "iopub.status.idle": "2024-10-10T01:37:33.337626Z",
     "shell.execute_reply": "2024-10-10T01:37:33.336819Z"
    },
    "papermill": {
     "duration": 0.239343,
     "end_time": "2024-10-10T01:37:33.339971",
     "exception": false,
     "start_time": "2024-10-10T01:37:33.100628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "data = pd.read_json('/kaggle/input/train-dsc2024/vimmsd-train.json',orient = 'index').reset_index()\n",
    "# data = data[data['label'] != 'not-sarcasm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0fc3a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:33.399632Z",
     "iopub.status.busy": "2024-10-10T01:37:33.398933Z",
     "iopub.status.idle": "2024-10-10T01:37:33.432552Z",
     "shell.execute_reply": "2024-10-10T01:37:33.431683Z"
    },
    "papermill": {
     "duration": 0.065993,
     "end_time": "2024-10-10T01:37:33.435162",
     "exception": false,
     "start_time": "2024-10-10T01:37:33.369169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, dev_data = train_test_split(data, test_size=0.1, stratify=data['label'], random_state = sss) \n",
    "train_data = train_data.reset_index() \n",
    "dev_data = dev_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f1d7ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:33.496305Z",
     "iopub.status.busy": "2024-10-10T01:37:33.495877Z",
     "iopub.status.idle": "2024-10-10T01:37:33.500833Z",
     "shell.execute_reply": "2024-10-10T01:37:33.499789Z"
    },
    "papermill": {
     "duration": 0.038637,
     "end_time": "2024-10-10T01:37:33.503037",
     "exception": false,
     "start_time": "2024-10-10T01:37:33.464400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalize_class = TextNormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ff0f901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:33.564140Z",
     "iopub.status.busy": "2024-10-10T01:37:33.563784Z",
     "iopub.status.idle": "2024-10-10T01:37:41.932836Z",
     "shell.execute_reply": "2024-10-10T01:37:41.932039Z"
    },
    "papermill": {
     "duration": 8.401824,
     "end_time": "2024-10-10T01:37:41.935136",
     "exception": false,
     "start_time": "2024-10-10T01:37:33.533312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data['caption'] = train_data['caption'].apply(lambda x: text_normalize(normalize_class.normalize(x)))\n",
    "dev_data['caption'] = dev_data['caption'].apply(lambda x: text_normalize(normalize_class.normalize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed1126b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:41.993828Z",
     "iopub.status.busy": "2024-10-10T01:37:41.993489Z",
     "iopub.status.idle": "2024-10-10T01:37:42.012273Z",
     "shell.execute_reply": "2024-10-10T01:37:42.011314Z"
    },
    "papermill": {
     "duration": 0.050315,
     "end_time": "2024-10-10T01:37:42.014425",
     "exception": false,
     "start_time": "2024-10-10T01:37:41.964110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9090</td>\n",
       "      <td>9090</td>\n",
       "      <td>110f61e365393bbd6234d89dec7adde94784b22ebf45d2...</td>\n",
       "      <td>hạt dẻ cười = )</td>\n",
       "      <td>multi-sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>898</td>\n",
       "      <td>898</td>\n",
       "      <td>aba83dfcaf0b3b515ce2d9663a062538ca257a6f99c8d4...</td>\n",
       "      <td>nếu cây ô-rô ( ilex aquifolium ) phát hiện lá ...</td>\n",
       "      <td>not-sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8124</td>\n",
       "      <td>8124</td>\n",
       "      <td>1d55a3f21394857c538ee4df518ee03254570a914243a4...</td>\n",
       "      <td>_f33 - bức xúc anh em xã hội quanh co đổ tội c...</td>\n",
       "      <td>multi-sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>3ca63743fa3c553b31e6dbde5884196019ae0d22899dd4...</td>\n",
       "      <td>nó lại hợp lý 🤣</td>\n",
       "      <td>multi-sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9652</td>\n",
       "      <td>9652</td>\n",
       "      <td>ce429a6a3e9ee68c6df95e9cda6619ebb22c217c37f4ac...</td>\n",
       "      <td>tính dụ fan arsenal xem đến hết trận hay gì = )</td>\n",
       "      <td>multi-sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9719</th>\n",
       "      <td>8305</td>\n",
       "      <td>8305</td>\n",
       "      <td>62c96968a00ddb87ef8e60505d8d1182084f4057781483...</td>\n",
       "      <td>🌟 một từ thôi : hay !</td>\n",
       "      <td>not-sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9720</th>\n",
       "      <td>10529</td>\n",
       "      <td>10529</td>\n",
       "      <td>d2b63d2ab3391166b4bfb71b7fbd20be617940bbe188be...</td>\n",
       "      <td>lâm minh cho biết sau khi bình tĩnh , cô không...</td>\n",
       "      <td>not-sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9721</th>\n",
       "      <td>4738</td>\n",
       "      <td>4738</td>\n",
       "      <td>94c6adcbd21f4dfc744ac059a61476a03ff775672d2e82...</td>\n",
       "      <td>bắt gặp một rapper cộm cán trong làng rap úc ....</td>\n",
       "      <td>multi-sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9722</th>\n",
       "      <td>6795</td>\n",
       "      <td>6795</td>\n",
       "      <td>618f7a296eedf5bd1ca491dbdcb465675648735bd32c2a...</td>\n",
       "      <td>đơn vị sản xuất series đu đêm metub cùng hoa h...</td>\n",
       "      <td>not-sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9723</th>\n",
       "      <td>10589</td>\n",
       "      <td>10589</td>\n",
       "      <td>6410a27eeb2d3f9f6f50ebab3666bac91012f25747fdcc...</td>\n",
       "      <td>giới mộ điệu dạo này có trend gì mới vị cà ! c...</td>\n",
       "      <td>not-sarcasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9724 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      level_0  index                                              image  \\\n",
       "0        9090   9090  110f61e365393bbd6234d89dec7adde94784b22ebf45d2...   \n",
       "1         898    898  aba83dfcaf0b3b515ce2d9663a062538ca257a6f99c8d4...   \n",
       "2        8124   8124  1d55a3f21394857c538ee4df518ee03254570a914243a4...   \n",
       "3         210    210  3ca63743fa3c553b31e6dbde5884196019ae0d22899dd4...   \n",
       "4        9652   9652  ce429a6a3e9ee68c6df95e9cda6619ebb22c217c37f4ac...   \n",
       "...       ...    ...                                                ...   \n",
       "9719     8305   8305  62c96968a00ddb87ef8e60505d8d1182084f4057781483...   \n",
       "9720    10529  10529  d2b63d2ab3391166b4bfb71b7fbd20be617940bbe188be...   \n",
       "9721     4738   4738  94c6adcbd21f4dfc744ac059a61476a03ff775672d2e82...   \n",
       "9722     6795   6795  618f7a296eedf5bd1ca491dbdcb465675648735bd32c2a...   \n",
       "9723    10589  10589  6410a27eeb2d3f9f6f50ebab3666bac91012f25747fdcc...   \n",
       "\n",
       "                                                caption          label  \n",
       "0                                       hạt dẻ cười = )  multi-sarcasm  \n",
       "1     nếu cây ô-rô ( ilex aquifolium ) phát hiện lá ...    not-sarcasm  \n",
       "2     _f33 - bức xúc anh em xã hội quanh co đổ tội c...  multi-sarcasm  \n",
       "3                                       nó lại hợp lý 🤣  multi-sarcasm  \n",
       "4       tính dụ fan arsenal xem đến hết trận hay gì = )  multi-sarcasm  \n",
       "...                                                 ...            ...  \n",
       "9719                              🌟 một từ thôi : hay !    not-sarcasm  \n",
       "9720  lâm minh cho biết sau khi bình tĩnh , cô không...    not-sarcasm  \n",
       "9721  bắt gặp một rapper cộm cán trong làng rap úc ....  multi-sarcasm  \n",
       "9722  đơn vị sản xuất series đu đêm metub cùng hoa h...    not-sarcasm  \n",
       "9723  giới mộ điệu dạo này có trend gì mới vị cà ! c...    not-sarcasm  \n",
       "\n",
       "[9724 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d9d7016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:42.075637Z",
     "iopub.status.busy": "2024-10-10T01:37:42.075348Z",
     "iopub.status.idle": "2024-10-10T01:37:42.084740Z",
     "shell.execute_reply": "2024-10-10T01:37:42.083984Z"
    },
    "papermill": {
     "duration": 0.041196,
     "end_time": "2024-10-10T01:37:42.086637",
     "exception": false,
     "start_time": "2024-10-10T01:37:42.045441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViMMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, img_folder, vision_processor, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.img_folder = img_folder\n",
    "        self.vision_processor = vision_processor\n",
    "        self.labels = ['not-sarcasm', 'image-sarcasm','text-sarcasm','multi-sarcasm']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.loc[idx,'caption']\n",
    "        img_name = self.data.loc[idx,'image']\n",
    "        img_file = os.path.join(self.img_folder,img_name)\n",
    "        label = self.data.loc[idx,'label']\n",
    "        label = self.labels.index(label)\n",
    "        \n",
    "        tokenizer_seq_dict = self.tokenizer(\"<image>\", text, max_length=self.max_len,truncation=True,padding='max_length', return_length=True, return_tensors='pt')\n",
    "        input_ids = tokenizer_seq_dict['input_ids'][0]\n",
    "        attention_mask = tokenizer_seq_dict['attention_mask']\n",
    "\n",
    "        # feature_file = os.path.join('dino_large_features/dino_large_features/' + img_name.split('.')[0] + '.npy')\n",
    "        # features = np.load(feature_file, allow_pickle=True)[()]\n",
    "\n",
    "        # pixel_values = features['pixel_values']\n",
    "\n",
    "        image = Image.open(img_file)        \n",
    "        pixel_values = self.vision_processor(image).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        return input_ids, attention_mask, pixel_values, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a390b396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:42.145718Z",
     "iopub.status.busy": "2024-10-10T01:37:42.145435Z",
     "iopub.status.idle": "2024-10-10T01:37:42.149395Z",
     "shell.execute_reply": "2024-10-10T01:37:42.148523Z"
    },
    "papermill": {
     "duration": 0.035772,
     "end_time": "2024-10-10T01:37:42.151339",
     "exception": false,
     "start_time": "2024-10-10T01:37:42.115567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRETRAINED_PATH = \"xlm-roberta-base\"\n",
    "VISION_PRETRAINED_PATH = 'facebook/dinov2-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc7782b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:42.210333Z",
     "iopub.status.busy": "2024-10-10T01:37:42.210061Z",
     "iopub.status.idle": "2024-10-10T01:37:42.451194Z",
     "shell.execute_reply": "2024-10-10T01:37:42.450303Z"
    },
    "papermill": {
     "duration": 0.27483,
     "end_time": "2024-10-10T01:37:42.455089",
     "exception": false,
     "start_time": "2024-10-10T01:37:42.180259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b038bdc219d406392cce74c3bde929c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vision_processor = AutoImageProcessor.from_pretrained(VISION_PRETRAINED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa633eee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:42.557273Z",
     "iopub.status.busy": "2024-10-10T01:37:42.556842Z",
     "iopub.status.idle": "2024-10-10T01:37:49.499350Z",
     "shell.execute_reply": "2024-10-10T01:37:49.498317Z"
    },
    "papermill": {
     "duration": 7.016599,
     "end_time": "2024-10-10T01:37:49.501841",
     "exception": false,
     "start_time": "2024-10-10T01:37:42.485242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 351M/351M [00:03<00:00, 108MiB/s]\n"
     ]
    }
   ],
   "source": [
    "_, _, vision_processor = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-16',\n",
    "    pretrained='openai'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7a7c9fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:49.568068Z",
     "iopub.status.busy": "2024-10-10T01:37:49.567698Z",
     "iopub.status.idle": "2024-10-10T01:37:52.835746Z",
     "shell.execute_reply": "2024-10-10T01:37:52.834701Z"
    },
    "papermill": {
     "duration": 3.303414,
     "end_time": "2024-10-10T01:37:52.838329",
     "exception": false,
     "start_time": "2024-10-10T01:37:49.534915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f9303cdc74473491ff8e76c329c95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ec4679e6bc43cfbadb330fac9b0196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2e5b7c96294547bb2465cda278a556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417696d0c32f44f3b4c526f819869986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_PATH)\n",
    "tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": [\"<image>\"]}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd5f44b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:52.906885Z",
     "iopub.status.busy": "2024-10-10T01:37:52.906509Z",
     "iopub.status.idle": "2024-10-10T01:37:52.911277Z",
     "shell.execute_reply": "2024-10-10T01:37:52.910425Z"
    },
    "papermill": {
     "duration": 0.041425,
     "end_time": "2024-10-10T01:37:52.913254",
     "exception": false,
     "start_time": "2024-10-10T01:37:52.871829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = ViMMDataset(train_data, tokenizer, '/kaggle/input/train-dsc2024/training-images/train-images', vision_processor, max_len = 200)\n",
    "dev_set = ViMMDataset(dev_data, tokenizer, '/kaggle/input/train-dsc2024/training-images/train-images', vision_processor, max_len = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8e168c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:52.980815Z",
     "iopub.status.busy": "2024-10-10T01:37:52.980144Z",
     "iopub.status.idle": "2024-10-10T01:37:52.984400Z",
     "shell.execute_reply": "2024-10-10T01:37:52.983553Z"
    },
    "papermill": {
     "duration": 0.039975,
     "end_time": "2024-10-10T01:37:52.986273",
     "exception": false,
     "start_time": "2024-10-10T01:37:52.946298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(len(train_set)):\n",
    "#     try:\n",
    "#         t = train_set[i]\n",
    "#     except:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbd4f1ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:53.053232Z",
     "iopub.status.busy": "2024-10-10T01:37:53.052407Z",
     "iopub.status.idle": "2024-10-10T01:37:53.116208Z",
     "shell.execute_reply": "2024-10-10T01:37:53.115278Z"
    },
    "papermill": {
     "duration": 0.09986,
     "end_time": "2024-10-10T01:37:53.118570",
     "exception": false,
     "start_time": "2024-10-10T01:37:53.018710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <image> </s> </s> ▁hạt ▁d ẻ ▁cười ▁= ▁) </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n"
     ]
    }
   ],
   "source": [
    "aa = 0\n",
    "for i in train_set[aa][0]:\n",
    "    print(tokenizer.convert_ids_to_tokens(i.item()),end=' ' )\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "267ab3a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:37:53.188784Z",
     "iopub.status.busy": "2024-10-10T01:37:53.187877Z",
     "iopub.status.idle": "2024-10-10T01:41:00.851671Z",
     "shell.execute_reply": "2024-10-10T01:41:00.850310Z"
    },
    "papermill": {
     "duration": 187.700866,
     "end_time": "2024-10-10T01:41:00.854365",
     "exception": false,
     "start_time": "2024-10-10T01:37:53.153499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9724/9724 [03:07<00:00, 51.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "y = []\n",
    "for i in tqdm(range(len(train_set))):\n",
    "  y.append(train_set[i][-1].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc51e305",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:01.174136Z",
     "iopub.status.busy": "2024-10-10T01:41:01.173450Z",
     "iopub.status.idle": "2024-10-10T01:41:01.180189Z",
     "shell.execute_reply": "2024-10-10T01:41:01.179378Z"
    },
    "papermill": {
     "duration": 0.169181,
     "end_time": "2024-10-10T01:41:01.182185",
     "exception": false,
     "start_time": "2024-10-10T01:41:01.013004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 3, 3, 3, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 3, 0, 0, 3, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "769820ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:01.500823Z",
     "iopub.status.busy": "2024-10-10T01:41:01.500028Z",
     "iopub.status.idle": "2024-10-10T01:41:01.508022Z",
     "shell.execute_reply": "2024-10-10T01:41:01.507042Z"
    },
    "papermill": {
     "duration": 0.169398,
     "end_time": "2024-10-10T01:41:01.510572",
     "exception": false,
     "start_time": "2024-10-10T01:41:01.341174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5456  398   69 3801] [0.00018328 0.00251256 0.01449275 0.00026309]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "counts = np.bincount(y)\n",
    "labels_weights = 1. / counts\n",
    "# labels_weights[-1] = labels_weights[-1]\n",
    "weights = labels_weights[y]\n",
    "print(counts,labels_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4fb5988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:01.840892Z",
     "iopub.status.busy": "2024-10-10T01:41:01.840413Z",
     "iopub.status.idle": "2024-10-10T01:41:01.847746Z",
     "shell.execute_reply": "2024-10-10T01:41:01.846315Z"
    },
    "papermill": {
     "duration": 0.181741,
     "end_time": "2024-10-10T01:41:01.850491",
     "exception": false,
     "start_time": "2024-10-10T01:41:01.668750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 4, shuffle = True)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_set, batch_size = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b84dfa",
   "metadata": {
    "papermill": {
     "duration": 0.158154,
     "end_time": "2024-10-10T01:41:02.179084",
     "exception": false,
     "start_time": "2024-10-10T01:41:02.020930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6aa2e984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:02.499706Z",
     "iopub.status.busy": "2024-10-10T01:41:02.499003Z",
     "iopub.status.idle": "2024-10-10T01:41:02.504120Z",
     "shell.execute_reply": "2024-10-10T01:41:02.503195Z"
    },
    "papermill": {
     "duration": 0.167211,
     "end_time": "2024-10-10T01:41:02.506083",
     "exception": false,
     "start_time": "2024-10-10T01:41:02.338872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49dfd42b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:02.825425Z",
     "iopub.status.busy": "2024-10-10T01:41:02.824580Z",
     "iopub.status.idle": "2024-10-10T01:41:02.833935Z",
     "shell.execute_reply": "2024-10-10T01:41:02.833215Z"
    },
    "papermill": {
     "duration": 0.171845,
     "end_time": "2024-10-10T01:41:02.835853",
     "exception": false,
     "start_time": "2024-10-10T01:41:02.664008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from einops_exts import rearrange_many\n",
    "from torch import einsum, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1c04804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:03.191112Z",
     "iopub.status.busy": "2024-10-10T01:41:03.190346Z",
     "iopub.status.idle": "2024-10-10T01:41:03.196237Z",
     "shell.execute_reply": "2024-10-10T01:41:03.195418Z"
    },
    "papermill": {
     "duration": 0.166149,
     "end_time": "2024-10-10T01:41:03.198452",
     "exception": false,
     "start_time": "2024-10-10T01:41:03.032303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22/3556953468.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=True if mode == 'FP16' else False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "mode = 'FP16'\n",
    "os.environ[\"TORCH_CUDNN_V8_API_ENABLED\"] = \"1\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True if mode == 'TF32' else False\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True if mode == 'FP16' else False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "def24853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:03.515564Z",
     "iopub.status.busy": "2024-10-10T01:41:03.514803Z",
     "iopub.status.idle": "2024-10-10T01:41:03.520607Z",
     "shell.execute_reply": "2024-10-10T01:41:03.519688Z"
    },
    "papermill": {
     "duration": 0.166366,
     "end_time": "2024-10-10T01:41:03.522523",
     "exception": false,
     "start_time": "2024-10-10T01:41:03.356157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def FeedForward(dim, mult=4):\n",
    "    inner_dim = int(dim * mult)\n",
    "    return nn.Sequential(\n",
    "        nn.LayerNorm(dim),\n",
    "        nn.Linear(dim, inner_dim, bias=False),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(inner_dim, dim, bias=False),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d17aff3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:03.840199Z",
     "iopub.status.busy": "2024-10-10T01:41:03.839824Z",
     "iopub.status.idle": "2024-10-10T01:41:03.844380Z",
     "shell.execute_reply": "2024-10-10T01:41:03.843470Z"
    },
    "papermill": {
     "duration": 0.166501,
     "end_time": "2024-10-10T01:41:03.846451",
     "exception": false,
     "start_time": "2024-10-10T01:41:03.679950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e682e80f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:04.160504Z",
     "iopub.status.busy": "2024-10-10T01:41:04.159694Z",
     "iopub.status.idle": "2024-10-10T01:41:04.170299Z",
     "shell.execute_reply": "2024-10-10T01:41:04.169410Z"
    },
    "papermill": {
     "duration": 0.169408,
     "end_time": "2024-10-10T01:41:04.172140",
     "exception": false,
     "start_time": "2024-10-10T01:41:04.002732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PerceiverAttention(nn.Module):\n",
    "    def __init__(self, *, dim, dim_head=64, heads=8):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.norm_media = nn.LayerNorm(dim)\n",
    "        self.norm_latents = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x, latents):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): image features\n",
    "                shape (b, T, n1, D)\n",
    "            latent (torch.Tensor): latent features\n",
    "                shape (b, T, n2, D)\n",
    "        \"\"\"\n",
    "        x = self.norm_media(x)\n",
    "        latents = self.norm_latents(latents)\n",
    "\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(latents)\n",
    "        kv_input = torch.cat((x, latents), dim=-2)\n",
    "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
    "        q, k, v = rearrange_many((q, k, v), \"b t n (h d) -> b h t n d\", h=h)\n",
    "        q = q * self.scale\n",
    "\n",
    "        # attention\n",
    "        sim = einsum(\"... i d, ... j d  -> ... i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"... i j, ... j d -> ... i d\", attn, v)\n",
    "        out = rearrange(out, \"b h t n d -> b t n (h d)\", h=h)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae62dc26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:04.486949Z",
     "iopub.status.busy": "2024-10-10T01:41:04.486047Z",
     "iopub.status.idle": "2024-10-10T01:41:04.497833Z",
     "shell.execute_reply": "2024-10-10T01:41:04.497025Z"
    },
    "papermill": {
     "duration": 0.171259,
     "end_time": "2024-10-10T01:41:04.499900",
     "exception": false,
     "start_time": "2024-10-10T01:41:04.328641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth=6,\n",
    "        dim_head=64,\n",
    "        heads=8,\n",
    "        num_latents=64,\n",
    "        max_num_media=None,\n",
    "        max_num_frames=None,\n",
    "        ff_mult=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
    "        self.frame_embs = (\n",
    "            nn.Parameter(torch.randn(max_num_frames, dim))\n",
    "            if exists(max_num_frames)\n",
    "            else None\n",
    "        )\n",
    "        self.media_time_embs = (\n",
    "            nn.Parameter(torch.randn(max_num_media, 1, dim))\n",
    "            if exists(max_num_media)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
    "                        FeedForward(dim=dim, mult=ff_mult),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): image features\n",
    "                shape (b, T, F, v, D)\n",
    "        Returns:\n",
    "            shape (b, T, n, D) where n is self.num_latents\n",
    "        \"\"\"\n",
    "        b, T, F, v = x.shape[:4]\n",
    "\n",
    "        # frame and media time embeddings\n",
    "        if exists(self.frame_embs):\n",
    "            frame_embs = repeat(self.frame_embs[:F], \"F d -> b T F v d\", b=b, T=T, v=v)\n",
    "            x = x + frame_embs\n",
    "        x = rearrange(\n",
    "            x, \"b T F v d -> b T (F v) d\"\n",
    "        )  # flatten the frame and spatial dimensions\n",
    "        if exists(self.media_time_embs):\n",
    "            x = x + self.media_time_embs[:T]\n",
    "\n",
    "        # blocks\n",
    "        latents = repeat(self.latents, \"n d -> b T n d\", b=b, T=T)\n",
    "        for attn, ff in self.layers:\n",
    "            latents = attn(x, latents) + latents\n",
    "            latents = ff(latents) + latents\n",
    "        return self.norm(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ea4aebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:04.827846Z",
     "iopub.status.busy": "2024-10-10T01:41:04.826956Z",
     "iopub.status.idle": "2024-10-10T01:41:04.843674Z",
     "shell.execute_reply": "2024-10-10T01:41:04.842907Z"
    },
    "papermill": {
     "duration": 0.180046,
     "end_time": "2024-10-10T01:41:04.845601",
     "exception": false,
     "start_time": "2024-10-10T01:41:04.665555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gated cross attention\n",
    "class MaskedCrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_visual,\n",
    "        dim_head=64,\n",
    "        heads=8,\n",
    "        only_attend_immediate_media=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(dim_visual, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
    "\n",
    "        # whether for text to only attend to immediate preceding image, or all previous images\n",
    "        self.only_attend_immediate_media = only_attend_immediate_media\n",
    "\n",
    "    def forward(self, x, media, media_locations=None, use_cached_media=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): text features\n",
    "                shape (B, T_txt, D_txt)\n",
    "            media (torch.Tensor): image features\n",
    "                shape (B, T_img, n, D_img) where n is the dim of the latents\n",
    "            media_locations: boolean mask identifying the media tokens in x\n",
    "                shape (B, T_txt)\n",
    "            use_cached_media: bool\n",
    "                If true, treat all of x as if they occur after the last media\n",
    "                registered in media_locations. T_txt does not need to exactly\n",
    "                equal media_locations.shape[1] in this case\n",
    "        \"\"\"\n",
    "\n",
    "        if not use_cached_media:\n",
    "            assert (\n",
    "                media_locations.shape[1] == x.shape[1]\n",
    "            ), f\"media_location.shape is {media_locations.shape} but x.shape is {x.shape}\"\n",
    "\n",
    "        T_txt = x.shape[1]\n",
    "        _, T_img, n = media.shape[:3]\n",
    "        h = self.heads\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        media = rearrange(media, \"b t n d -> b (t n) d\")\n",
    "\n",
    "        k, v = self.to_kv(media).chunk(2, dim=-1)\n",
    "        q, k, v = rearrange_many((q, k, v), \"b n (h d) -> b h n d\", h=h)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"... i d, ... j d -> ... i j\", q, k)\n",
    "\n",
    "        if exists(media_locations):\n",
    "            media_time = torch.arange(T_img, device=x.device) + 1\n",
    "\n",
    "            if use_cached_media:\n",
    "                # text time is set to the last cached media location\n",
    "                text_time = repeat(\n",
    "                    torch.count_nonzero(media_locations, dim=1),\n",
    "                    \"b -> b i\",\n",
    "                    i=T_txt,\n",
    "                )\n",
    "            else:\n",
    "                # at each boolean of True, increment the time counter (relative to media time)\n",
    "                text_time = media_locations.cumsum(dim=-1)\n",
    "\n",
    "            # text time must equal media time if only attending to most immediate image\n",
    "            # otherwise, as long as text time is greater than media time (if attending to all previous images / media)\n",
    "            mask_op = torch.eq if self.only_attend_immediate_media else torch.ge\n",
    "\n",
    "            text_to_media_mask = mask_op(\n",
    "                rearrange(text_time, \"b i -> b 1 i 1\"),\n",
    "                repeat(media_time, \"j -> 1 1 1 (j n)\", n=n),\n",
    "            )\n",
    "            sim = sim.masked_fill(~text_to_media_mask, -torch.finfo(sim.dtype).max)\n",
    "\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        if exists(media_locations) and self.only_attend_immediate_media:\n",
    "            # any text without a preceding media needs to have attention zeroed out\n",
    "            text_without_media_mask = text_time == 0\n",
    "            text_without_media_mask = rearrange(\n",
    "                text_without_media_mask, \"b i -> b 1 i 1\"\n",
    "            )\n",
    "            attn = attn.masked_fill(text_without_media_mask, 0.0)\n",
    "\n",
    "        out = einsum(\"... i j, ... j d -> ... i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d7f99cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:05.163756Z",
     "iopub.status.busy": "2024-10-10T01:41:05.162912Z",
     "iopub.status.idle": "2024-10-10T01:41:05.171391Z",
     "shell.execute_reply": "2024-10-10T01:41:05.170553Z"
    },
    "papermill": {
     "duration": 0.16907,
     "end_time": "2024-10-10T01:41:05.173343",
     "exception": false,
     "start_time": "2024-10-10T01:41:05.004273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GatedCrossAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_visual,\n",
    "        dim_head=64,\n",
    "        heads=8,\n",
    "        ff_mult=4,\n",
    "        only_attend_immediate_media=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn = MaskedCrossAttention(\n",
    "            dim=dim,\n",
    "            dim_visual=dim_visual,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            only_attend_immediate_media=only_attend_immediate_media,\n",
    "        )\n",
    "        self.attn_gate = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        self.ff = FeedForward(dim, mult=ff_mult)\n",
    "        self.ff_gate = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        media,\n",
    "        media_locations=None,\n",
    "        use_cached_media=False,\n",
    "    ):\n",
    "        x = (\n",
    "            self.attn(\n",
    "                x,\n",
    "                media,\n",
    "                media_locations=media_locations,\n",
    "                use_cached_media=use_cached_media,\n",
    "            )\n",
    "            * self.attn_gate.tanh()\n",
    "            + x\n",
    "        )\n",
    "        x = self.ff(x) * self.ff_gate.tanh() + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19382948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:05.486988Z",
     "iopub.status.busy": "2024-10-10T01:41:05.486608Z",
     "iopub.status.idle": "2024-10-10T01:41:05.492523Z",
     "shell.execute_reply": "2024-10-10T01:41:05.491623Z"
    },
    "papermill": {
     "duration": 0.165296,
     "end_time": "2024-10-10T01:41:05.494553",
     "exception": false,
     "start_time": "2024-10-10T01:41:05.329257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_with_stopping_condition(\n",
    "    module, apply_fn, apply_condition=None, stopping_condition=None, **other_args\n",
    "):\n",
    "    if stopping_condition(module):\n",
    "        return\n",
    "    if apply_condition(module):\n",
    "        apply_fn(module, **other_args)\n",
    "    for child in module.children():\n",
    "        apply_with_stopping_condition(\n",
    "            child,\n",
    "            apply_fn,\n",
    "            apply_condition=apply_condition,\n",
    "            stopping_condition=stopping_condition,\n",
    "            **other_args\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccd65854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:05.809816Z",
     "iopub.status.busy": "2024-10-10T01:41:05.809447Z",
     "iopub.status.idle": "2024-10-10T01:41:05.817373Z",
     "shell.execute_reply": "2024-10-10T01:41:05.816455Z"
    },
    "papermill": {
     "duration": 0.168331,
     "end_time": "2024-10-10T01:41:05.819365",
     "exception": false,
     "start_time": "2024-10-10T01:41:05.651034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extend_instance(obj, mixin):\n",
    "    \"\"\"Apply mixins to a class instance after creation\"\"\"\n",
    "    base_cls = obj.__class__\n",
    "    base_cls_name = obj.__class__.__name__\n",
    "    obj.__class__ = type(\n",
    "        base_cls_name, (mixin, base_cls), {}\n",
    "    )  # mixin needs to go first for our forward() logic to work\n",
    "\n",
    "\n",
    "def getattr_recursive(obj, att):\n",
    "    \"\"\"\n",
    "    Return nested attribute of obj\n",
    "    Example: getattr_recursive(obj, 'a.b.c') is equivalent to obj.a.b.c\n",
    "    \"\"\"\n",
    "    if att == \"\":\n",
    "        return obj\n",
    "    i = att.find(\".\")\n",
    "    if i < 0:\n",
    "        return getattr(obj, att)\n",
    "    else:\n",
    "        return getattr_recursive(getattr(obj, att[:i]), att[i + 1 :])\n",
    "\n",
    "\n",
    "def setattr_recursive(obj, att, val):\n",
    "    \"\"\"\n",
    "    Set nested attribute of obj\n",
    "    Example: setattr_recursive(obj, 'a.b.c', val) is equivalent to obj.a.b.c = val\n",
    "    \"\"\"\n",
    "    if \".\" in att:\n",
    "        obj = getattr_recursive(obj, \".\".join(att.split(\".\")[:-1]))\n",
    "    setattr(obj, att.split(\".\")[-1], val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "406d7d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:06.136951Z",
     "iopub.status.busy": "2024-10-10T01:41:06.136033Z",
     "iopub.status.idle": "2024-10-10T01:41:06.146905Z",
     "shell.execute_reply": "2024-10-10T01:41:06.146030Z"
    },
    "papermill": {
     "duration": 0.172896,
     "end_time": "2024-10-10T01:41:06.148942",
     "exception": false,
     "start_time": "2024-10-10T01:41:05.976046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FlamingoLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    FlamingoLayer is a wrapper around the GatedCrossAttentionBlock and DecoderLayer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, gated_cross_attn_layer, encoder_layer, gradient_checkpointing=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gated_cross_attn_layer = gated_cross_attn_layer\n",
    "        self.vis_x = None\n",
    "        self.encoder_layer = encoder_layer\n",
    "        self.media_locations = None\n",
    "        if self.gated_cross_attn_layer is not None:\n",
    "            self.gated_cross_attn_layer._use_gradient_checkpointing = (\n",
    "                gradient_checkpointing\n",
    "            )\n",
    "\n",
    "    def is_conditioned(self) -> bool:\n",
    "        \"\"\"Check whether the layer is conditioned.\"\"\"\n",
    "        return self.vis_x is not None and self.media_locations is not None\n",
    "\n",
    "    # Used this great idea from this implementation of Flamingo (https://github.com/dhansmair/flamingo-mini/)\n",
    "    def condition_vis_x(self, vis_x):\n",
    "        self.vis_x = vis_x\n",
    "\n",
    "    def condition_media_locations(self, media_locations):\n",
    "        self.media_locations = media_locations\n",
    "\n",
    "    def condition_use_cached_media(self, use_cached_media):\n",
    "        self.use_cached_media = use_cached_media\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        lang_x, \n",
    "        attention_mask,\n",
    "        *args,\n",
    "        **encoder_layer_kwargs\n",
    "    ):\n",
    "        # Cross attention\n",
    "        if self.gated_cross_attn_layer is not None:\n",
    "            if self.vis_x is None:\n",
    "                raise ValueError(\"vis_x must be conditioned before forward pass\")\n",
    "\n",
    "            if self.media_locations is None:\n",
    "                raise ValueError(\n",
    "                    \"media_locations must be conditioned before forward pass\"\n",
    "                )\n",
    "\n",
    "            lang_x = self.gated_cross_attn_layer(\n",
    "                lang_x,\n",
    "                self.vis_x,\n",
    "                media_locations=self.media_locations,\n",
    "                use_cached_media=self.use_cached_media,\n",
    "            )\n",
    "\n",
    "        # Normal decoder layer\n",
    "        lang_x = self.encoder_layer(\n",
    "            lang_x, attention_mask, *args\n",
    "        )\n",
    "        return lang_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb51b33b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:06.486650Z",
     "iopub.status.busy": "2024-10-10T01:41:06.485760Z",
     "iopub.status.idle": "2024-10-10T01:41:06.501272Z",
     "shell.execute_reply": "2024-10-10T01:41:06.500336Z"
    },
    "papermill": {
     "duration": 0.183543,
     "end_time": "2024-10-10T01:41:06.503214",
     "exception": false,
     "start_time": "2024-10-10T01:41:06.319671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor,CLIPModel, CLIPProcessor, AutoTokenizer, AutoModel,RobertaModel\n",
    "\n",
    "class FlamingoLMMixin(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixin to add cross-attention layers to a language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def set_encoder_layers_attr_name(self, encoder_layers_attr_name):\n",
    "        self.encoder_layers_attr_name = encoder_layers_attr_name\n",
    "\n",
    "    def _get_encoder_layers(self):\n",
    "        return getattr_recursive(self, self.encoder_layers_attr_name)\n",
    "\n",
    "    def _set_encoder_layers(self, value):\n",
    "        setattr_recursive(self, self.encoder_layers_attr_name, value)\n",
    "\n",
    "    def init_flamingo(\n",
    "        self,\n",
    "        media_token_id,\n",
    "        lang_hidden_size,\n",
    "        vis_hidden_size,\n",
    "        cross_attn_every_n_layers,\n",
    "        gradient_checkpointing,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Flamingo by adding a new gated cross attn to the decoder. Store the media token id for computing the media locations.\n",
    "        \"\"\"\n",
    "        self.old_encoder_blocks = self._get_encoder_layers()\n",
    "        self.gated_cross_attn_layers = nn.ModuleList(\n",
    "            [\n",
    "                GatedCrossAttentionBlock(\n",
    "                    dim=lang_hidden_size, dim_visual=vis_hidden_size\n",
    "                )\n",
    "                if (layer_idx + 1) % cross_attn_every_n_layers == 0\n",
    "                else None\n",
    "                for layer_idx, _ in enumerate(self._get_encoder_layers())\n",
    "            ]\n",
    "        )\n",
    "        self.init_flamingo_layers(gradient_checkpointing)\n",
    "        self.media_token_id = media_token_id\n",
    "        self.initialized_flamingo = True\n",
    "        self._use_cached_vision_x = False\n",
    "\n",
    "    def init_flamingo_layers(self, gradient_checkpointing):\n",
    "        \"\"\"\n",
    "        Re initializes the FlamingoLayers.\n",
    "        Propagates any changes made to self.gated_corss_attn_layers or self.old_encoder_blocks\n",
    "        \"\"\"\n",
    "        self._set_encoder_layers(\n",
    "            nn.ModuleList(\n",
    "                [\n",
    "                    FlamingoLayer(\n",
    "                        gated_cross_attn_layer, encoder_layer, gradient_checkpointing\n",
    "                    )\n",
    "                    for gated_cross_attn_layer, encoder_layer in zip(\n",
    "                        self.gated_cross_attn_layers, self.old_encoder_blocks\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids,attention_mask, **kwargs):\n",
    "\n",
    "        \"\"\"Condition the Flamingo layers on the media locations before forward()\"\"\"\n",
    "        if not self.initialized_flamingo:\n",
    "            raise ValueError(\n",
    "                \"Flamingo layers are not initialized. Please call `init_flamingo` first.\"\n",
    "            )\n",
    "        media_locations = input_ids == self.media_token_id\n",
    "\n",
    "        # if there are media already cached and we're generating and there are no media tokens in the input,\n",
    "        # we'll assume that ALL input tokens should attend to the last previous media that is cached.\n",
    "        # this is especially important for HF generate() compatibility, since generate() calls forward()\n",
    "        # repeatedly one token at a time (with no media tokens).\n",
    "        # without this check, the model would not attend to any images when generating (after the first token)\n",
    "        use_cached_media_locations = (\n",
    "            self._use_cached_vision_x\n",
    "            and self.is_conditioned()\n",
    "            and not media_locations.any()\n",
    "        )\n",
    "\n",
    "        for layer in self._get_encoder_layers():\n",
    "            if not use_cached_media_locations:\n",
    "                layer.condition_media_locations(media_locations)\n",
    "            layer.condition_use_cached_media(use_cached_media_locations)\n",
    "\n",
    "        # package arguments for the other parent's forward. since we don't know the order of the arguments,\n",
    "        # make them all kwargs\n",
    "\n",
    "        kwargs[\"input_ids\"] = input_ids\n",
    "        kwargs[\"attention_mask\"] = attention_mask\n",
    "\n",
    "        return super().forward(**kwargs)  # Call the other parent's forward method\n",
    "\n",
    "    def is_conditioned(self) -> bool:\n",
    "        \"\"\"Check whether all decoder layers are already conditioned.\"\"\"\n",
    "        return all(l.is_conditioned() for l in self._get_encoder_layers())\n",
    "\n",
    "    def clear_conditioned_layers(self):\n",
    "        for layer in self._get_encoder_layers():\n",
    "            layer.condition_vis_x(None)\n",
    "            layer.condition_media_locations(None)\n",
    "            layer.condition_use_cached_media(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4703b920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:06.858107Z",
     "iopub.status.busy": "2024-10-10T01:41:06.857697Z",
     "iopub.status.idle": "2024-10-10T01:41:07.438958Z",
     "shell.execute_reply": "2024-10-10T01:41:07.437793Z"
    },
    "papermill": {
     "duration": 0.743768,
     "end_time": "2024-10-10T01:41:07.441478",
     "exception": false,
     "start_time": "2024-10-10T01:41:06.697710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP,\n",
    ")\n",
    "\n",
    "class Flamingo(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_encoder: nn.Module,\n",
    "        lang_encoder: nn.Module,\n",
    "        vis_dim: int,\n",
    "        media_token_id: int,\n",
    "        n_classes = 4,\n",
    "        cross_attn_every_n_layers: int = 1,\n",
    "        gradient_checkpointing: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vision_encoder (nn.Module): HF CLIPModel\n",
    "            lang_encoder (nn.Module): HF causal language model\n",
    "                vis_dim (int): Dimension of the visual features.\n",
    "                Visual features are projected to match this shape along the last dimension.\n",
    "            cross_attn_every_n_layers (int, optional): How often to apply cross attention after transformer layer. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.media_token_id = media_token_id\n",
    "        self.vis_dim = vis_dim\n",
    "        if hasattr(lang_encoder.config, \"d_model\"):\n",
    "            self.lang_dim = lang_encoder.config.d_model  # mpt uses d_model\n",
    "        else:\n",
    "            self.lang_dim = lang_encoder.config.hidden_size\n",
    "\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.perceiver = PerceiverResampler(dim=self.vis_dim)\n",
    "        self.lang_encoder = lang_encoder\n",
    "        self.lang_encoder.init_flamingo(\n",
    "            media_token_id=media_token_id,\n",
    "            lang_hidden_size=self.lang_dim,\n",
    "            vis_hidden_size=self.vis_dim,\n",
    "            cross_attn_every_n_layers=cross_attn_every_n_layers,\n",
    "            gradient_checkpointing=gradient_checkpointing,\n",
    "        )\n",
    "        self.classifier = nn.Linear(self.lang_dim*4*2,n_classes)\n",
    "\n",
    "        self._use_gradient_checkpointing = gradient_checkpointing\n",
    "        self.perceiver._use_gradient_checkpointing = gradient_checkpointing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        vision_x: torch.Tensor,\n",
    "        lang_x: torch.Tensor,\n",
    "        attention_mask = None, \n",
    "        clear_conditioned_layers: bool = True,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Forward pass of Flamingo.\n",
    "\n",
    "        Args:\n",
    "            vision_x (torch.Tensor): Vision input\n",
    "                shape (B, T_img, F, C, H, W) with F=1\n",
    "            lang_x (torch.Tensor): Language input ids\n",
    "                shape (B, T_txt)\n",
    "            attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n",
    "            labels (torch.Tensor, optional): Labels. Defaults to None.\n",
    "            clear_conditioned_layers: if True, clear the conditioned layers\n",
    "                once the foward pass is completed. Set this to false if the\n",
    "                same set of images will be reused in another subsequent\n",
    "                forward pass.\n",
    "            past_key_values: pre-computed values to pass to language model.\n",
    "                See past_key_values documentation in Hugging Face\n",
    "                CausalLM models.\n",
    "            use_cache: whether to use cached key values. See use_cache\n",
    "                documentation in Hugging Face CausalLM models.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.lang_encoder.initialized_flamingo\n",
    "        ), \"Flamingo layers are not initialized. Please call `init_flamingo` first.\"\n",
    "\n",
    "        assert (\n",
    "            self.lang_encoder._use_cached_vision_x or vision_x is not None\n",
    "        ), \"Must provide either vision_x or have precached media using cache_media().\"\n",
    "        \n",
    "        self.lang_encoder._use_cached_vision_x = True\n",
    "        self._encode_vision_x(vision_x=vision_x)\n",
    "        self._condition_media_locations(input_ids=lang_x)\n",
    "\n",
    "        output = self.lang_encoder(\n",
    "            input_ids=lang_x,\n",
    "            attention_mask = attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        # output = output.pooler_output\n",
    "        # output = self.classifier(output)\n",
    "\n",
    "        hidden_states = output.hidden_states\n",
    "        last_hidden_state = torch.cat(hidden_states[-4:], dim=-1) # B, 1 + seq_len, H*4\n",
    "        text_features = last_hidden_state[:,0,:] # B, H*4\n",
    "        visual_features = last_hidden_state[:,1,:] # B, H*4\n",
    "        cls_and_visual = torch.cat([text_features,visual_features], dim=1)\n",
    "        output = self.classifier(cls_and_visual)\n",
    "        \n",
    "        self.lang_encoder.clear_conditioned_layers()\n",
    "        # self.lang_encoder._use_cached_vision_x = False\n",
    "\n",
    "        return output\n",
    "\n",
    "        \n",
    "    def _encode_vision_x(self, vision_x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute media tokens from vision input by passing it through vision encoder and conditioning language model.\n",
    "        Args:\n",
    "            vision_x (torch.Tensor): Vision input\n",
    "                shape (B, T_img, F, C, H, W)\n",
    "                Images in the same chunk are collated along T_img, and frames are collated along F\n",
    "                Currently only F=1 is supported (single-frame videos)\n",
    "\n",
    "        rearrange code based on https://github.com/dhansmair/flamingo-mini\n",
    "        \"\"\"\n",
    "\n",
    "        assert vision_x.ndim == 6, \"vision_x should be of shape (b, T_img, F, C, H, W)\"\n",
    "        b, T, F = vision_x.shape[:3]\n",
    "        assert F == 1, \"Only single frame supported\"\n",
    "\n",
    "        vision_x = rearrange(vision_x, \"b T F c h w -> (b T F) c h w\")\n",
    "        with torch.no_grad():\n",
    "            # vision_x = self.vision_encoder(vision_x)[1].unsqueeze(1) #check\n",
    "            vision_x = self.vision_encoder(vision_x)[1]\n",
    "\n",
    "        vision_x = rearrange(vision_x, \"(b T F) v d -> b T F v d\", b=b, T=T, F=F)\n",
    "        vision_x = self.perceiver(vision_x)\n",
    "        # print('vision size:', vision_x.size())\n",
    "        for layer in self.lang_encoder._get_encoder_layers():\n",
    "            layer.condition_vis_x(vision_x)\n",
    "        def clip_grad_norm_(max_norm):\n",
    "            self.perceiver.clip_grad_norm_(max_norm)\n",
    "            for layer in self.lang_encoder.gated_cross_attn_layers:\n",
    "                if layer is not None:\n",
    "                    layer.clip_grad_norm_(max_norm)\n",
    "            self.lang_encoder.get_input_embeddings().clip_grad_norm_(max_norm)\n",
    "\n",
    "        self.clip_grad_norm_ = clip_grad_norm_\n",
    "\n",
    "    def _condition_media_locations(self, input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the media token locations from lang_x and condition the language model on these.\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Language input\n",
    "                shape (B, T_txt)\n",
    "        \"\"\"\n",
    "        media_locations = input_ids == self.media_token_id\n",
    "\n",
    "        for layer in self.lang_encoder._get_encoder_layers():\n",
    "            layer.condition_media_locations(media_locations)\n",
    "\n",
    "    def cache_media(self, input_ids: torch.Tensor, vision_x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Pre-cache a prompt/sequence of images / text for log-likelihood evaluations.\n",
    "        All subsequent calls to forward() will generate attending to the LAST\n",
    "        image in vision_x.\n",
    "        This is not meant to be used to cache things for generate().\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Language input\n",
    "                shape (B, T_txt)\n",
    "            vision_x (torch.Tensor): Vision input\n",
    "                shape (B, T_img, F, C, H, W)\n",
    "                Images in the same chunk are collated along T_img, and frames are collated along F\n",
    "                Currently only F=1 is supported (single-frame videos)\n",
    "        \"\"\"\n",
    "        self._encode_vision_x(vision_x=vision_x)\n",
    "        self._condition_media_locations(input_ids=input_ids)\n",
    "        self.lang_encoder._use_cached_vision_x = True\n",
    "\n",
    "    def uncache_media(self):\n",
    "        \"\"\"\n",
    "        Clear all conditioning.\n",
    "        \"\"\"\n",
    "        self.lang_encoder.clear_conditioned_layers()\n",
    "        self.lang_encoder._use_cached_vision_x = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5253f4e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:07.758900Z",
     "iopub.status.busy": "2024-10-10T01:41:07.758020Z",
     "iopub.status.idle": "2024-10-10T01:41:07.762733Z",
     "shell.execute_reply": "2024-10-10T01:41:07.761815Z"
    },
    "papermill": {
     "duration": 0.165637,
     "end_time": "2024-10-10T01:41:07.765341",
     "exception": false,
     "start_time": "2024-10-10T01:41:07.599704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "__KNOWN_encoder_layerS_ATTR_NAMES = {\n",
    "    'roberta': 'encoder.layer',\n",
    "    'MultilingualCLIP': 'transformer.encoder.layer',\n",
    "    'ernie': 'encoder.layers'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aff00035",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:08.081458Z",
     "iopub.status.busy": "2024-10-10T01:41:08.080761Z",
     "iopub.status.idle": "2024-10-10T01:41:08.086130Z",
     "shell.execute_reply": "2024-10-10T01:41:08.085232Z"
    },
    "papermill": {
     "duration": 0.165692,
     "end_time": "2024-10-10T01:41:08.088067",
     "exception": false,
     "start_time": "2024-10-10T01:41:07.922375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _infer_encoder_layers_attr_name(model):\n",
    "    for k in __KNOWN_encoder_layerS_ATTR_NAMES:\n",
    "        if k.lower() in model.__class__.__name__.lower():\n",
    "            return __KNOWN_encoder_layerS_ATTR_NAMES[k]\n",
    "\n",
    "    raise ValueError(\n",
    "        f\"We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d9872ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:08.401888Z",
     "iopub.status.busy": "2024-10-10T01:41:08.401533Z",
     "iopub.status.idle": "2024-10-10T01:41:08.406327Z",
     "shell.execute_reply": "2024-10-10T01:41:08.405428Z"
    },
    "papermill": {
     "duration": 0.163673,
     "end_time": "2024-10-10T01:41:08.408270",
     "exception": false,
     "start_time": "2024-10-10T01:41:08.244597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor,CLIPModel, CLIPProcessor, AutoTokenizer, AutoModel,RobertaModel\n",
    "from transformers import CLIPVisionModel, CLIPVisionConfig\n",
    "\n",
    "import torch\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc4fb764",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:08.726851Z",
     "iopub.status.busy": "2024-10-10T01:41:08.726186Z",
     "iopub.status.idle": "2024-10-10T01:41:17.720494Z",
     "shell.execute_reply": "2024-10-10T01:41:17.719467Z"
    },
    "papermill": {
     "duration": 9.157572,
     "end_time": "2024-10-10T01:41:17.723136",
     "exception": false,
     "start_time": "2024-10-10T01:41:08.565564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cd35ebc77c4cd581f269f0e1a18ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang_encoder = AutoModel.from_pretrained(PRETRAINED_PATH)\n",
    "# vision_encoder = AutoModel.from_pretrained(VISION_PRETRAINED_PATH)\n",
    "\n",
    "temp_vision, _, _ = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-16',\n",
    "    pretrained='openai'\n",
    ")\n",
    "\n",
    "vision_encoder = temp_vision.visual\n",
    "vision_encoder.output_tokens = True\n",
    "vision_encoder.attn_pool = vision_encoder.proj = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40d00372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:18.104243Z",
     "iopub.status.busy": "2024-10-10T01:41:18.103573Z",
     "iopub.status.idle": "2024-10-10T01:41:18.114172Z",
     "shell.execute_reply": "2024-10-10T01:41:18.112917Z"
    },
    "papermill": {
     "duration": 0.233526,
     "end_time": "2024-10-10T01:41:18.117172",
     "exception": false,
     "start_time": "2024-10-10T01:41:17.883646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable parameter vision: 85799424\n",
      "trainable parameter text: 278043648\n"
     ]
    }
   ],
   "source": [
    "print(\"trainable parameter vision:\", sum(p.numel() for p in vision_encoder.parameters()))\n",
    "print(\"trainable parameter text:\", sum(p.numel() for p in lang_encoder.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d47830c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:18.508827Z",
     "iopub.status.busy": "2024-10-10T01:41:18.508246Z",
     "iopub.status.idle": "2024-10-10T01:41:18.513931Z",
     "shell.execute_reply": "2024-10-10T01:41:18.512997Z"
    },
    "papermill": {
     "duration": 0.19821,
     "end_time": "2024-10-10T01:41:18.516428",
     "exception": false,
     "start_time": "2024-10-10T01:41:18.318218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "extend_instance(lang_encoder, FlamingoLMMixin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55473cd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:18.901800Z",
     "iopub.status.busy": "2024-10-10T01:41:18.900853Z",
     "iopub.status.idle": "2024-10-10T01:41:22.517387Z",
     "shell.execute_reply": "2024-10-10T01:41:22.516399Z"
    },
    "papermill": {
     "duration": 3.808271,
     "end_time": "2024-10-10T01:41:22.519590",
     "exception": false,
     "start_time": "2024-10-10T01:41:18.711319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(250003, 768, padding_idx=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layers_attr_name  = _infer_encoder_layers_attr_name(lang_encoder)\n",
    "lang_encoder.set_encoder_layers_attr_name(encoder_layers_attr_name)\n",
    "lang_encoder.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f52d2023",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:22.843084Z",
     "iopub.status.busy": "2024-10-10T01:41:22.842168Z",
     "iopub.status.idle": "2024-10-10T01:41:23.649708Z",
     "shell.execute_reply": "2024-10-10T01:41:23.648900Z"
    },
    "papermill": {
     "duration": 0.972022,
     "end_time": "2024-10-10T01:41:23.652114",
     "exception": false,
     "start_time": "2024-10-10T01:41:22.680092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Flamingo(\n",
    "    vision_encoder,\n",
    "    lang_encoder,\n",
    "    cross_attn_every_n_layers=1,\n",
    "    vis_dim = 768,\n",
    "    media_token_id = tokenizer.convert_tokens_to_ids('<image>'),\n",
    "    gradient_checkpointing=False,\n",
    "    n_classes = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b29b590c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:23.970203Z",
     "iopub.status.busy": "2024-10-10T01:41:23.969513Z",
     "iopub.status.idle": "2024-10-10T01:41:24.827729Z",
     "shell.execute_reply": "2024-10-10T01:41:24.826670Z"
    },
    "papermill": {
     "duration": 1.020617,
     "end_time": "2024-10-10T01:41:24.829887",
     "exception": false,
     "start_time": "2024-10-10T01:41:23.809270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flamingo(\n",
       "  (vision_encoder): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (perceiver): PerceiverResampler(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x ModuleList(\n",
       "        (0): PerceiverAttention(\n",
       "          (norm_media): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm_latents): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lang_encoder): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250003, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (encoder_layer): XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): XLMRobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (old_encoder_blocks): ModuleList(\n",
       "      (0-11): 12 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (gated_cross_attn_layers): ModuleList(\n",
       "      (0-11): 12 x GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=768, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=768, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=6144, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe16729c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:25.175841Z",
     "iopub.status.busy": "2024-10-10T01:41:25.175069Z",
     "iopub.status.idle": "2024-10-10T01:41:25.185457Z",
     "shell.execute_reply": "2024-10-10T01:41:25.184437Z"
    },
    "papermill": {
     "duration": 0.179902,
     "end_time": "2024-10-10T01:41:25.187304",
     "exception": false,
     "start_time": "2024-10-10T01:41:25.007402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable parameter text-vision: 477229852\n"
     ]
    }
   ],
   "source": [
    "print(\"trainable parameter text-vision:\", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d139574a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:25.565136Z",
     "iopub.status.busy": "2024-10-10T01:41:25.563946Z",
     "iopub.status.idle": "2024-10-10T01:41:26.749136Z",
     "shell.execute_reply": "2024-10-10T01:41:26.748131Z"
    },
    "papermill": {
     "duration": 1.355051,
     "end_time": "2024-10-10T01:41:26.751313",
     "exception": false,
     "start_time": "2024-10-10T01:41:25.396262",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8953, -0.7289,  0.3897,  0.5291],\n",
       "        [-0.9052, -0.6666,  0.3146,  0.4813],\n",
       "        [-0.7840, -0.7115,  0.3036,  0.5682],\n",
       "        [-0.8815, -0.6927,  0.3846,  0.5628]], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    break\n",
    "    \n",
    "with torch.no_grad():\n",
    "    input_ids, attention_mask, pixel_values, label = data\n",
    "    pixel_values = pixel_values.float()\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    label = label.to(device)\n",
    "    \n",
    "    logits = model(\n",
    "            vision_x =  pixel_values,\n",
    "            lang_x =  input_ids,\n",
    "            attention_mask = attention_mask\n",
    "            )\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491f2e4",
   "metadata": {
    "papermill": {
     "duration": 0.157369,
     "end_time": "2024-10-10T01:41:27.071007",
     "exception": false,
     "start_time": "2024-10-10T01:41:26.913638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "948e83a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:27.399941Z",
     "iopub.status.busy": "2024-10-10T01:41:27.399278Z",
     "iopub.status.idle": "2024-10-10T01:41:27.422348Z",
     "shell.execute_reply": "2024-10-10T01:41:27.421620Z"
    },
    "papermill": {
     "duration": 0.19475,
     "end_time": "2024-10-10T01:41:27.424319",
     "exception": false,
     "start_time": "2024-10-10T01:41:27.229569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class AdEMAMix(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999, 0.9999), eps=1e-8,\n",
    "                 weight_decay=0, alpha=5.0, T_alpha_beta3=None):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        assert len(betas) == 3, f\"Invalid beta parameters: {betas}, expected 3\"\n",
    "        assert all(0.0 <= beta < 1.0 for beta in betas), f\"Invalid beta parameters: {betas}\"\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        \n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
    "                        alpha=alpha, T_alpha_beta3=T_alpha_beta3)\n",
    "        super(AdEMAMix, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AdEMAMix, self).__setstate__(state)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            exp_avg_slow = []\n",
    "            state_steps = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    if p.grad.is_sparse:\n",
    "                        raise RuntimeError('AdEMAMix does not support sparse gradients')\n",
    "                    grads.append(p.grad)\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    # Lazy state initialization\n",
    "                    if len(state) == 0:\n",
    "                        state['step'] = 0\n",
    "                        # Exponential moving average of gradient values\n",
    "                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        # Exponential moving average of squared gradient values\n",
    "                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        # Slow exponential moving average\n",
    "                        state['exp_avg_slow'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                    exp_avgs.append(state['exp_avg'])\n",
    "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "                    exp_avg_slow.append(state['exp_avg_slow'])\n",
    "                    state['step'] += 1\n",
    "                    state_steps.append(state['step'])\n",
    "\n",
    "            beta1, beta2, beta3 = group['betas']\n",
    "            alpha = group['alpha']\n",
    "            T_alpha_beta3 = group['T_alpha_beta3']\n",
    "\n",
    "            self._update_adamemix(\n",
    "                params_with_grad,\n",
    "                grads,\n",
    "                exp_avgs,\n",
    "                exp_avg_sqs,\n",
    "                exp_avg_slow,\n",
    "                state_steps,\n",
    "                beta1=beta1,\n",
    "                beta2=beta2,\n",
    "                beta3=beta3,\n",
    "                alpha=alpha,\n",
    "                T_alpha_beta3=T_alpha_beta3,\n",
    "                lr=group['lr'],\n",
    "                weight_decay=group['weight_decay'],\n",
    "                eps=group['eps'],\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _update_adamemix(self, params, grads, exp_avgs, exp_avg_sqs, exp_avg_slow, state_steps,\n",
    "                         beta1, beta2, beta3, alpha, T_alpha_beta3, lr, weight_decay, eps):\n",
    "        \n",
    "        for i, param in enumerate(params):\n",
    "            grad = grads[i]\n",
    "            exp_avg = exp_avgs[i]\n",
    "            exp_avg_sq = exp_avg_sqs[i]\n",
    "            exp_avg_slow_i = exp_avg_slow[i]\n",
    "            step = state_steps[i]\n",
    "\n",
    "            bias_correction1 = 1 - beta1 ** step\n",
    "            bias_correction2 = 1 - beta2 ** step\n",
    "\n",
    "            if T_alpha_beta3 is not None:\n",
    "                alpha_t = min(step * alpha / T_alpha_beta3, alpha)\n",
    "                beta3_t = min(math.exp(math.log(beta1) * math.log(beta3) / \n",
    "                              ((1 - step / T_alpha_beta3) * math.log(beta3) + \n",
    "                               (step / T_alpha_beta3) * math.log(beta1))), beta3)\n",
    "            else:\n",
    "                alpha_t = alpha\n",
    "                beta3_t = beta3\n",
    "\n",
    "            # Decay the first and second moment running average coefficient\n",
    "            exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "            exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "            exp_avg_slow_i.mul_(beta3_t).add_(grad, alpha=1 - beta3_t)\n",
    "\n",
    "            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
    "\n",
    "            step_size = lr / bias_correction1\n",
    "\n",
    "            if weight_decay != 0:\n",
    "                param.add_(param, alpha=-weight_decay * lr)\n",
    "\n",
    "            param.addcdiv_(exp_avg + alpha_t * exp_avg_slow_i, denom, value=-step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63a33cfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:27.742535Z",
     "iopub.status.busy": "2024-10-10T01:41:27.741747Z",
     "iopub.status.idle": "2024-10-10T01:41:27.755267Z",
     "shell.execute_reply": "2024-10-10T01:41:27.754360Z"
    },
    "papermill": {
     "duration": 0.176139,
     "end_time": "2024-10-10T01:41:27.757088",
     "exception": false,
     "start_time": "2024-10-10T01:41:27.580949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                        lr=3e-5)\n",
    "# optimizer = AdEMAMix(optimizer_grouped_parameters,\n",
    "#                         lr=3e-5,weight_decay = 0.1, alpha=8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8818d550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:28.078789Z",
     "iopub.status.busy": "2024-10-10T01:41:28.077913Z",
     "iopub.status.idle": "2024-10-10T01:41:28.094853Z",
     "shell.execute_reply": "2024-10-10T01:41:28.093962Z"
    },
    "papermill": {
     "duration": 0.17919,
     "end_time": "2024-10-10T01:41:28.096829",
     "exception": false,
     "start_time": "2024-10-10T01:41:27.917639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This implements the Adafocal loss function proposed in the NeurIPS-2022 paper \"AdaFocal: Calibration-aware Adaptive Focal Loss\".\n",
    "Authors: Arindam Ghosh, Thomas Schaaf, and Matt Gormley.\n",
    "Url: https://proceedings.neurips.cc/paper_files/paper/2022/hash/0a692a24dbc744fca340b9ba33bc6522-Abstract-Conference.html\n",
    "'''\n",
    "\n",
    "import os, json, math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "\n",
    "class AdaFocal(nn.Module):\n",
    "    def __init__(self, device=None):            \n",
    "        super(AdaFocal, self).__init__()\n",
    "\n",
    "    # This function updates the bin statistics which are used by the Adafocal loss at every epoch.\n",
    "    def update_bin_stats(self, val_adabin_dict):\n",
    "        for bin_no in range(self.num_bins):\n",
    "            # This is the Adafocal gamma update rule\n",
    "            prev_gamma = self.bin_stats[bin_no]['gamma']\n",
    "            exp_term = val_adabin_dict[bin_no]['calibration_gap']\n",
    "            if prev_gamma > 0:\n",
    "                next_gamma = prev_gamma * math.exp(self.lamda*exp_term)\n",
    "            else:\n",
    "                next_gamma = prev_gamma * math.exp(-self.lamda*exp_term)    \n",
    "            # This switches between focal and inverse-focal loss when required.\n",
    "            if abs(next_gamma) < self.switch_pt:\n",
    "                if next_gamma > 0:\n",
    "                    next_gamma = -self.switch_pt\n",
    "                else:\n",
    "                    next_gamma = self.switch_pt\n",
    "            self.bin_stats[bin_no]['gamma'] = max(min(next_gamma, self.gamma_max), self.gamma_min) # gamma-clipping\n",
    "            self.bin_stats[bin_no]['lower_boundary'] = val_adabin_dict[bin_no]['lower_bound']\n",
    "            self.bin_stats[bin_no]['upper_boundary'] = val_adabin_dict[bin_no]['upper_bound']\n",
    "        return\n",
    "\n",
    "    # This function selects the gammas for each sample based on which bin it falls into.\n",
    "    def get_gamma_per_sample(self, pt):\n",
    "        gamma_list = []\n",
    "        batch_size = pt.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            pt_sample = pt[i].item()\n",
    "            for bin_no, stats in self.bin_stats.items():\n",
    "                if bin_no==0 and pt_sample < stats['upper_boundary']:\n",
    "                    break\n",
    "                elif bin_no==self.num_bins-1 and pt_sample >= stats['lower_boundary']:\n",
    "                    break\n",
    "                elif pt_sample >= stats['lower_boundary'] and pt_sample < stats['upper_boundary']:\n",
    "                    break\n",
    "            gamma_list.append(stats['gamma'])\n",
    "        return torch.tensor(gamma_list).to(self.device)\n",
    "\n",
    "    # This computes the loss value to be returned for back-propagation.\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        gamma = self.get_gamma_per_sample(pt)\n",
    "        gamma_sign = torch.sign(gamma)\n",
    "        gamma_mag = torch.abs(gamma)\n",
    "        pt = gamma_sign * pt\n",
    "        loss = -1 * ((1 - pt + 1e-20)**gamma_mag) * logpt # 1e-20 added for numerical stability \n",
    " \n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fe1b483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:28.415725Z",
     "iopub.status.busy": "2024-10-10T01:41:28.414804Z",
     "iopub.status.idle": "2024-10-10T01:41:28.435889Z",
     "shell.execute_reply": "2024-10-10T01:41:28.435019Z"
    },
    "papermill": {
     "duration": 0.181743,
     "end_time": "2024-10-10T01:41:28.437883",
     "exception": false,
     "start_time": "2024-10-10T01:41:28.256140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Implementation of Dual Focal Loss.\n",
    "Reference:\n",
    "[1]  Tao, Linwei, Minjing Dong, and Chang Xu. \"Dual Focal Loss for Calibration.\" arXiv preprint arXiv:2305.13665 (2023).\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class AdaDualFocalLoss(nn.Module):\n",
    "    def __init__(self, size_average=False, device = None):\n",
    "        super(AdaDualFocalLoss, self).__init__()\n",
    "        self.size_average = size_average\n",
    "        self.num_bins = 15\n",
    "        self.lamda = 1.0\n",
    "        self.gamma_initial = 2.0\n",
    "        self.switch_pt = 0.2\n",
    "        self.gamma_max = 20.0\n",
    "        self.gamma_min = -2.0\n",
    "        self.update_gamma_every = -1\n",
    "        self.device = device\n",
    "        # This initializes the bin_stats variable\n",
    "        self.bin_stats = collections.defaultdict(dict)\n",
    "        for bin_no in range(self.num_bins):\n",
    "            self.bin_stats[bin_no]['lower_boundary'] = bin_no*(1/self.num_bins)\n",
    "            self.bin_stats[bin_no]['upper_boundary'] = (bin_no+1)*(1/self.num_bins)\n",
    "            self.bin_stats[bin_no]['gamma'] = self.gamma_initial\n",
    "\n",
    "    # This function updates the bin statistics which are used by the Adafocal loss at every epoch.\n",
    "    def update_bin_stats(self, val_adabin_dict):\n",
    "        for bin_no in range(self.num_bins):\n",
    "            # This is the Adafocal gamma update rule\n",
    "            prev_gamma = self.bin_stats[bin_no]['gamma']\n",
    "            exp_term = val_adabin_dict[bin_no]['calibration_gap']\n",
    "            if prev_gamma > 0:\n",
    "                next_gamma = prev_gamma * math.exp(self.lamda*exp_term)\n",
    "            else:\n",
    "                next_gamma = prev_gamma * math.exp(-self.lamda*exp_term)    \n",
    "            # This switches between focal and inverse-focal loss when required.\n",
    "            if abs(next_gamma) < self.switch_pt:\n",
    "                if next_gamma > 0:\n",
    "                    next_gamma = -self.switch_pt\n",
    "                else:\n",
    "                    next_gamma = self.switch_pt\n",
    "            self.bin_stats[bin_no]['gamma'] = max(min(next_gamma, self.gamma_max), self.gamma_min) # gamma-clipping\n",
    "            self.bin_stats[bin_no]['lower_boundary'] = val_adabin_dict[bin_no]['lower_bound']\n",
    "            self.bin_stats[bin_no]['upper_boundary'] = val_adabin_dict[bin_no]['upper_bound']\n",
    "        return\n",
    "\n",
    "    # This function selects the gammas for each sample based on which bin it falls into.\n",
    "    def get_gamma_per_sample(self, pt):\n",
    "        gamma_list = []\n",
    "        batch_size = pt.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            pt_sample = pt[i].item()\n",
    "            for bin_no, stats in self.bin_stats.items():\n",
    "                if bin_no==0 and pt_sample < stats['upper_boundary']:\n",
    "                    break\n",
    "                elif bin_no==self.num_bins-1 and pt_sample >= stats['lower_boundary']:\n",
    "                    break\n",
    "                elif pt_sample >= stats['lower_boundary'] and pt_sample < stats['upper_boundary']:\n",
    "                    break\n",
    "            gamma_list.append(stats['gamma'])\n",
    "        return torch.tensor(gamma_list).to(self.device)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logp_k = F.log_softmax(input, dim=1)\n",
    "        softmax_logits = logp_k.exp()\n",
    "        logp_k = logp_k.gather(1, target)\n",
    "        logp_k = logp_k.view(-1)\n",
    "        p_k = logp_k.exp()  # p_k: probility at target label\n",
    "        p_j_mask = torch.lt(softmax_logits, p_k.reshape(p_k.shape[0], 1)) * 1  # mask all logit larger and equal than p_k\n",
    "        p_j = torch.topk(p_j_mask * softmax_logits, 1)[0].squeeze()\n",
    "\n",
    "        gamma = self.get_gamma_per_sample(p_k)\n",
    "        gamma_sign = torch.sign(gamma)\n",
    "        gamma_mag = torch.abs(gamma)\n",
    "        p_k = gamma_sign * p_k\n",
    "\n",
    "        loss = -1 * ((1 - p_k + p_j + 1e-20) ** gamma_mag) * logp_k\n",
    "\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5458549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:28.753794Z",
     "iopub.status.busy": "2024-10-10T01:41:28.753429Z",
     "iopub.status.idle": "2024-10-10T01:41:28.757604Z",
     "shell.execute_reply": "2024-10-10T01:41:28.756724Z"
    },
    "papermill": {
     "duration": 0.164195,
     "end_time": "2024-10-10T01:41:28.759546",
     "exception": false,
     "start_time": "2024-10-10T01:41:28.595351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c0038c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:29.080426Z",
     "iopub.status.busy": "2024-10-10T01:41:29.079572Z",
     "iopub.status.idle": "2024-10-10T01:41:29.084261Z",
     "shell.execute_reply": "2024-10-10T01:41:29.083365Z"
    },
    "papermill": {
     "duration": 0.167655,
     "end_time": "2024-10-10T01:41:29.086295",
     "exception": false,
     "start_time": "2024-10-10T01:41:28.918640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = AdaDualFocalLoss(size_average=True, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "baa81dab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:29.407137Z",
     "iopub.status.busy": "2024-10-10T01:41:29.406706Z",
     "iopub.status.idle": "2024-10-10T01:41:29.411700Z",
     "shell.execute_reply": "2024-10-10T01:41:29.410785Z"
    },
    "papermill": {
     "duration": 0.168464,
     "end_time": "2024-10-10T01:41:29.413744",
     "exception": false,
     "start_time": "2024-10-10T01:41:29.245280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_training_step = len(train_loader)*NUM_EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=num_training_step//10, num_training_steps=num_training_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37d5e65e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:29.728857Z",
     "iopub.status.busy": "2024-10-10T01:41:29.728277Z",
     "iopub.status.idle": "2024-10-10T01:41:29.734248Z",
     "shell.execute_reply": "2024-10-10T01:41:29.733350Z"
    },
    "papermill": {
     "duration": 0.165127,
     "end_time": "2024-10-10T01:41:29.736046",
     "exception": false,
     "start_time": "2024-10-10T01:41:29.570919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250002"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('<image>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91d72986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:30.053546Z",
     "iopub.status.busy": "2024-10-10T01:41:30.052656Z",
     "iopub.status.idle": "2024-10-10T01:41:30.065117Z",
     "shell.execute_reply": "2024-10-10T01:41:30.064285Z"
    },
    "papermill": {
     "duration": 0.17257,
     "end_time": "2024-10-10T01:41:30.067031",
     "exception": false,
     "start_time": "2024-10-10T01:41:29.894461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, criterion, dataloader, epoch,device,scaler):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # batch accumulation parameter\n",
    "    accum_iter = 2\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        for i, data in enumerate(tepoch):\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            input_ids, attention_mask, pixel_values, label = data\n",
    "            pixel_values = pixel_values.float()\n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            pixel_values = pixel_values.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.bfloat16 if mode == 'BF16' else torch.float16, enabled=True if '16' in mode else False):\n",
    "                logits = model(\n",
    "                    vision_x =  pixel_values,\n",
    "                    lang_x =  input_ids,\n",
    "                    attention_mask = attention_mask\n",
    "                    )\n",
    "                losses = criterion(logits,label)\n",
    "                \n",
    "            lll = losses.item()\n",
    "            epoch_loss += lll\n",
    "            \n",
    "            all_asp_loss = losses / accum_iter\n",
    "            scaler.scale(all_asp_loss).backward()\n",
    "            embed_grad = (\n",
    "                    model.lang_encoder.get_input_embeddings().weight.grad\n",
    "                        )\n",
    "            zero_mask = torch.zeros_like(embed_grad)\n",
    "            zero_mask[250002] = torch.ones_like(zero_mask[250002])\n",
    "            model.lang_encoder.get_input_embeddings().weight.grad = (\n",
    "                                embed_grad * zero_mask\n",
    "            )\n",
    "\n",
    "            if ((i + 1) % accum_iter == 0) or (i + 1 == len(dataloader)):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            tepoch.set_postfix(loss=lll)\n",
    "\n",
    "        epoch_loss /= i\n",
    "        print(f\"At EPOCH {epoch}, loss = {epoch_loss}\")\n",
    "        print(\"======================================\")\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aedf11c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:30.385040Z",
     "iopub.status.busy": "2024-10-10T01:41:30.384618Z",
     "iopub.status.idle": "2024-10-10T01:41:30.390051Z",
     "shell.execute_reply": "2024-10-10T01:41:30.389133Z"
    },
    "papermill": {
     "duration": 0.167281,
     "end_time": "2024-10-10T01:41:30.391933",
     "exception": false,
     "start_time": "2024-10-10T01:41:30.224652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    p_macro, r_macro, f_macro, support_macro \\\n",
    "      = precision_recall_fscore_support(y_true, y_pred, average=None,zero_division=0.0,labels = [0,1,2,3])\n",
    "    return p_macro, r_macro, f_macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86c4150a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:30.712731Z",
     "iopub.status.busy": "2024-10-10T01:41:30.711865Z",
     "iopub.status.idle": "2024-10-10T01:41:30.723035Z",
     "shell.execute_reply": "2024-10-10T01:41:30.722140Z"
    },
    "papermill": {
     "duration": 0.173272,
     "end_time": "2024-10-10T01:41:30.725078",
     "exception": false,
     "start_time": "2024-10-10T01:41:30.551806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llabels = ['not-sarcasm', 'image-sarcasm','text-sarcasm','multi-sarcasm']\n",
    "def eval(model, criterion, dataloader,device):\n",
    "\n",
    "    print(\"EVAL STEP: \")\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        for i, data in enumerate(tepoch):\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            input_ids, attention_mask, pixel_values, label = data\n",
    "            pixel_values = pixel_values.float()\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            pixel_values = pixel_values.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(\n",
    "                    vision_x =  pixel_values,\n",
    "                    lang_x =  input_ids,\n",
    "                    attention_mask = attention_mask\n",
    "                    )\n",
    "                \n",
    "                eval_losses = criterion(logits,label)\n",
    "                pred = np.argmax(logits.cpu().detach(),axis = -1).to('cpu').numpy()\n",
    "                label = label.to('cpu').numpy()\n",
    "                \n",
    "                preds.append(pred)\n",
    "                labels.append(label)\n",
    "\n",
    "            epoch_loss += eval_losses\n",
    "            \n",
    "        epoch_loss /= i\n",
    "        print(f\"Loss = {epoch_loss}\")\n",
    "        \n",
    "        preds = np.concatenate(preds)\n",
    "        labels = np.concatenate(labels)\n",
    "        precision, recall, f1_score = macro_f1(labels,preds)\n",
    "        for idx, ll in enumerate(llabels):\n",
    "            print(f\"{ll}:\")\n",
    "            print(\"\\t- Precision:\", precision[idx])\n",
    "            print(\"\\t- Recall:\", recall[idx])\n",
    "            print(\"\\t- F1-score:\", f1_score[idx])\n",
    "            print(\"========================\")\n",
    "        print(\"===============NEXT STEP===============\")\n",
    "\n",
    "        return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37457d8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:31.081721Z",
     "iopub.status.busy": "2024-10-10T01:41:31.081324Z",
     "iopub.status.idle": "2024-10-10T01:41:31.086355Z",
     "shell.execute_reply": "2024-10-10T01:41:31.085412Z"
    },
    "papermill": {
     "duration": 0.166491,
     "end_time": "2024-10-10T01:41:31.088271",
     "exception": false,
     "start_time": "2024-10-10T01:41:30.921780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(path, model, epoch):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "    },path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7de17c9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:31.406421Z",
     "iopub.status.busy": "2024-10-10T01:41:31.405662Z",
     "iopub.status.idle": "2024-10-10T01:41:31.410182Z",
     "shell.execute_reply": "2024-10-10T01:41:31.409290Z"
    },
    "papermill": {
     "duration": 0.16626,
     "end_time": "2024-10-10T01:41:31.412157",
     "exception": false,
     "start_time": "2024-10-10T01:41:31.245897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    check_point = torch.load(path)\n",
    "    return check_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8b56b13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:31.734614Z",
     "iopub.status.busy": "2024-10-10T01:41:31.733742Z",
     "iopub.status.idle": "2024-10-10T01:41:31.745594Z",
     "shell.execute_reply": "2024-10-10T01:41:31.744674Z"
    },
    "papermill": {
     "duration": 0.177434,
     "end_time": "2024-10-10T01:41:31.747607",
     "exception": false,
     "start_time": "2024-10-10T01:41:31.570173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def test_classification_net_adafocal(model, data_loader, device, num_bins, num_labels=None):\n",
    "    '''\n",
    "    This function reports classification accuracy and confusion matrix over a dataset.\n",
    "    '''\n",
    "    \n",
    "    model.eval()\n",
    "    labels_list = []\n",
    "    predictions_list = []\n",
    "    confidence_vals_list = []\n",
    "    loss = 0.0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader):\n",
    "            input_ids, attention_mask, pixel_values, labels = data\n",
    "            pixel_values = pixel_values.float()\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            pixel_values = pixel_values.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(\n",
    "                vision_x =  pixel_values,\n",
    "                lang_x =  input_ids,\n",
    "                attention_mask = attention_mask\n",
    "                )\n",
    "            \n",
    "            # Compute NLL (cross entropy loss)\n",
    "            loss += F.cross_entropy(logits, labels, reduction='sum').item()\n",
    "\n",
    "            if i == 0:\n",
    "                fulldataset_logits = logits\n",
    "            else:\n",
    "                fulldataset_logits = torch.cat((fulldataset_logits, logits), dim=0)\n",
    "\n",
    "            log_softmax = F.log_softmax(logits, dim=1)\n",
    "            log_confidence_vals, predictions = torch.max(log_softmax, dim=1)\n",
    "            confidence_vals = log_confidence_vals.exp()\n",
    "\n",
    "            predictions_list.extend(predictions.cpu().numpy().tolist())\n",
    "            confidence_vals_list.extend(confidence_vals.cpu().numpy().tolist())\n",
    "            labels_list.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "            num_samples += len(data)\n",
    "\n",
    "    accuracy = accuracy_score(labels_list, predictions_list)\n",
    "    return loss/num_samples, confusion_matrix(labels_list, predictions_list), accuracy, labels_list,\\\n",
    "        predictions_list, confidence_vals_list, fulldataset_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49ff8a2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:32.086158Z",
     "iopub.status.busy": "2024-10-10T01:41:32.085748Z",
     "iopub.status.idle": "2024-10-10T01:41:32.097822Z",
     "shell.execute_reply": "2024-10-10T01:41:32.096878Z"
    },
    "papermill": {
     "duration": 0.17286,
     "end_time": "2024-10-10T01:41:32.099702",
     "exception": false,
     "start_time": "2024-10-10T01:41:31.926842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adaECE_error_mukhoti(confs, preds, labels, num_bins=15):\n",
    "    def histedges_equalN(x):\n",
    "        npt = len(x)\n",
    "        return np.interp(np.linspace(0, npt, num_bins + 1), np.arange(npt), np.sort(x))\n",
    "        # y = numpy.interp(x, xp, fp) returns the interpolated function values for pints in x using xp-fp points already given. \n",
    "        # Let x = np.array([0.26, 0.53, 0.61, 0.75, 0.94, 0.99])\n",
    "        # np.linspace(0, 6, 3 + 1) -> array([ 0.,  2.,  4., 6.])\n",
    "        # np.arange(6) -> array([0, 1, 2, 3, 4, 5])\n",
    "        # np.interp(np.linspace(0, 6, 3+1), np.arange(6), np.sort(x)) -> array([0.26, 0.61, 0.94, 0.99])\n",
    "\n",
    "    confidences, predictions, labels = torch.FloatTensor(confs), torch.FloatTensor(preds), torch.FloatTensor(labels)\n",
    "    accuracies = predictions.eq(labels)\n",
    "    n, bin_boundaries = np.histogram(confidences.cpu().detach(), histedges_equalN(confidences.cpu().detach()))\n",
    "\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    ece = torch.zeros(1)\n",
    "    bin_dict = collections.defaultdict(dict)\n",
    "    bin_num = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        # Calculated |confidence - accuracy| in each bin\n",
    "        in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "        prop_in_bin = in_bin.float().mean()\n",
    "        if prop_in_bin.item() > 0:\n",
    "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "        else:\n",
    "            accuracy_in_bin = torch.zeros(1)\n",
    "            avg_confidence_in_bin = torch.zeros(1)\n",
    "        # Save the bin stats to be returned\n",
    "        bin_dict[bin_num]['lower_bound'] = bin_lower\n",
    "        bin_dict[bin_num]['upper_bound'] = bin_upper\n",
    "        bin_dict[bin_num]['prop_in_bin'] = prop_in_bin.item()\n",
    "        bin_dict[bin_num]['accuracy_in_bin'] = accuracy_in_bin.item()\n",
    "        bin_dict[bin_num]['avg_confidence_in_bin'] = avg_confidence_in_bin.item()\n",
    "        bin_dict[bin_num]['calibration_gap'] = bin_dict[bin_num]['avg_confidence_in_bin'] - bin_dict[bin_num]['accuracy_in_bin']\n",
    "        bin_num += 1\n",
    "        \n",
    "    return ece, bin_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "022c0790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T01:41:32.417970Z",
     "iopub.status.busy": "2024-10-10T01:41:32.417594Z",
     "iopub.status.idle": "2024-10-10T03:20:09.442745Z",
     "shell.execute_reply": "2024-10-10T03:20:09.441666Z"
    },
    "papermill": {
     "duration": 5917.186149,
     "end_time": "2024-10-10T03:20:09.445202",
     "exception": false,
     "start_time": "2024-10-10T01:41:32.259053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2431/2431 [11:03<00:00,  3.67batch/s, loss=0.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At EPOCH 0, loss = 0.5688137521829318\n",
      "======================================\n",
      "EVAL STEP: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 271/271 [00:45<00:00,  5.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.45713359117507935\n",
      "not-sarcasm:\n",
      "\t- Precision: 0.7235023041474654\n",
      "\t- Recall: 0.7772277227722773\n",
      "\t- F1-score: 0.7494033412887828\n",
      "========================\n",
      "image-sarcasm:\n",
      "\t- Precision: 0.0\n",
      "\t- Recall: 0.0\n",
      "\t- F1-score: 0.0\n",
      "========================\n",
      "text-sarcasm:\n",
      "\t- Precision: 0.0\n",
      "\t- Recall: 0.0\n",
      "\t- F1-score: 0.0\n",
      "========================\n",
      "multi-sarcasm:\n",
      "\t- Precision: 0.6186046511627907\n",
      "\t- Recall: 0.6288416075650118\n",
      "\t- F1-score: 0.6236811254396248\n",
      "========================\n",
      "===============NEXT STEP===============\n",
      "update bins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2431/2431 [11:03<00:00,  3.66batch/s, loss=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At EPOCH 1, loss = 0.4581217275257893\n",
      "======================================\n",
      "EVAL STEP: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 271/271 [00:36<00:00,  7.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.4182615578174591\n",
      "not-sarcasm:\n",
      "\t- Precision: 0.8207964601769911\n",
      "\t- Recall: 0.6122112211221122\n",
      "\t- F1-score: 0.7013232514177694\n",
      "========================\n",
      "image-sarcasm:\n",
      "\t- Precision: 0.0\n",
      "\t- Recall: 0.0\n",
      "\t- F1-score: 0.0\n",
      "========================\n",
      "text-sarcasm:\n",
      "\t- Precision: 0.0\n",
      "\t- Recall: 0.0\n",
      "\t- F1-score: 0.0\n",
      "========================\n",
      "multi-sarcasm:\n",
      "\t- Precision: 0.5596184419713831\n",
      "\t- Recall: 0.8321513002364066\n",
      "\t- F1-score: 0.6692015209125475\n",
      "========================\n",
      "===============NEXT STEP===============\n",
      "update bins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2431/2431 [11:03<00:00,  3.67batch/s, loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At EPOCH 2, loss = 0.42832648187871913\n",
      "======================================\n",
      "EVAL STEP: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 271/271 [00:36<00:00,  7.41batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.4036566913127899\n",
      "not-sarcasm:\n",
      "\t- Precision: 0.818359375\n",
      "\t- Recall: 0.6914191419141914\n",
      "\t- F1-score: 0.7495527728085868\n",
      "========================\n",
      "image-sarcasm:\n",
      "\t- Precision: 0.8333333333333334\n",
      "\t- Recall: 0.11363636363636363\n",
      "\t- F1-score: 0.19999999999999998\n",
      "========================\n",
      "text-sarcasm:\n",
      "\t- Precision: 0.0\n",
      "\t- Recall: 0.0\n",
      "\t- F1-score: 0.0\n",
      "========================\n",
      "multi-sarcasm:\n",
      "\t- Precision: 0.6074600355239786\n",
      "\t- Recall: 0.8085106382978723\n",
      "\t- F1-score: 0.6937119675456389\n",
      "========================\n",
      "===============NEXT STEP===============\n",
      "update bins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2431/2431 [11:04<00:00,  3.66batch/s, loss=0.497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At EPOCH 3, loss = 0.3873145458232918\n",
      "======================================\n",
      "EVAL STEP: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 271/271 [00:36<00:00,  7.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.40584972500801086\n",
      "not-sarcasm:\n",
      "\t- Precision: 0.7149059334298119\n",
      "\t- Recall: 0.8151815181518152\n",
      "\t- F1-score: 0.7617579028527371\n",
      "========================\n",
      "image-sarcasm:\n",
      "\t- Precision: 0.75\n",
      "\t- Recall: 0.06818181818181818\n",
      "\t- F1-score: 0.125\n",
      "========================\n",
      "text-sarcasm:\n",
      "\t- Precision: 0.0\n",
      "\t- Recall: 0.0\n",
      "\t- F1-score: 0.0\n",
      "========================\n",
      "multi-sarcasm:\n",
      "\t- Precision: 0.6450777202072538\n",
      "\t- Recall: 0.5886524822695035\n",
      "\t- F1-score: 0.6155747836835599\n",
      "========================\n",
      "===============NEXT STEP===============\n",
      "update bins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2431/2431 [11:04<00:00,  3.66batch/s, loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At EPOCH 4, loss = 0.4116156750943113\n",
      "======================================\n",
      "EVAL STEP: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 271/271 [00:37<00:00,  7.24batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.4283238351345062\n",
      "not-sarcasm:\n",
      "\t- Precision: 0.8058076225045372\n",
      "\t- Recall: 0.7326732673267327\n",
      "\t- F1-score: 0.7675021607605877\n",
      "========================\n",
      "image-sarcasm:\n",
      "\t- Precision: 0.4\n",
      "\t- Recall: 0.045454545454545456\n",
      "\t- F1-score: 0.0816326530612245\n",
      "========================\n",
      "text-sarcasm:\n",
      "\t- Precision: 0.0\n",
      "\t- Recall: 0.0\n",
      "\t- F1-score: 0.0\n",
      "========================\n",
      "multi-sarcasm:\n",
      "\t- Precision: 0.6190476190476191\n",
      "\t- Recall: 0.7683215130023641\n",
      "\t- F1-score: 0.6856540084388186\n",
      "========================\n",
      "===============NEXT STEP===============\n",
      "update bins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2431/2431 [11:07<00:00,  3.64batch/s, loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At EPOCH 5, loss = 0.33511294884829484\n",
      "======================================\n",
      "EVAL STEP: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 271/271 [00:37<00:00,  7.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.3857438862323761\n",
      "not-sarcasm:\n",
      "\t- Precision: 0.774671052631579\n",
      "\t- Recall: 0.7772277227722773\n",
      "\t- F1-score: 0.7759472817133443\n",
      "========================\n",
      "image-sarcasm:\n",
      "\t- Precision: 0.75\n",
      "\t- Recall: 0.13636363636363635\n",
      "\t- F1-score: 0.23076923076923075\n",
      "========================\n",
      "text-sarcasm:\n",
      "\t- Precision: 1.0\n",
      "\t- Recall: 0.125\n",
      "\t- F1-score: 0.2222222222222222\n",
      "========================\n",
      "multi-sarcasm:\n",
      "\t- Precision: 0.6487068965517241\n",
      "\t- Recall: 0.7115839243498818\n",
      "\t- F1-score: 0.6786922209695604\n",
      "========================\n",
      "===============NEXT STEP===============\n",
      "update bins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2431/2431 [11:06<00:00,  3.65batch/s, loss=0.0163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At EPOCH 6, loss = 0.28250485618202087\n",
      "======================================\n",
      "EVAL STEP: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 271/271 [00:37<00:00,  7.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.3981431722640991\n",
      "not-sarcasm:\n",
      "\t- Precision: 0.815018315018315\n",
      "\t- Recall: 0.7343234323432343\n",
      "\t- F1-score: 0.7725694444444445\n",
      "========================\n",
      "image-sarcasm:\n",
      "\t- Precision: 0.6\n",
      "\t- Recall: 0.13636363636363635\n",
      "\t- F1-score: 0.22222222222222218\n",
      "========================\n",
      "text-sarcasm:\n",
      "\t- Precision: 1.0\n",
      "\t- Recall: 0.125\n",
      "\t- F1-score: 0.2222222222222222\n",
      "========================\n",
      "multi-sarcasm:\n",
      "\t- Precision: 0.6374045801526718\n",
      "\t- Recall: 0.789598108747045\n",
      "\t- F1-score: 0.7053854276663146\n",
      "========================\n",
      "===============NEXT STEP===============\n",
      "update bins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 2431/2431 [11:05<00:00,  3.66batch/s, loss=0.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At EPOCH 7, loss = 0.23231302230856515\n",
      "======================================\n",
      "EVAL STEP: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 271/271 [00:36<00:00,  7.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.442472904920578\n",
      "not-sarcasm:\n",
      "\t- Precision: 0.7699680511182109\n",
      "\t- Recall: 0.7953795379537953\n",
      "\t- F1-score: 0.7824675324675324\n",
      "========================\n",
      "image-sarcasm:\n",
      "\t- Precision: 0.42857142857142855\n",
      "\t- Recall: 0.13636363636363635\n",
      "\t- F1-score: 0.20689655172413793\n",
      "========================\n",
      "text-sarcasm:\n",
      "\t- Precision: 1.0\n",
      "\t- Recall: 0.125\n",
      "\t- F1-score: 0.2222222222222222\n",
      "========================\n",
      "multi-sarcasm:\n",
      "\t- Precision: 0.6681818181818182\n",
      "\t- Recall: 0.6950354609929078\n",
      "\t- F1-score: 0.6813441483198146\n",
      "========================\n",
      "===============NEXT STEP===============\n",
      "update bins\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "count = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_one_epoch(model, criterion, train_loader, epoch, device,scaler)\n",
    "    eval_loss = eval(model, criterion,dev_loader,device)\n",
    "    print(\"update bins\")\n",
    "    loss, out_confusion_matrix, acc, labels, predictions, confidences, logits = test_classification_net_adafocal(model, data_loader = dev_loader, device = device, num_bins = 15)\n",
    "    adaece, adabin_dict = adaECE_error_mukhoti(confidences, predictions, labels, num_bins=15)\n",
    "    criterion.update_bin_stats(adabin_dict)\n",
    "    \n",
    "    if epoch >= NUM_EPOCHS - 3:\n",
    "        save_model(f'model_{epoch}.pth',model, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278205f",
   "metadata": {
    "papermill": {
     "duration": 5.289279,
     "end_time": "2024-10-10T03:20:19.987786",
     "exception": false,
     "start_time": "2024-10-10T03:20:14.698507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAKE RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c6f2e3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T03:20:31.030239Z",
     "iopub.status.busy": "2024-10-10T03:20:31.029355Z",
     "iopub.status.idle": "2024-10-10T03:20:31.486555Z",
     "shell.execute_reply": "2024-10-10T03:20:31.485168Z"
    },
    "papermill": {
     "duration": 6.132388,
     "end_time": "2024-10-10T03:20:31.488209",
     "exception": true,
     "start_time": "2024-10-10T03:20:25.355821",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File kaggle/input/dev-dsc2024/vimmsd-public-test.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkaggle/input/dev-dsc2024/vimmsd-public-test.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43morient\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/json/_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/json/_json.py:904\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/json/_json.py:960\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    952\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    959\u001b[0m ):\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    968\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File kaggle/input/dev-dsc2024/vimmsd-public-test.json does not exist"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_json('kaggle/input/dev-dsc2024/vimmsd-public-test.json',orient = 'index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df471d6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalize_class = TextNormalize()\n",
    "test_data['caption'] = test_data['caption'].apply(lambda x: normalize_class.normalize(text_normalize(convert_unicode(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221b73e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_PATH)\n",
    "tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": [\"<image>\"]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4081ec4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# PHASE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d894c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDS_1(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, img_folder, vision_processor, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.img_folder = img_folder\n",
    "        self.vision_processor = vision_processor\n",
    "        self.labels = ['not-sarcasm','sarcasm']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.loc[idx,'caption']\n",
    "        \n",
    "        index = self.data.loc[idx,'index']\n",
    "        img_name = self.data.loc[idx,'image']\n",
    "        img_file = os.path.join(self.img_folder,img_name)\n",
    "        \n",
    "        tokenizer_seq_dict = self.tokenizer(f\"<image> {text}\", max_length=self.max_len,truncation=True,padding='max_length', return_length=True, return_tensors='pt')\n",
    "        input_ids = tokenizer_seq_dict['input_ids'][0]\n",
    "        attention_mask = torch.zeros(input_ids.size())\n",
    "\n",
    "        image = Image.open(img_file)\n",
    "        pixel_values = self.vision_processor(images=image, return_tensors=\"pt\")['pixel_values'].unsqueeze(1)\n",
    "        \n",
    "        return input_ids, attention_mask, pixel_values, index   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c116ad0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vision_processor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20adca81",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set = TestDS_1(test_data, tokenizer, '/kaggle/input/dev-dsc2024/public-test-images/dev-images', vision_processor, max_len = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c8464",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5020521",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang_encoder = AutoModel.from_pretrained('susnato/ernie-m-base_pytorch')\n",
    "vision_encoder = CLIPVisionModel.from_pretrained('openai/clip-vit-large-patch14')\n",
    "\n",
    "extend_instance(lang_encoder, FlamingoLMMixin)\n",
    "encoder_layers_attr_name  = _infer_encoder_layers_attr_name(lang_encoder)\n",
    "lang_encoder.set_encoder_layers_attr_name(encoder_layers_attr_name)\n",
    "lang_encoder.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f318863",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_1 = Flamingo(\n",
    "    vision_encoder,\n",
    "    lang_encoder,\n",
    "    cross_attn_every_n_layers=1,\n",
    "    vis_dim = 1024,\n",
    "    media_token_id = tokenizer.convert_tokens_to_ids('<image>'),\n",
    "    n_classes = 1,\n",
    "    gradient_checkpointing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ca8cc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef2fe6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = load_model('phase1_7.pth')\n",
    "model_1.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51896dda",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = test_set.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4cc10",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for data in tqdm(test_loader):\n",
    "    \n",
    "    input_ids, attention_mask, pixel_values, indexes = data\n",
    "    pixel_values = pixel_values.float()\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_1(\n",
    "            vision_x =  pixel_values,\n",
    "            lang_x =  input_ids,\n",
    "            )\n",
    "        pred = (torch.nn.functional.sigmoid(logits) >= 0.6).to(torch.int).cpu().detach().numpy().reshape(-1)\n",
    "        \n",
    "    for idx, pred in zip(indexes,pred):\n",
    "        results[idx.item()] = labels[pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6059d7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cdc6f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    'results': results,\n",
    "    'phase': 'dev'\n",
    "}\n",
    "len(final_results['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a6cd3e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('phase_1_results.json', 'w',encoding = 'utf-8') as f:\n",
    "    json.dump(final_results, f, ensure_ascii = False, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24bd6c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e4ead77",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# PHASE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17074531",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, img_folder, vision_processor, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.img_folder = img_folder\n",
    "        self.vision_processor = vision_processor\n",
    "        self.labels = ['image-sarcasm','text-sarcasm','multi-sarcasm']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.loc[idx,'caption']\n",
    "        \n",
    "        index = self.data.loc[idx,'index']\n",
    "        img_name = self.data.loc[idx,'image']\n",
    "        img_file = os.path.join(self.img_folder,img_name)\n",
    "        \n",
    "        tokenizer_seq_dict = self.tokenizer(f\"<image> {text}\", max_length=self.max_len,truncation=True,padding='max_length', return_length=True, return_tensors='pt')\n",
    "        input_ids = tokenizer_seq_dict['input_ids'][0]\n",
    "        attention_mask = torch.zeros(input_ids.size())\n",
    "\n",
    "        image = read_image(img_file).permute(1,2,0)[:,:,:3]\n",
    "        pixel_values = self.vision_processor(images=image, return_tensors=\"pt\")['pixel_values'].unsqueeze(1)\n",
    "        \n",
    "        return input_ids, attention_mask, pixel_values, index   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0874649e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data_2 = pd.read_json('phase_1_results.json').reset_index()\n",
    "test_data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fed921",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data_2 = test_data_2[test_data_2['results'] == 'sarcasm']\n",
    "test_data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d732ae1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data_2 = test_data[test_data['index'].isin(test_data_2['index'].values.tolist())].reset_index()\n",
    "test_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457ac17",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set = TestDS(test_data_2, tokenizer, '/kaggle/input/dev-dsc2024/public-test-images/dev-images', vision_processor, max_len = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da127bba",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74458b4b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba576e94",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VISION_PRETRAINED_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067beabe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang_encoder = AutoModel.from_pretrained(PRETRAINED_PATH)\n",
    "vision_encoder = AutoModel.from_pretrained(VISION_PRETRAINED_PATH)\n",
    "\n",
    "extend_instance(lang_encoder, FlamingoLMMixin)\n",
    "encoder_layers_attr_name  = _infer_encoder_layers_attr_name(lang_encoder)\n",
    "lang_encoder.set_encoder_layers_attr_name(encoder_layers_attr_name)\n",
    "lang_encoder.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91ee66",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Flamingo(\n",
    "    vision_encoder,\n",
    "    lang_encoder,\n",
    "    cross_attn_every_n_layers=1,\n",
    "    vis_dim = 1024,\n",
    "    media_token_id = tokenizer.convert_tokens_to_ids('<image>'),\n",
    "    gradient_checkpointing=False,\n",
    "    n_classes = 3\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c068a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = load_model('model_4.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb6bca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = test_set.labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77797ba3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for data in tqdm(test_loader):\n",
    "    \n",
    "    input_ids, attention_mask, pixel_values, indexes = data\n",
    "    pixel_values = pixel_values.float()\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            vision_x =  pixel_values,\n",
    "            lang_x =  input_ids,\n",
    "            )\n",
    "        pred = np.argmax(logits.cpu().detach(),axis = -1).to('cpu').numpy()\n",
    "    \n",
    "    for idx, pred in zip(indexes,pred):\n",
    "        results[idx.item()] = labels[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053133a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    'results': results,\n",
    "    'phase': 'dev'\n",
    "}\n",
    "len(final_results['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a36d1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834713e1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('phase_2_results.json', 'w',encoding = 'utf-8') as f:\n",
    "    json.dump(final_results, f, ensure_ascii = False, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d967c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('phase_1_results.json', 'r',encoding = 'utf-8') as f:\n",
    "    phase_1_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaf180b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('phase_2_results.json', 'r',encoding = 'utf-8') as f:\n",
    "    phase_2_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eeb300",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final = phase_1_results.copy()\n",
    "len(final['results'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10782bc0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k,v in phase_2_results['results'].items():\n",
    "    final['results'][k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5107165",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(final['results'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b60f3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('results.json', 'w',encoding = 'utf-8') as f:\n",
    "    json.dump(final, f, ensure_ascii = False, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1ccdb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5828704,
     "sourceId": 9565924,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5828698,
     "isSourceIdPinned": true,
     "sourceId": 9565944,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6275.26416,
   "end_time": "2024-10-10T03:20:39.921336",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-10T01:36:04.657176",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0044038783e94696819c435de74efa03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "041a9ed9186b460fb77ad7358eddf569": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "072165ccb296468eaff1c10ca80c408e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "08f1736d40624f59a5efa9fe056a217b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "09b62b3759ca425e949d621ede012dcb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_94bf5f725d824e24854df096d124ae94",
       "placeholder": "​",
       "style": "IPY_MODEL_9c139805dec949e0b843e82381a3a882",
       "value": " 9.10M/9.10M [00:00&lt;00:00, 18.8MB/s]"
      }
     },
     "0e9c69bec2b9448a8eb8ad0683c29216": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e984f724dd294a259b46ee5665a48b28",
       "placeholder": "​",
       "style": "IPY_MODEL_08f1736d40624f59a5efa9fe056a217b",
       "value": "tokenizer.json: 100%"
      }
     },
     "0f5e703e6da6491298fad8187b8cd808": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0044038783e94696819c435de74efa03",
       "placeholder": "​",
       "style": "IPY_MODEL_f71a66f4092a44c4964ccfe6e1b9cba3",
       "value": "sentencepiece.bpe.model: 100%"
      }
     },
     "1173553aea1c4d2f95c7cca821c0132f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13009b1c45d14ad5a162f0790bf2589c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1395cb9dddec42759c11b2e67179753b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1b1cde4db53f4b07b91839fdcafba75a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dd22dbc22e5b4908956cb1b4a669ea6b",
       "placeholder": "​",
       "style": "IPY_MODEL_adfe7f9d29d14a728d1d9406cb01dd2f",
       "value": " 5.07M/5.07M [00:00&lt;00:00, 25.8MB/s]"
      }
     },
     "1cadcc3b58664f7a865611905baa8b9b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d02d2c7040c4921a9a70b16b04ebbea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e6cf79146a6462596e42adb8535961f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ec53fb1a7b04e6e80f3f29c1a1bc91c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d49b60816bea49ddadae100e97ab082f",
       "placeholder": "​",
       "style": "IPY_MODEL_1395cb9dddec42759c11b2e67179753b",
       "value": " 615/615 [00:00&lt;00:00, 48.0kB/s]"
      }
     },
     "22b3535d035b431ba6946e0eba9e47f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b6f8ef34859a4d3187ec9aad319eb2f2",
       "max": 5069051.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f35dead553614a2f8c4f2cb8f416042c",
       "value": 5069051.0
      }
     },
     "244a71e1921148afa74cd2a1a030e154": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2574f52f2ee049eab3b3c122b4cafbdf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_511753a0c4a0453281d3f145b49e0d78",
       "placeholder": "​",
       "style": "IPY_MODEL_ae58267c9e484fe782c1a735ead3087a",
       "value": "model.safetensors: 100%"
      }
     },
     "2878c199330e45e0ab85a8bf454c3e23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28d02e6661684d12bde134586557d3d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f7870e2d5b54ecaa0dae8487beabaa6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "32299ac6d3c84ea6928d75f38d2ecd46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "37ec4679e6bc43cfbadb330fac9b0196": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fbd554139782448aa3c0dbde29e7c8b2",
        "IPY_MODEL_ff0c47387152445ca2736fdd7de1fc95",
        "IPY_MODEL_1ec53fb1a7b04e6e80f3f29c1a1bc91c"
       ],
       "layout": "IPY_MODEL_658e9bd371124a368adda8f8299456d6"
      }
     },
     "3ea0bb0e13974589af5679bb41814976": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "417696d0c32f44f3b4c526f819869986": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0e9c69bec2b9448a8eb8ad0683c29216",
        "IPY_MODEL_da35109f2b39494e991d60d8e7cd4fde",
        "IPY_MODEL_09b62b3759ca425e949d621ede012dcb"
       ],
       "layout": "IPY_MODEL_7d2d30afcaaa4bc591f54aeea3434f67"
      }
     },
     "4c0379cc4e264cb4ab32e5677773350d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4c636ca4ea694aeeb33c9141a5e62748": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "511753a0c4a0453281d3f145b49e0d78": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53b8c892b3c948ceb6ec17c45d7a80f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_28d02e6661684d12bde134586557d3d7",
       "placeholder": "​",
       "style": "IPY_MODEL_54728b24deb14e489b4f1bb183cabf5a",
       "value": " 1.12G/1.12G [00:05&lt;00:00, 240MB/s]"
      }
     },
     "54728b24deb14e489b4f1bb183cabf5a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5b3832a9287c4d43b215ee9dd6dfcacc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_32299ac6d3c84ea6928d75f38d2ecd46",
       "max": 1115567652.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4c0379cc4e264cb4ab32e5677773350d",
       "value": 1115567652.0
      }
     },
     "658e9bd371124a368adda8f8299456d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f1ce9d510554f39829809cc92ff79e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7682bb66fa0642f4a6a5712fcee6abfa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d2d30afcaaa4bc591f54aeea3434f67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7f6901046e504db2b739ce14a6ae5ec1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "86d1eaef94524174977c73aeeaf7275f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1d02d2c7040c4921a9a70b16b04ebbea",
       "placeholder": "​",
       "style": "IPY_MODEL_7f6901046e504db2b739ce14a6ae5ec1",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "94bf5f725d824e24854df096d124ae94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9b038bdc219d406392cce74c3bde929c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d1fc09f5cd4f4a07b003599c0b3fac53",
        "IPY_MODEL_ea1c65c9eeb24b21b6c9b7a3e1334a22",
        "IPY_MODEL_e807bae524d5400eadff7b463987f2be"
       ],
       "layout": "IPY_MODEL_041a9ed9186b460fb77ad7358eddf569"
      }
     },
     "9c139805dec949e0b843e82381a3a882": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a3ba62e0b33a4ccdaa94879de48a920a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ab53457bfe4342c888aa987790ee3359",
       "placeholder": "​",
       "style": "IPY_MODEL_244a71e1921148afa74cd2a1a030e154",
       "value": " 25.0/25.0 [00:00&lt;00:00, 1.64kB/s]"
      }
     },
     "a467f23b84904d25b187f4b1203310a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab53457bfe4342c888aa987790ee3359": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "adfe7f9d29d14a728d1d9406cb01dd2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ae58267c9e484fe782c1a735ead3087a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b6f8ef34859a4d3187ec9aad319eb2f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bd174ee35ae54db592fd59e3494f4289": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bd2e5b7c96294547bb2465cda278a556": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0f5e703e6da6491298fad8187b8cd808",
        "IPY_MODEL_22b3535d035b431ba6946e0eba9e47f7",
        "IPY_MODEL_1b1cde4db53f4b07b91839fdcafba75a"
       ],
       "layout": "IPY_MODEL_d835dbcb8a3e43f984abe5e37562fb93"
      }
     },
     "bfa7107915f547c4a44d554ef7dfd125": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2f5f9ef949c43c498e52896dfe5061e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a467f23b84904d25b187f4b1203310a2",
       "max": 25.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_eca70c1cb45249089ba882893a9bbdf3",
       "value": 25.0
      }
     },
     "d1fc09f5cd4f4a07b003599c0b3fac53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bd174ee35ae54db592fd59e3494f4289",
       "placeholder": "​",
       "style": "IPY_MODEL_eb85d857334a4b34bd2ca782fb77817b",
       "value": "preprocessor_config.json: 100%"
      }
     },
     "d49b60816bea49ddadae100e97ab082f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d835dbcb8a3e43f984abe5e37562fb93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "da35109f2b39494e991d60d8e7cd4fde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7682bb66fa0642f4a6a5712fcee6abfa",
       "max": 9096718.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_13009b1c45d14ad5a162f0790bf2589c",
       "value": 9096718.0
      }
     },
     "dd22dbc22e5b4908956cb1b4a669ea6b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e807bae524d5400eadff7b463987f2be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3ea0bb0e13974589af5679bb41814976",
       "placeholder": "​",
       "style": "IPY_MODEL_6f1ce9d510554f39829809cc92ff79e7",
       "value": " 436/436 [00:00&lt;00:00, 33.6kB/s]"
      }
     },
     "e984f724dd294a259b46ee5665a48b28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9cd35ebc77c4cd581f269f0e1a18ec6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2574f52f2ee049eab3b3c122b4cafbdf",
        "IPY_MODEL_5b3832a9287c4d43b215ee9dd6dfcacc",
        "IPY_MODEL_53b8c892b3c948ceb6ec17c45d7a80f1"
       ],
       "layout": "IPY_MODEL_bfa7107915f547c4a44d554ef7dfd125"
      }
     },
     "ea1c65c9eeb24b21b6c9b7a3e1334a22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2878c199330e45e0ab85a8bf454c3e23",
       "max": 436.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_072165ccb296468eaff1c10ca80c408e",
       "value": 436.0
      }
     },
     "eb85d857334a4b34bd2ca782fb77817b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "eca70c1cb45249089ba882893a9bbdf3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f1f9303cdc74473491ff8e76c329c95c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_86d1eaef94524174977c73aeeaf7275f",
        "IPY_MODEL_c2f5f9ef949c43c498e52896dfe5061e",
        "IPY_MODEL_a3ba62e0b33a4ccdaa94879de48a920a"
       ],
       "layout": "IPY_MODEL_1e6cf79146a6462596e42adb8535961f"
      }
     },
     "f35dead553614a2f8c4f2cb8f416042c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f71a66f4092a44c4964ccfe6e1b9cba3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fbd554139782448aa3c0dbde29e7c8b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1cadcc3b58664f7a865611905baa8b9b",
       "placeholder": "​",
       "style": "IPY_MODEL_2f7870e2d5b54ecaa0dae8487beabaa6",
       "value": "config.json: 100%"
      }
     },
     "ff0c47387152445ca2736fdd7de1fc95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1173553aea1c4d2f95c7cca821c0132f",
       "max": 615.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4c636ca4ea694aeeb33c9141a5e62748",
       "value": 615.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
