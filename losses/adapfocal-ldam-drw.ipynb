{"cells":[{"cell_type":"markdown","metadata":{},"source":["# IMPORT LIBRARY"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:37:55.397052Z","iopub.status.busy":"2024-10-09T05:37:55.396599Z","iopub.status.idle":"2024-10-09T05:39:20.153238Z","shell.execute_reply":"2024-10-09T05:39:20.152033Z","shell.execute_reply.started":"2024-10-09T05:37:55.397010Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["!pip install underthesea\n","!pip install sentencepiece\n","# !pip install transformers==4.44.2\n","!pip install einops\n","!pip install einops_exts\n","!pip install pandas\n","# !pip install torch==2.4.0\n","# !pip install torchvision==0.19.0 \n","!pip install open_clip_torch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:20.156124Z","iopub.status.busy":"2024-10-09T05:39:20.155703Z","iopub.status.idle":"2024-10-09T05:39:29.261826Z","shell.execute_reply":"2024-10-09T05:39:29.260881Z","shell.execute_reply.started":"2024-10-09T05:39:20.156079Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from underthesea import word_tokenize,text_normalize\n","import torch\n","from transformers import AutoTokenizer,get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","import pandas as pd\n","import json\n","import numpy as np\n","from PIL import Image\n","from torchvision import transforms\n","import os\n","import re\n","import sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:29.263810Z","iopub.status.busy":"2024-10-09T05:39:29.263155Z","iopub.status.idle":"2024-10-09T05:39:43.651056Z","shell.execute_reply":"2024-10-09T05:39:43.649999Z","shell.execute_reply.started":"2024-10-09T05:39:29.263770Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from transformers import AutoFeatureExtractor,CLIPModel, CLIPProcessor, AutoTokenizer, AutoModel,RobertaModel, AutoImageProcessor\n","from transformers import CLIPVisionModel, CLIPVisionConfig\n","\n","import torch\n","from torchvision.io import read_image\n","import open_clip"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:43.653832Z","iopub.status.busy":"2024-10-09T05:39:43.653464Z","iopub.status.idle":"2024-10-09T05:39:43.665848Z","shell.execute_reply":"2024-10-09T05:39:43.664756Z","shell.execute_reply.started":"2024-10-09T05:39:43.653793Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["sss = 18 + 19\n","torch.manual_seed(sss)\n","np.random.seed(sss)"]},{"cell_type":"markdown","metadata":{},"source":["# TEXT PRE-PROCESSING"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:43.668308Z","iopub.status.busy":"2024-10-09T05:39:43.667692Z","iopub.status.idle":"2024-10-09T05:39:43.694870Z","shell.execute_reply":"2024-10-09T05:39:43.693691Z","shell.execute_reply.started":"2024-10-09T05:39:43.668249Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def convert_unicode(text):\n","  char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n","  charutf8 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n","  char1252 = char1252.split('|')\n","  charutf8 = charutf8.split('|')\n","\n","  dic = {}\n","  for i in range(len(char1252)): dic[char1252[i]] = charutf8[i]\n","  return re.sub(\n","      r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n","      lambda x: dic[x.group()], text\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:43.696993Z","iopub.status.busy":"2024-10-09T05:39:43.696596Z","iopub.status.idle":"2024-10-09T05:39:43.733181Z","shell.execute_reply":"2024-10-09T05:39:43.732126Z","shell.execute_reply.started":"2024-10-09T05:39:43.696956Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import unicodedata\n","\n","class TextNormalize:\n","    def __init__(self):\n","        self.vowels_to_ids = {}\n","        self.vowels_table = [\n","            ['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a' ],\n","            ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n","            ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n","            ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e' ],\n","            ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n","            ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i' ],\n","            ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o' ],\n","            ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'o'],\n","            ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n","            ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u' ],\n","            ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n","            ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y' ]\n","        ]\n","        pass\n","\n","    def createVowelsTable(self):\n","        \"\"\"Create Vowels Table\"\"\"\n","        for i in range(len(self.vowels_table)):\n","            for j in range(len(self.vowels_table[i]) - 1):\n","                self.vowels_to_ids[self.vowels_table[i][j]] = (i, j)\n","\n","    def IsValidVietnameseWord(self,word):\n","        \"\"\"Nguyên âm chỉ có thể đứng chung với nguyên âm. Một từ không thể có 2 nguyên âm cách nhau bởi 1 phụ âm\"\"\"\n","        chars = list(word)\n","        #nguyen am\n","        vowel_index = -1\n","        for i in range(len(chars)):\n","            idx_vowel_table = self.vowels_to_ids.get(chars[i],(-1,-1))[0]\n","            if idx_vowel_table != -1:\n","                if vowel_index == -1:\n","                    vowel_index = i\n","                else:\n","                    if i - vowel_index != 1:\n","                        return False\n","                    vowel_index = i\n","        return True\n","\n","    def WordStandardized(self,word):\n","        \"\"\"Standardize Word\"\"\"\n","        if not self.IsValidVietnameseWord(word):\n","            return word\n","\n","        chars = list(word)\n","        vowel_indexes = []\n","\n","        # tìm vị trí nguyên âm\n","        qu_or_gi = False\n","        thanh_dieu = 0\n","        for i in range(len(chars)):\n","            vowel_table_row, vowel_table_col = self.vowels_to_ids.get(chars[i],(-1,-1))\n","            if vowel_table_row == -1 :\n","                continue\n","            # qu\n","            if vowel_table_row == 9:\n","                if i != 0 and chars[i-1] == 'q':\n","                    chars[i] = 'u'\n","                    qu_or_gi = True\n","            # gi\n","            elif vowel_table_row == 5:\n","                if i != 0 and chars[i-1] == 'g':\n","                    chars[i] = 'i'\n","                    qu_or_gi = True\n","\n","            # có chứa thanh điệu\n","            if vowel_table_col != 0:\n","                thanh_dieu = vowel_table_col\n","                chars[i] = self.vowels_table[vowel_table_row][0]\n","\n","            vowel_indexes.append(i)\n","        # 1 nguyên âm\n","        if len(vowel_indexes) == 1:\n","            c = chars[vowel_indexes[0]]\n","            chars[vowel_indexes[0]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n","            return ''.join(chars)\n","\n","        for idx_vowel in vowel_indexes:\n","            vowel_table_row, vowel_table_col = self.vowels_to_ids.get(chars[idx_vowel],(-1,-1))\n","            #ê, ơ, ô\n","            if vowel_table_row == 4 or vowel_table_row == 7 or vowel_table_row == 8:\n","                c = chars[idx_vowel]\n","                chars[idx_vowel] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n","                return ''.join(chars)\n","\n","            # kiểm tra qu và gi, 2-3 nguyên âm thì nguyên âm thứ 2 chứa dấu\n","            if qu_or_gi:\n","                if len(vowel_indexes) == 2 or len(vowel_indexes) == 3:\n","                    c = chars[vowel_indexes[1]]\n","                    chars[vowel_indexes[1]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n","                return ''.join(chars)\n","\n","            # 2 nguyên âm\n","            if len(vowel_indexes) == 2:\n","                # âm cuối là nguyên âm\n","                if vowel_indexes[-1] == len(chars) - 1:\n","                    c = chars[vowel_indexes[0]]\n","                    chars[vowel_indexes[0]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n","                else:\n","                    c = chars[vowel_indexes[-1]]\n","                    chars[vowel_indexes[-1]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n","                return ''.join(chars)\n","\n","            elif len(vowel_indexes) == 3:\n","                # âm cuối là nguyên âm\n","                if vowel_indexes[-1] == len(chars) - 1:\n","                    c = chars[vowel_indexes[1]]\n","                    chars[vowel_indexes[1]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n","                else:\n","                    c = chars[vowel_indexes[-1]]\n","                    chars[vowel_indexes[-1]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n","                return ''.join(chars)\n","\n","        return ''.join(chars)\n","\n","    def normalize(self,text):\n","\n","\n","        #Chuyen sang viet thuong\n","        text = text.lower()\n","        text = unicodedata.normalize('NFC', text)\n","\n","        # Rút gọn từ kéo dài\n","        text = re.sub(r'(\\W)\\1+',r'\\1',text)\n","\n","        # xóa các emoji dư thừa\n","#         emoji_pattern = re.compile(\n","#             \"[\"\n","#             \"\\U0001F600-\\U0001F64F\"  # Emoticons\n","#             \"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n","#             \"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n","#             \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n","#             \"\\U00002500-\\U00002BEF\"  # Chinese char\n","#             \"\\U00002702-\\U000027B0\"\n","#             \"\\U00002702-\\U000027B0\"\n","#             \"\\U000024C2-\\U0001F251\"\n","#             \"\\U0001f926-\\U0001f937\"\n","#             \"\\U00010000-\\U0010ffff\"\n","#             \"]+\",\n","#             flags=re.UNICODE,\n","#         )\n","#         text = emoji_pattern.sub(r'',text) # no emoji\n","        # remove hastag\n","        text = re.sub(\"(@[A-Za-z0-9]+)|(#[0-9A-Za-z]+)\",\"\", text)\n","\n","        # xóa space d\n","        text = re.sub(r\"( )\\1+\",r'\\1',text)\n","#         text = re.sub(r\"[:)^@!`~%;?(\\+\\-\\'\\\"]+\",r'',text)\n","        text = text.replace(\"“\",\"\")\n","        text = text.replace(\"\\n\", \" \")\n","        text = \" \".join(text.split())\n","        return text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:43.734988Z","iopub.status.busy":"2024-10-09T05:39:43.734572Z","iopub.status.idle":"2024-10-09T05:39:44.873449Z","shell.execute_reply":"2024-10-09T05:39:44.872294Z","shell.execute_reply.started":"2024-10-09T05:39:43.734950Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:44.875438Z","iopub.status.busy":"2024-10-09T05:39:44.875089Z","iopub.status.idle":"2024-10-09T05:39:45.151732Z","shell.execute_reply":"2024-10-09T05:39:45.150817Z","shell.execute_reply.started":"2024-10-09T05:39:44.875397Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split \n","\n","data = pd.read_json('/kaggle/input/train-dsc2024/vimmsd-train.json',orient = 'index').reset_index()\n","# data = data[data['label'] != 'not-sarcasm']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:45.153523Z","iopub.status.busy":"2024-10-09T05:39:45.153107Z","iopub.status.idle":"2024-10-09T05:39:45.190456Z","shell.execute_reply":"2024-10-09T05:39:45.189334Z","shell.execute_reply.started":"2024-10-09T05:39:45.153475Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["train_data, dev_data = train_test_split(data, test_size=0.1, stratify=data['label'], random_state = sss) \n","train_data = train_data.reset_index() \n","dev_data = dev_data.reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:45.195468Z","iopub.status.busy":"2024-10-09T05:39:45.195067Z","iopub.status.idle":"2024-10-09T05:39:45.200705Z","shell.execute_reply":"2024-10-09T05:39:45.199503Z","shell.execute_reply.started":"2024-10-09T05:39:45.195426Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["normalize_class = TextNormalize()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:45.202669Z","iopub.status.busy":"2024-10-09T05:39:45.202218Z","iopub.status.idle":"2024-10-09T05:39:54.818987Z","shell.execute_reply":"2024-10-09T05:39:54.817855Z","shell.execute_reply.started":"2024-10-09T05:39:45.202590Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["train_data['caption'] = train_data['caption'].apply(lambda x: text_normalize(normalize_class.normalize(x)))\n","dev_data['caption'] = dev_data['caption'].apply(lambda x: text_normalize(normalize_class.normalize(x)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:54.820856Z","iopub.status.busy":"2024-10-09T05:39:54.820471Z","iopub.status.idle":"2024-10-09T05:39:54.843998Z","shell.execute_reply":"2024-10-09T05:39:54.842746Z","shell.execute_reply.started":"2024-10-09T05:39:54.820817Z"},"trusted":true},"outputs":[],"source":["train_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:54.845765Z","iopub.status.busy":"2024-10-09T05:39:54.845347Z","iopub.status.idle":"2024-10-09T05:39:54.856465Z","shell.execute_reply":"2024-10-09T05:39:54.855400Z","shell.execute_reply.started":"2024-10-09T05:39:54.845727Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class ViMMDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, tokenizer, img_folder, vision_processor, max_len):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.img_folder = img_folder\n","        self.vision_processor = vision_processor\n","        self.labels = ['not-sarcasm', 'image-sarcasm','text-sarcasm','multi-sarcasm']\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","    \n","    def __getitem__(self, idx):\n","        text = self.data.loc[idx,'caption']\n","        img_name = self.data.loc[idx,'image']\n","        img_file = os.path.join(self.img_folder,img_name)\n","        label = self.data.loc[idx,'label']\n","        label = self.labels.index(label)\n","        \n","        tokenizer_seq_dict = self.tokenizer(\"<image>\", text, max_length=self.max_len,truncation=True,padding='max_length', return_length=True, return_tensors='pt')\n","        input_ids = tokenizer_seq_dict['input_ids'][0]\n","        attention_mask = tokenizer_seq_dict['attention_mask']\n","\n","        # feature_file = os.path.join('dino_large_features/dino_large_features/' + img_name.split('.')[0] + '.npy')\n","        # features = np.load(feature_file, allow_pickle=True)[()]\n","\n","        # pixel_values = features['pixel_values']\n","\n","        image = Image.open(img_file)        \n","        pixel_values = self.vision_processor(image).unsqueeze(0).unsqueeze(0)\n","\n","        return input_ids, attention_mask, pixel_values, torch.tensor(label)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:54.858009Z","iopub.status.busy":"2024-10-09T05:39:54.857671Z","iopub.status.idle":"2024-10-09T05:39:54.870689Z","shell.execute_reply":"2024-10-09T05:39:54.869620Z","shell.execute_reply.started":"2024-10-09T05:39:54.857974Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["PRETRAINED_PATH = \"xlm-roberta-base\"\n","VISION_PRETRAINED_PATH = 'facebook/dinov2-large'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:54.872493Z","iopub.status.busy":"2024-10-09T05:39:54.872076Z","iopub.status.idle":"2024-10-09T05:39:55.068249Z","shell.execute_reply":"2024-10-09T05:39:55.067100Z","shell.execute_reply.started":"2024-10-09T05:39:54.872444Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["vision_processor = AutoImageProcessor.from_pretrained(VISION_PRETRAINED_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:39:55.070062Z","iopub.status.busy":"2024-10-09T05:39:55.069696Z","iopub.status.idle":"2024-10-09T05:40:14.538969Z","shell.execute_reply":"2024-10-09T05:40:14.537728Z","shell.execute_reply.started":"2024-10-09T05:39:55.070024Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["_, _, vision_processor = open_clip.create_model_and_transforms(\n","    'ViT-B-16',\n","    pretrained='openai'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:40:14.540808Z","iopub.status.busy":"2024-10-09T05:40:14.540417Z","iopub.status.idle":"2024-10-09T05:40:17.172297Z","shell.execute_reply":"2024-10-09T05:40:17.171224Z","shell.execute_reply.started":"2024-10-09T05:40:14.540769Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_PATH)\n","tokenizer.add_special_tokens(\n","        {\"additional_special_tokens\": [\"<image>\"]}\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:40:17.174861Z","iopub.status.busy":"2024-10-09T05:40:17.173929Z","iopub.status.idle":"2024-10-09T05:40:17.180744Z","shell.execute_reply":"2024-10-09T05:40:17.179385Z","shell.execute_reply.started":"2024-10-09T05:40:17.174806Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["train_set = ViMMDataset(train_data, tokenizer, '/kaggle/input/train-dsc2024/training-images/train-images', vision_processor, max_len = 200)\n","dev_set = ViMMDataset(dev_data, tokenizer, '/kaggle/input/train-dsc2024/training-images/train-images', vision_processor, max_len = 200)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:40:17.182345Z","iopub.status.busy":"2024-10-09T05:40:17.182042Z","iopub.status.idle":"2024-10-09T05:40:17.198659Z","shell.execute_reply":"2024-10-09T05:40:17.197682Z","shell.execute_reply.started":"2024-10-09T05:40:17.182304Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# for i in range(len(train_set)):\n","#     try:\n","#         t = train_set[i]\n","#     except:\n","#         print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:40:17.200626Z","iopub.status.busy":"2024-10-09T05:40:17.200177Z","iopub.status.idle":"2024-10-09T05:40:17.281390Z","shell.execute_reply":"2024-10-09T05:40:17.280259Z","shell.execute_reply.started":"2024-10-09T05:40:17.200585Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["aa = 0\n","for i in train_set[aa][0]:\n","    print(tokenizer.convert_ids_to_tokens(i.item()),end=' ' )\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:40:42.491220Z","iopub.status.busy":"2024-10-09T05:40:42.490816Z","iopub.status.idle":"2024-10-09T05:44:25.602808Z","shell.execute_reply":"2024-10-09T05:44:25.601559Z","shell.execute_reply.started":"2024-10-09T05:40:42.491182Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","y = []\n","for i in tqdm(range(len(train_set))):\n","  y.append(train_set[i][-1].item())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.605705Z","iopub.status.busy":"2024-10-09T05:44:25.605211Z","iopub.status.idle":"2024-10-09T05:44:25.613959Z","shell.execute_reply":"2024-10-09T05:44:25.612815Z","shell.execute_reply.started":"2024-10-09T05:44:25.605650Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["y[:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.616568Z","iopub.status.busy":"2024-10-09T05:44:25.615615Z","iopub.status.idle":"2024-10-09T05:44:25.630860Z","shell.execute_reply":"2024-10-09T05:44:25.629703Z","shell.execute_reply.started":"2024-10-09T05:44:25.616517Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import numpy as np\n","from torch.utils.data.sampler import WeightedRandomSampler\n","\n","counts = np.bincount(y)\n","labels_weights = 1. / counts\n","# labels_weights[-1] = labels_weights[-1]\n","weights = labels_weights[y]\n","print(counts,labels_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.634766Z","iopub.status.busy":"2024-10-09T05:44:25.634025Z","iopub.status.idle":"2024-10-09T05:44:25.644815Z","shell.execute_reply":"2024-10-09T05:44:25.643644Z","shell.execute_reply.started":"2024-10-09T05:44:25.634706Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(train_set, batch_size = 4, shuffle = False)\n","dev_loader = torch.utils.data.DataLoader(dev_set, batch_size = 4)"]},{"cell_type":"markdown","metadata":{},"source":["# MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.647002Z","iopub.status.busy":"2024-10-09T05:44:25.646536Z","iopub.status.idle":"2024-10-09T05:44:25.657554Z","shell.execute_reply":"2024-10-09T05:44:25.656461Z","shell.execute_reply.started":"2024-10-09T05:44:25.646964Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import copy\n","import numpy as np\n","import math\n","import torch.nn.functional as F\n","from transformers import AutoModel\n","from torch.autograd import Variable\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.659919Z","iopub.status.busy":"2024-10-09T05:44:25.658949Z","iopub.status.idle":"2024-10-09T05:44:25.669388Z","shell.execute_reply":"2024-10-09T05:44:25.668177Z","shell.execute_reply.started":"2024-10-09T05:44:25.659874Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import torch\n","from einops import rearrange, repeat\n","from einops_exts import rearrange_many\n","from torch import einsum, nn"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.671405Z","iopub.status.busy":"2024-10-09T05:44:25.671021Z","iopub.status.idle":"2024-10-09T05:44:25.683221Z","shell.execute_reply":"2024-10-09T05:44:25.681683Z","shell.execute_reply.started":"2024-10-09T05:44:25.671350Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import os\n","mode = 'FP16'\n","os.environ[\"TORCH_CUDNN_V8_API_ENABLED\"] = \"1\"\n","torch.backends.cuda.matmul.allow_tf32 = True if mode == 'TF32' else False\n","scaler = torch.cuda.amp.GradScaler(enabled=True if mode == 'FP16' else False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.685348Z","iopub.status.busy":"2024-10-09T05:44:25.684909Z","iopub.status.idle":"2024-10-09T05:44:25.699797Z","shell.execute_reply":"2024-10-09T05:44:25.698742Z","shell.execute_reply.started":"2024-10-09T05:44:25.685297Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def exists(val):\n","    return val is not None\n","\n","def FeedForward(dim, mult=4):\n","    inner_dim = int(dim * mult)\n","    return nn.Sequential(\n","        nn.LayerNorm(dim),\n","        nn.Linear(dim, inner_dim, bias=False),\n","        nn.GELU(),\n","        nn.Linear(inner_dim, dim, bias=False),\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.701729Z","iopub.status.busy":"2024-10-09T05:44:25.701322Z","iopub.status.idle":"2024-10-09T05:44:25.712492Z","shell.execute_reply":"2024-10-09T05:44:25.711466Z","shell.execute_reply.started":"2024-10-09T05:44:25.701689Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.717805Z","iopub.status.busy":"2024-10-09T05:44:25.716821Z","iopub.status.idle":"2024-10-09T05:44:25.732540Z","shell.execute_reply":"2024-10-09T05:44:25.731376Z","shell.execute_reply.started":"2024-10-09T05:44:25.717759Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class PerceiverAttention(nn.Module):\n","    def __init__(self, *, dim, dim_head=64, heads=8):\n","        super().__init__()\n","        self.scale = dim_head**-0.5\n","        self.heads = heads\n","        inner_dim = dim_head * heads\n","\n","        self.norm_media = nn.LayerNorm(dim)\n","        self.norm_latents = nn.LayerNorm(dim)\n","\n","        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n","        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n","        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n","\n","    def forward(self, x, latents):\n","        \"\"\"\n","        Args:\n","            x (torch.Tensor): image features\n","                shape (b, T, n1, D)\n","            latent (torch.Tensor): latent features\n","                shape (b, T, n2, D)\n","        \"\"\"\n","        x = self.norm_media(x)\n","        latents = self.norm_latents(latents)\n","\n","        h = self.heads\n","\n","        q = self.to_q(latents)\n","        kv_input = torch.cat((x, latents), dim=-2)\n","        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n","        q, k, v = rearrange_many((q, k, v), \"b t n (h d) -> b h t n d\", h=h)\n","        q = q * self.scale\n","\n","        # attention\n","        sim = einsum(\"... i d, ... j d  -> ... i j\", q, k)\n","        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n","        attn = sim.softmax(dim=-1)\n","\n","        out = einsum(\"... i j, ... j d -> ... i d\", attn, v)\n","        out = rearrange(out, \"b h t n d -> b t n (h d)\", h=h)\n","        return self.to_out(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.735017Z","iopub.status.busy":"2024-10-09T05:44:25.734465Z","iopub.status.idle":"2024-10-09T05:44:25.748673Z","shell.execute_reply":"2024-10-09T05:44:25.747582Z","shell.execute_reply.started":"2024-10-09T05:44:25.734946Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class PerceiverResampler(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        dim,\n","        depth=6,\n","        dim_head=64,\n","        heads=8,\n","        num_latents=64,\n","        max_num_media=None,\n","        max_num_frames=None,\n","        ff_mult=4,\n","    ):\n","        super().__init__()\n","        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n","        self.frame_embs = (\n","            nn.Parameter(torch.randn(max_num_frames, dim))\n","            if exists(max_num_frames)\n","            else None\n","        )\n","        self.media_time_embs = (\n","            nn.Parameter(torch.randn(max_num_media, 1, dim))\n","            if exists(max_num_media)\n","            else None\n","        )\n","\n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(\n","                nn.ModuleList(\n","                    [\n","                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n","                        FeedForward(dim=dim, mult=ff_mult),\n","                    ]\n","                )\n","            )\n","\n","        self.norm = nn.LayerNorm(dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (torch.Tensor): image features\n","                shape (b, T, F, v, D)\n","        Returns:\n","            shape (b, T, n, D) where n is self.num_latents\n","        \"\"\"\n","        b, T, F, v = x.shape[:4]\n","\n","        # frame and media time embeddings\n","        if exists(self.frame_embs):\n","            frame_embs = repeat(self.frame_embs[:F], \"F d -> b T F v d\", b=b, T=T, v=v)\n","            x = x + frame_embs\n","        x = rearrange(\n","            x, \"b T F v d -> b T (F v) d\"\n","        )  # flatten the frame and spatial dimensions\n","        if exists(self.media_time_embs):\n","            x = x + self.media_time_embs[:T]\n","\n","        # blocks\n","        latents = repeat(self.latents, \"n d -> b T n d\", b=b, T=T)\n","        for attn, ff in self.layers:\n","            latents = attn(x, latents) + latents\n","            latents = ff(latents) + latents\n","        return self.norm(latents)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.750876Z","iopub.status.busy":"2024-10-09T05:44:25.750180Z","iopub.status.idle":"2024-10-09T05:44:25.772186Z","shell.execute_reply":"2024-10-09T05:44:25.770941Z","shell.execute_reply.started":"2024-10-09T05:44:25.750812Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# gated cross attention\n","class MaskedCrossAttention(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        dim,\n","        dim_visual,\n","        dim_head=64,\n","        heads=8,\n","        only_attend_immediate_media=True,\n","    ):\n","        super().__init__()\n","        self.scale = dim_head**-0.5\n","        self.heads = heads\n","        inner_dim = dim_head * heads\n","\n","        self.norm = nn.LayerNorm(dim)\n","\n","        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n","        self.to_kv = nn.Linear(dim_visual, inner_dim * 2, bias=False)\n","        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n","\n","        # whether for text to only attend to immediate preceding image, or all previous images\n","        self.only_attend_immediate_media = only_attend_immediate_media\n","\n","    def forward(self, x, media, media_locations=None, use_cached_media=False):\n","        \"\"\"\n","        Args:\n","            x (torch.Tensor): text features\n","                shape (B, T_txt, D_txt)\n","            media (torch.Tensor): image features\n","                shape (B, T_img, n, D_img) where n is the dim of the latents\n","            media_locations: boolean mask identifying the media tokens in x\n","                shape (B, T_txt)\n","            use_cached_media: bool\n","                If true, treat all of x as if they occur after the last media\n","                registered in media_locations. T_txt does not need to exactly\n","                equal media_locations.shape[1] in this case\n","        \"\"\"\n","\n","        if not use_cached_media:\n","            assert (\n","                media_locations.shape[1] == x.shape[1]\n","            ), f\"media_location.shape is {media_locations.shape} but x.shape is {x.shape}\"\n","\n","        T_txt = x.shape[1]\n","        _, T_img, n = media.shape[:3]\n","        h = self.heads\n","\n","        x = self.norm(x)\n","\n","        q = self.to_q(x)\n","        media = rearrange(media, \"b t n d -> b (t n) d\")\n","\n","        k, v = self.to_kv(media).chunk(2, dim=-1)\n","        q, k, v = rearrange_many((q, k, v), \"b n (h d) -> b h n d\", h=h)\n","\n","        q = q * self.scale\n","\n","        sim = einsum(\"... i d, ... j d -> ... i j\", q, k)\n","\n","        if exists(media_locations):\n","            media_time = torch.arange(T_img, device=x.device) + 1\n","\n","            if use_cached_media:\n","                # text time is set to the last cached media location\n","                text_time = repeat(\n","                    torch.count_nonzero(media_locations, dim=1),\n","                    \"b -> b i\",\n","                    i=T_txt,\n","                )\n","            else:\n","                # at each boolean of True, increment the time counter (relative to media time)\n","                text_time = media_locations.cumsum(dim=-1)\n","\n","            # text time must equal media time if only attending to most immediate image\n","            # otherwise, as long as text time is greater than media time (if attending to all previous images / media)\n","            mask_op = torch.eq if self.only_attend_immediate_media else torch.ge\n","\n","            text_to_media_mask = mask_op(\n","                rearrange(text_time, \"b i -> b 1 i 1\"),\n","                repeat(media_time, \"j -> 1 1 1 (j n)\", n=n),\n","            )\n","            sim = sim.masked_fill(~text_to_media_mask, -torch.finfo(sim.dtype).max)\n","\n","        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n","        attn = sim.softmax(dim=-1)\n","\n","        if exists(media_locations) and self.only_attend_immediate_media:\n","            # any text without a preceding media needs to have attention zeroed out\n","            text_without_media_mask = text_time == 0\n","            text_without_media_mask = rearrange(\n","                text_without_media_mask, \"b i -> b 1 i 1\"\n","            )\n","            attn = attn.masked_fill(text_without_media_mask, 0.0)\n","\n","        out = einsum(\"... i j, ... j d -> ... i d\", attn, v)\n","        out = rearrange(out, \"b h n d -> b n (h d)\")\n","        return self.to_out(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.774226Z","iopub.status.busy":"2024-10-09T05:44:25.773852Z","iopub.status.idle":"2024-10-09T05:44:25.790681Z","shell.execute_reply":"2024-10-09T05:44:25.789425Z","shell.execute_reply.started":"2024-10-09T05:44:25.774188Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class GatedCrossAttentionBlock(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        dim,\n","        dim_visual,\n","        dim_head=64,\n","        heads=8,\n","        ff_mult=4,\n","        only_attend_immediate_media=True,\n","    ):\n","        super().__init__()\n","        self.attn = MaskedCrossAttention(\n","            dim=dim,\n","            dim_visual=dim_visual,\n","            dim_head=dim_head,\n","            heads=heads,\n","            only_attend_immediate_media=only_attend_immediate_media,\n","        )\n","        self.attn_gate = nn.Parameter(torch.tensor([0.0]))\n","\n","        self.ff = FeedForward(dim, mult=ff_mult)\n","        self.ff_gate = nn.Parameter(torch.tensor([0.0]))\n","\n","    def forward(\n","        self,\n","        x,\n","        media,\n","        media_locations=None,\n","        use_cached_media=False,\n","    ):\n","        x = (\n","            self.attn(\n","                x,\n","                media,\n","                media_locations=media_locations,\n","                use_cached_media=use_cached_media,\n","            )\n","            * self.attn_gate.tanh()\n","            + x\n","        )\n","        x = self.ff(x) * self.ff_gate.tanh() + x\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.792364Z","iopub.status.busy":"2024-10-09T05:44:25.791957Z","iopub.status.idle":"2024-10-09T05:44:25.806165Z","shell.execute_reply":"2024-10-09T05:44:25.805058Z","shell.execute_reply.started":"2024-10-09T05:44:25.792326Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def apply_with_stopping_condition(\n","    module, apply_fn, apply_condition=None, stopping_condition=None, **other_args\n","):\n","    if stopping_condition(module):\n","        return\n","    if apply_condition(module):\n","        apply_fn(module, **other_args)\n","    for child in module.children():\n","        apply_with_stopping_condition(\n","            child,\n","            apply_fn,\n","            apply_condition=apply_condition,\n","            stopping_condition=stopping_condition,\n","            **other_args\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.807796Z","iopub.status.busy":"2024-10-09T05:44:25.807411Z","iopub.status.idle":"2024-10-09T05:44:25.819742Z","shell.execute_reply":"2024-10-09T05:44:25.818617Z","shell.execute_reply.started":"2024-10-09T05:44:25.807758Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def extend_instance(obj, mixin):\n","    \"\"\"Apply mixins to a class instance after creation\"\"\"\n","    base_cls = obj.__class__\n","    base_cls_name = obj.__class__.__name__\n","    obj.__class__ = type(\n","        base_cls_name, (mixin, base_cls), {}\n","    )  # mixin needs to go first for our forward() logic to work\n","\n","\n","def getattr_recursive(obj, att):\n","    \"\"\"\n","    Return nested attribute of obj\n","    Example: getattr_recursive(obj, 'a.b.c') is equivalent to obj.a.b.c\n","    \"\"\"\n","    if att == \"\":\n","        return obj\n","    i = att.find(\".\")\n","    if i < 0:\n","        return getattr(obj, att)\n","    else:\n","        return getattr_recursive(getattr(obj, att[:i]), att[i + 1 :])\n","\n","\n","def setattr_recursive(obj, att, val):\n","    \"\"\"\n","    Set nested attribute of obj\n","    Example: setattr_recursive(obj, 'a.b.c', val) is equivalent to obj.a.b.c = val\n","    \"\"\"\n","    if \".\" in att:\n","        obj = getattr_recursive(obj, \".\".join(att.split(\".\")[:-1]))\n","    setattr(obj, att.split(\".\")[-1], val)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.821334Z","iopub.status.busy":"2024-10-09T05:44:25.820971Z","iopub.status.idle":"2024-10-09T05:44:25.838405Z","shell.execute_reply":"2024-10-09T05:44:25.837192Z","shell.execute_reply.started":"2024-10-09T05:44:25.821278Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class FlamingoLayer(nn.Module):\n","    \"\"\"\n","    FlamingoLayer is a wrapper around the GatedCrossAttentionBlock and DecoderLayer.\n","    \"\"\"\n","\n","    def __init__(\n","        self, gated_cross_attn_layer, encoder_layer, gradient_checkpointing=False\n","    ):\n","        super().__init__()\n","        self.gated_cross_attn_layer = gated_cross_attn_layer\n","        self.vis_x = None\n","        self.encoder_layer = encoder_layer\n","        self.media_locations = None\n","        if self.gated_cross_attn_layer is not None:\n","            self.gated_cross_attn_layer._use_gradient_checkpointing = (\n","                gradient_checkpointing\n","            )\n","\n","    def is_conditioned(self) -> bool:\n","        \"\"\"Check whether the layer is conditioned.\"\"\"\n","        return self.vis_x is not None and self.media_locations is not None\n","\n","    # Used this great idea from this implementation of Flamingo (https://github.com/dhansmair/flamingo-mini/)\n","    def condition_vis_x(self, vis_x):\n","        self.vis_x = vis_x\n","\n","    def condition_media_locations(self, media_locations):\n","        self.media_locations = media_locations\n","\n","    def condition_use_cached_media(self, use_cached_media):\n","        self.use_cached_media = use_cached_media\n","\n","    def forward(\n","        self,\n","        lang_x, \n","        attention_mask,\n","        *args,\n","        **encoder_layer_kwargs\n","    ):\n","        # Cross attention\n","        if self.gated_cross_attn_layer is not None:\n","            if self.vis_x is None:\n","                raise ValueError(\"vis_x must be conditioned before forward pass\")\n","\n","            if self.media_locations is None:\n","                raise ValueError(\n","                    \"media_locations must be conditioned before forward pass\"\n","                )\n","\n","            lang_x = self.gated_cross_attn_layer(\n","                lang_x,\n","                self.vis_x,\n","                media_locations=self.media_locations,\n","                use_cached_media=self.use_cached_media,\n","            )\n","\n","        # Normal decoder layer\n","        lang_x = self.encoder_layer(\n","            lang_x, attention_mask, *args\n","        )\n","        return lang_x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.840241Z","iopub.status.busy":"2024-10-09T05:44:25.839867Z","iopub.status.idle":"2024-10-09T05:44:25.859304Z","shell.execute_reply":"2024-10-09T05:44:25.858077Z","shell.execute_reply.started":"2024-10-09T05:44:25.840185Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from transformers import AutoFeatureExtractor,CLIPModel, CLIPProcessor, AutoTokenizer, AutoModel,RobertaModel\n","\n","class FlamingoLMMixin(nn.Module):\n","    \"\"\"\n","    Mixin to add cross-attention layers to a language model.\n","    \"\"\"\n","\n","    def set_encoder_layers_attr_name(self, encoder_layers_attr_name):\n","        self.encoder_layers_attr_name = encoder_layers_attr_name\n","\n","    def _get_encoder_layers(self):\n","        return getattr_recursive(self, self.encoder_layers_attr_name)\n","\n","    def _set_encoder_layers(self, value):\n","        setattr_recursive(self, self.encoder_layers_attr_name, value)\n","\n","    def init_flamingo(\n","        self,\n","        media_token_id,\n","        lang_hidden_size,\n","        vis_hidden_size,\n","        cross_attn_every_n_layers,\n","        gradient_checkpointing,\n","    ):\n","        \"\"\"\n","        Initialize Flamingo by adding a new gated cross attn to the decoder. Store the media token id for computing the media locations.\n","        \"\"\"\n","        self.old_encoder_blocks = self._get_encoder_layers()\n","        self.gated_cross_attn_layers = nn.ModuleList(\n","            [\n","                GatedCrossAttentionBlock(\n","                    dim=lang_hidden_size, dim_visual=vis_hidden_size\n","                )\n","                if (layer_idx + 1) % cross_attn_every_n_layers == 0\n","                else None\n","                for layer_idx, _ in enumerate(self._get_encoder_layers())\n","            ]\n","        )\n","        self.init_flamingo_layers(gradient_checkpointing)\n","        self.media_token_id = media_token_id\n","        self.initialized_flamingo = True\n","        self._use_cached_vision_x = False\n","\n","    def init_flamingo_layers(self, gradient_checkpointing):\n","        \"\"\"\n","        Re initializes the FlamingoLayers.\n","        Propagates any changes made to self.gated_corss_attn_layers or self.old_encoder_blocks\n","        \"\"\"\n","        self._set_encoder_layers(\n","            nn.ModuleList(\n","                [\n","                    FlamingoLayer(\n","                        gated_cross_attn_layer, encoder_layer, gradient_checkpointing\n","                    )\n","                    for gated_cross_attn_layer, encoder_layer in zip(\n","                        self.gated_cross_attn_layers, self.old_encoder_blocks\n","                    )\n","                ]\n","            )\n","        )\n","\n","    def forward(self, input_ids,attention_mask, **kwargs):\n","\n","        \"\"\"Condition the Flamingo layers on the media locations before forward()\"\"\"\n","        if not self.initialized_flamingo:\n","            raise ValueError(\n","                \"Flamingo layers are not initialized. Please call `init_flamingo` first.\"\n","            )\n","        media_locations = input_ids == self.media_token_id\n","\n","        # if there are media already cached and we're generating and there are no media tokens in the input,\n","        # we'll assume that ALL input tokens should attend to the last previous media that is cached.\n","        # this is especially important for HF generate() compatibility, since generate() calls forward()\n","        # repeatedly one token at a time (with no media tokens).\n","        # without this check, the model would not attend to any images when generating (after the first token)\n","        use_cached_media_locations = (\n","            self._use_cached_vision_x\n","            and self.is_conditioned()\n","            and not media_locations.any()\n","        )\n","\n","        for layer in self._get_encoder_layers():\n","            if not use_cached_media_locations:\n","                layer.condition_media_locations(media_locations)\n","            layer.condition_use_cached_media(use_cached_media_locations)\n","\n","        # package arguments for the other parent's forward. since we don't know the order of the arguments,\n","        # make them all kwargs\n","\n","        kwargs[\"input_ids\"] = input_ids\n","        kwargs[\"attention_mask\"] = attention_mask\n","\n","        return super().forward(**kwargs)  # Call the other parent's forward method\n","\n","    def is_conditioned(self) -> bool:\n","        \"\"\"Check whether all decoder layers are already conditioned.\"\"\"\n","        return all(l.is_conditioned() for l in self._get_encoder_layers())\n","\n","    def clear_conditioned_layers(self):\n","        for layer in self._get_encoder_layers():\n","            layer.condition_vis_x(None)\n","            layer.condition_media_locations(None)\n","            layer.condition_use_cached_media(None)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.861604Z","iopub.status.busy":"2024-10-09T05:44:25.861100Z","iopub.status.idle":"2024-10-09T05:44:25.888994Z","shell.execute_reply":"2024-10-09T05:44:25.887795Z","shell.execute_reply.started":"2024-10-09T05:44:25.861532Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import torch\n","from einops import rearrange\n","from torch import nn\n","from transformers.modeling_outputs import CausalLMOutputWithPast\n","from torch.distributed.fsdp import (\n","    FullyShardedDataParallel as FSDP,\n",")\n","\n","class Flamingo(nn.Module):\n","    def __init__(\n","        self,\n","        vision_encoder: nn.Module,\n","        lang_encoder: nn.Module,\n","        vis_dim: int,\n","        media_token_id: int,\n","        n_classes = 4,\n","        cross_attn_every_n_layers: int = 1,\n","        gradient_checkpointing: bool = False,\n","    ):\n","        \"\"\"\n","        Args:\n","            vision_encoder (nn.Module): HF CLIPModel\n","            lang_encoder (nn.Module): HF causal language model\n","                vis_dim (int): Dimension of the visual features.\n","                Visual features are projected to match this shape along the last dimension.\n","            cross_attn_every_n_layers (int, optional): How often to apply cross attention after transformer layer. Defaults to 1.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.media_token_id = media_token_id\n","        self.vis_dim = vis_dim\n","        if hasattr(lang_encoder.config, \"d_model\"):\n","            self.lang_dim = lang_encoder.config.d_model  # mpt uses d_model\n","        else:\n","            self.lang_dim = lang_encoder.config.hidden_size\n","\n","        self.vision_encoder = vision_encoder\n","        self.perceiver = PerceiverResampler(dim=self.vis_dim)\n","        self.lang_encoder = lang_encoder\n","        self.lang_encoder.init_flamingo(\n","            media_token_id=media_token_id,\n","            lang_hidden_size=self.lang_dim,\n","            vis_hidden_size=self.vis_dim,\n","            cross_attn_every_n_layers=cross_attn_every_n_layers,\n","            gradient_checkpointing=gradient_checkpointing,\n","        )\n","        self.classifier = nn.Linear(self.lang_dim*4*2,n_classes)\n","\n","        self._use_gradient_checkpointing = gradient_checkpointing\n","        self.perceiver._use_gradient_checkpointing = gradient_checkpointing\n","\n","    def forward(\n","        self,\n","        vision_x: torch.Tensor,\n","        lang_x: torch.Tensor,\n","        attention_mask = None, \n","        clear_conditioned_layers: bool = True,\n","        ):\n","        \"\"\"\n","        Forward pass of Flamingo.\n","\n","        Args:\n","            vision_x (torch.Tensor): Vision input\n","                shape (B, T_img, F, C, H, W) with F=1\n","            lang_x (torch.Tensor): Language input ids\n","                shape (B, T_txt)\n","            attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n","            labels (torch.Tensor, optional): Labels. Defaults to None.\n","            clear_conditioned_layers: if True, clear the conditioned layers\n","                once the foward pass is completed. Set this to false if the\n","                same set of images will be reused in another subsequent\n","                forward pass.\n","            past_key_values: pre-computed values to pass to language model.\n","                See past_key_values documentation in Hugging Face\n","                CausalLM models.\n","            use_cache: whether to use cached key values. See use_cache\n","                documentation in Hugging Face CausalLM models.\n","        \"\"\"\n","        assert (\n","            self.lang_encoder.initialized_flamingo\n","        ), \"Flamingo layers are not initialized. Please call `init_flamingo` first.\"\n","\n","        assert (\n","            self.lang_encoder._use_cached_vision_x or vision_x is not None\n","        ), \"Must provide either vision_x or have precached media using cache_media().\"\n","        \n","        self.lang_encoder._use_cached_vision_x = True\n","        self._encode_vision_x(vision_x=vision_x)\n","        self._condition_media_locations(input_ids=lang_x)\n","\n","        output = self.lang_encoder(\n","            input_ids=lang_x,\n","            attention_mask = attention_mask,\n","            output_hidden_states=True\n","        )\n","        # output = output.pooler_output\n","        # output = self.classifier(output)\n","\n","        hidden_states = output.hidden_states\n","        last_hidden_state = torch.cat(hidden_states[-4:], dim=-1) # B, 1 + seq_len, H*4\n","        text_features = last_hidden_state[:,0,:] # B, H*4\n","        visual_features = last_hidden_state[:,1,:] # B, H*4\n","        cls_and_visual = torch.cat([text_features,visual_features], dim=1)\n","        output = self.classifier(cls_and_visual)\n","        \n","        self.lang_encoder.clear_conditioned_layers()\n","        # self.lang_encoder._use_cached_vision_x = False\n","\n","        return output\n","\n","        \n","    def _encode_vision_x(self, vision_x: torch.Tensor):\n","        \"\"\"\n","        Compute media tokens from vision input by passing it through vision encoder and conditioning language model.\n","        Args:\n","            vision_x (torch.Tensor): Vision input\n","                shape (B, T_img, F, C, H, W)\n","                Images in the same chunk are collated along T_img, and frames are collated along F\n","                Currently only F=1 is supported (single-frame videos)\n","\n","        rearrange code based on https://github.com/dhansmair/flamingo-mini\n","        \"\"\"\n","\n","        assert vision_x.ndim == 6, \"vision_x should be of shape (b, T_img, F, C, H, W)\"\n","        b, T, F = vision_x.shape[:3]\n","        assert F == 1, \"Only single frame supported\"\n","\n","        vision_x = rearrange(vision_x, \"b T F c h w -> (b T F) c h w\")\n","        with torch.no_grad():\n","            # vision_x = self.vision_encoder(vision_x)[1].unsqueeze(1) #check\n","            vision_x = self.vision_encoder(vision_x)[1]\n","\n","        vision_x = rearrange(vision_x, \"(b T F) v d -> b T F v d\", b=b, T=T, F=F)\n","        vision_x = self.perceiver(vision_x)\n","        # print('vision size:', vision_x.size())\n","        for layer in self.lang_encoder._get_encoder_layers():\n","            layer.condition_vis_x(vision_x)\n","        def clip_grad_norm_(max_norm):\n","            self.perceiver.clip_grad_norm_(max_norm)\n","            for layer in self.lang_encoder.gated_cross_attn_layers:\n","                if layer is not None:\n","                    layer.clip_grad_norm_(max_norm)\n","            self.lang_encoder.get_input_embeddings().clip_grad_norm_(max_norm)\n","\n","        self.clip_grad_norm_ = clip_grad_norm_\n","\n","    def _condition_media_locations(self, input_ids: torch.Tensor):\n","        \"\"\"\n","        Compute the media token locations from lang_x and condition the language model on these.\n","        Args:\n","            input_ids (torch.Tensor): Language input\n","                shape (B, T_txt)\n","        \"\"\"\n","        media_locations = input_ids == self.media_token_id\n","\n","        for layer in self.lang_encoder._get_encoder_layers():\n","            layer.condition_media_locations(media_locations)\n","\n","    def cache_media(self, input_ids: torch.Tensor, vision_x: torch.Tensor):\n","        \"\"\"\n","        Pre-cache a prompt/sequence of images / text for log-likelihood evaluations.\n","        All subsequent calls to forward() will generate attending to the LAST\n","        image in vision_x.\n","        This is not meant to be used to cache things for generate().\n","        Args:\n","            input_ids (torch.Tensor): Language input\n","                shape (B, T_txt)\n","            vision_x (torch.Tensor): Vision input\n","                shape (B, T_img, F, C, H, W)\n","                Images in the same chunk are collated along T_img, and frames are collated along F\n","                Currently only F=1 is supported (single-frame videos)\n","        \"\"\"\n","        self._encode_vision_x(vision_x=vision_x)\n","        self._condition_media_locations(input_ids=input_ids)\n","        self.lang_encoder._use_cached_vision_x = True\n","\n","    def uncache_media(self):\n","        \"\"\"\n","        Clear all conditioning.\n","        \"\"\"\n","        self.lang_encoder.clear_conditioned_layers()\n","        self.lang_encoder._use_cached_vision_x = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.890830Z","iopub.status.busy":"2024-10-09T05:44:25.890437Z","iopub.status.idle":"2024-10-09T05:44:25.904608Z","shell.execute_reply":"2024-10-09T05:44:25.903287Z","shell.execute_reply.started":"2024-10-09T05:44:25.890791Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["__KNOWN_encoder_layerS_ATTR_NAMES = {\n","    'roberta': 'encoder.layer',\n","    'MultilingualCLIP': 'transformer.encoder.layer',\n","    'ernie': 'encoder.layers'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.906759Z","iopub.status.busy":"2024-10-09T05:44:25.906297Z","iopub.status.idle":"2024-10-09T05:44:25.917415Z","shell.execute_reply":"2024-10-09T05:44:25.916304Z","shell.execute_reply.started":"2024-10-09T05:44:25.906705Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def _infer_encoder_layers_attr_name(model):\n","    for k in __KNOWN_encoder_layerS_ATTR_NAMES:\n","        if k.lower() in model.__class__.__name__.lower():\n","            return __KNOWN_encoder_layerS_ATTR_NAMES[k]\n","\n","    raise ValueError(\n","        f\"We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.\"\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.919262Z","iopub.status.busy":"2024-10-09T05:44:25.918812Z","iopub.status.idle":"2024-10-09T05:44:25.929302Z","shell.execute_reply":"2024-10-09T05:44:25.928196Z","shell.execute_reply.started":"2024-10-09T05:44:25.919211Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from transformers import AutoFeatureExtractor,CLIPModel, CLIPProcessor, AutoTokenizer, AutoModel,RobertaModel\n","from transformers import CLIPVisionModel, CLIPVisionConfig\n","\n","import torch\n","from torchvision.io import read_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:25.931066Z","iopub.status.busy":"2024-10-09T05:44:25.930469Z","iopub.status.idle":"2024-10-09T05:44:30.072054Z","shell.execute_reply":"2024-10-09T05:44:30.071035Z","shell.execute_reply.started":"2024-10-09T05:44:25.931020Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["lang_encoder = AutoModel.from_pretrained(PRETRAINED_PATH)\n","# vision_encoder = AutoModel.from_pretrained(VISION_PRETRAINED_PATH)\n","\n","temp_vision, _, _ = open_clip.create_model_and_transforms(\n","    'ViT-B-16',\n","    pretrained='openai'\n",")\n","\n","vision_encoder = temp_vision.visual\n","vision_encoder.output_tokens = True\n","vision_encoder.attn_pool = vision_encoder.proj = None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:30.073613Z","iopub.status.busy":"2024-10-09T05:44:30.073267Z","iopub.status.idle":"2024-10-09T05:44:30.082364Z","shell.execute_reply":"2024-10-09T05:44:30.081231Z","shell.execute_reply.started":"2024-10-09T05:44:30.073570Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["print(\"trainable parameter vision:\", sum(p.numel() for p in vision_encoder.parameters()))\n","print(\"trainable parameter text:\", sum(p.numel() for p in lang_encoder.parameters()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:30.084523Z","iopub.status.busy":"2024-10-09T05:44:30.084106Z","iopub.status.idle":"2024-10-09T05:44:30.094565Z","shell.execute_reply":"2024-10-09T05:44:30.093519Z","shell.execute_reply.started":"2024-10-09T05:44:30.084475Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["extend_instance(lang_encoder, FlamingoLMMixin)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:30.098096Z","iopub.status.busy":"2024-10-09T05:44:30.096001Z","iopub.status.idle":"2024-10-09T05:44:34.095274Z","shell.execute_reply":"2024-10-09T05:44:34.094057Z","shell.execute_reply.started":"2024-10-09T05:44:30.098057Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["encoder_layers_attr_name  = _infer_encoder_layers_attr_name(lang_encoder)\n","lang_encoder.set_encoder_layers_attr_name(encoder_layers_attr_name)\n","lang_encoder.resize_token_embeddings(len(tokenizer))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:34.097749Z","iopub.status.busy":"2024-10-09T05:44:34.097043Z","iopub.status.idle":"2024-10-09T05:44:34.977040Z","shell.execute_reply":"2024-10-09T05:44:34.975965Z","shell.execute_reply.started":"2024-10-09T05:44:34.097700Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["model = Flamingo(\n","    vision_encoder,\n","    lang_encoder,\n","    cross_attn_every_n_layers=1,\n","    vis_dim = 768,\n","    media_token_id = tokenizer.convert_tokens_to_ids('<image>'),\n","    gradient_checkpointing=False,\n","    n_classes = 4\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:34.978853Z","iopub.status.busy":"2024-10-09T05:44:34.978411Z","iopub.status.idle":"2024-10-09T05:44:35.769514Z","shell.execute_reply":"2024-10-09T05:44:35.768417Z","shell.execute_reply.started":"2024-10-09T05:44:34.978804Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:35.775074Z","iopub.status.busy":"2024-10-09T05:44:35.774710Z","iopub.status.idle":"2024-10-09T05:44:35.784817Z","shell.execute_reply":"2024-10-09T05:44:35.783580Z","shell.execute_reply.started":"2024-10-09T05:44:35.775037Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["print(\"trainable parameter text-vision:\", sum(p.numel() for p in model.parameters()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:44:35.786680Z","iopub.status.busy":"2024-10-09T05:44:35.786271Z","iopub.status.idle":"2024-10-09T05:44:36.930744Z","shell.execute_reply":"2024-10-09T05:44:36.929449Z","shell.execute_reply.started":"2024-10-09T05:44:35.786613Z"},"jupyter":{"source_hidden":true},"scrolled":true,"trusted":true},"outputs":[],"source":["for data in train_loader:\n","    break\n","    \n","with torch.no_grad():\n","    input_ids, attention_mask, pixel_values, label = data\n","    pixel_values = pixel_values.float()\n","    \n","    input_ids = input_ids.to(device)\n","    attention_mask = attention_mask.to(device)\n","    pixel_values = pixel_values.to(device)\n","    label = label.to(device)\n","    \n","    logits = model(\n","            vision_x =  pixel_values,\n","            lang_x =  input_ids,\n","            attention_mask = attention_mask\n","            )\n","logits"]},{"cell_type":"markdown","metadata":{},"source":["# TRAINING"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:35.789261Z","iopub.status.busy":"2024-10-09T05:45:35.788807Z","iopub.status.idle":"2024-10-09T05:45:35.803533Z","shell.execute_reply":"2024-10-09T05:45:35.802353Z","shell.execute_reply.started":"2024-10-09T05:45:35.789217Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","\n","optimizer = torch.optim.AdamW(optimizer_grouped_parameters,\n","                        lr=2e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:36.154610Z","iopub.status.busy":"2024-10-09T05:45:36.154013Z","iopub.status.idle":"2024-10-09T05:45:36.178472Z","shell.execute_reply":"2024-10-09T05:45:36.177013Z","shell.execute_reply.started":"2024-10-09T05:45:36.154537Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["'''\n","Implementation of Focal Loss with adaptive gamma.\n","Reference:\n","[1]  T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, Focal loss for dense object detection.\n","     arXiv preprint arXiv:1708.02002, 2017.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","from scipy.special import lambertw\n","import numpy as np\n","\n","\n","def get_gamma(p=0.2):\n","    '''\n","    Get the gamma for a given pt where the function g(p, gamma) = 1\n","    '''\n","    y = ((1-p)**(1-(1-p)/(p*np.log(p)))/(p*np.log(p)))*np.log(1-p)\n","    gamma_complex = (1-p)/(p*np.log(p)) + lambertw(-y + 1e-12, k=-1)/np.log(1-p)\n","    gamma = np.real(gamma_complex) #gamma for which p_t > p results in g(p_t,gamma)<1\n","    return gamma\n","\n","ps = [0.2, 0.5]\n","gammas = [5.0, 3.0]\n","i = 0\n","gamma_dic = {}\n","for p in ps:\n","    gamma_dic[p] = gammas[i]\n","    i += 1\n","\n","class FocalLossAdaptive(nn.Module):\n","    def __init__(self, gamma=0, size_average=False, device=None):\n","        super(FocalLossAdaptive, self).__init__()\n","        self.size_average = size_average\n","        self.gamma = gamma\n","        self.device = device\n","\n","    def get_gamma_list(self, pt):\n","        gamma_list = []\n","        batch_size = pt.shape[0]\n","        for i in range(batch_size):\n","            pt_sample = pt[i].item()\n","            if (pt_sample >= 0.5):\n","                gamma_list.append(self.gamma)\n","                continue\n","            # Choosing the gamma for the sample\n","            for key in sorted(gamma_dic.keys()):\n","                if pt_sample < key:\n","                    gamma_list.append(gamma_dic[key])\n","                    break\n","        return torch.tensor(gamma_list).to(self.device)\n","\n","    def forward(self, input, target):\n","        if input.dim()>2:\n","            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n","            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n","            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n","        target = target.view(-1,1)\n","        logpt = F.log_softmax(input, dim=1)\n","        logpt = logpt.gather(1,target)\n","        logpt = logpt.view(-1)\n","        pt = logpt.exp()\n","        gamma = self.get_gamma_list(pt)\n","        loss = -1 * (1-pt)**gamma * logpt\n","        if self.size_average: return loss.mean()\n","        else: return loss.sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:36.457704Z","iopub.status.busy":"2024-10-09T05:45:36.456599Z","iopub.status.idle":"2024-10-09T05:45:36.471591Z","shell.execute_reply":"2024-10-09T05:45:36.470402Z","shell.execute_reply.started":"2024-10-09T05:45:36.457641Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","class LDAMLoss(nn.Module):\n","    \n","    def __init__(self, cls_num_list, max_m=0.5, weight=None, s=30):\n","        super(LDAMLoss, self).__init__()\n","        m_list = 1.0 / np.sqrt(np.sqrt(cls_num_list))\n","        m_list = m_list * (max_m / np.max(m_list))\n","        m_list = torch.cuda.FloatTensor(m_list)\n","        self.m_list = m_list\n","        assert s > 0\n","        self.s = s\n","        self.weight = weight\n","\n","    def forward(self, x, target):\n","        index = torch.zeros_like(x, dtype=torch.uint8)\n","        index.scatter_(1, target.data.view(-1, 1), 1)\n","        \n","        index_float = index.type(torch.cuda.FloatTensor)\n","        batch_m = torch.matmul(self.m_list[None, :], index_float.transpose(0,1))\n","        batch_m = batch_m.view((-1, 1))\n","        x_m = x - batch_m\n","    \n","        output = torch.where(index, x_m, x)\n","        return F.cross_entropy(self.s*output, target, weight=self.weight)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:36.696536Z","iopub.status.busy":"2024-10-09T05:45:36.696100Z","iopub.status.idle":"2024-10-09T05:45:36.714976Z","shell.execute_reply":"2024-10-09T05:45:36.713865Z","shell.execute_reply.started":"2024-10-09T05:45:36.696493Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import torch\n","\n","# Data class frequencies\n","class_counts = torch.tensor(counts)\n","total = class_counts.sum()\n","\n","# Inverse proportional to class frequencies\n","alpha = total / class_counts\n","alpha = alpha / alpha.sum()  # Normalize\n","\n","print(alpha)  # This will give you weights to use\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:36.884220Z","iopub.status.busy":"2024-10-09T05:45:36.883786Z","iopub.status.idle":"2024-10-09T05:45:36.889980Z","shell.execute_reply":"2024-10-09T05:45:36.888477Z","shell.execute_reply.started":"2024-10-09T05:45:36.884180Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["NUM_EPOCHS = 8"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:38.144806Z","iopub.status.busy":"2024-10-09T05:45:38.144042Z","iopub.status.idle":"2024-10-09T05:45:38.151417Z","shell.execute_reply":"2024-10-09T05:45:38.149994Z","shell.execute_reply.started":"2024-10-09T05:45:38.144744Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["num_training_step = len(train_loader)*NUM_EPOCHS\n","scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=num_training_step//10, num_training_steps=num_training_step)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:38.392117Z","iopub.status.busy":"2024-10-09T05:45:38.391691Z","iopub.status.idle":"2024-10-09T05:45:38.399787Z","shell.execute_reply":"2024-10-09T05:45:38.398522Z","shell.execute_reply.started":"2024-10-09T05:45:38.392080Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["tokenizer.convert_tokens_to_ids('<image>')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:38.604578Z","iopub.status.busy":"2024-10-09T05:45:38.603579Z","iopub.status.idle":"2024-10-09T05:45:38.617950Z","shell.execute_reply":"2024-10-09T05:45:38.616819Z","shell.execute_reply.started":"2024-10-09T05:45:38.604529Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","def train_one_epoch(model, criterion_1, criterion_2, dataloader, epoch,device,scaler):\n","    epoch_loss = 0\n","\n","    model.train()\n","\n","    # batch accumulation parameter\n","    accum_iter = 1\n","\n","    with tqdm(dataloader, unit=\"batch\") as tepoch:\n","        for i, data in enumerate(tepoch):\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","\n","            input_ids, attention_mask, pixel_values, label = data\n","            pixel_values = pixel_values.float()\n","            \n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            pixel_values = pixel_values.to(device)\n","            label = label.to(device)\n","            \n","            with torch.autocast(device_type='cuda', dtype=torch.bfloat16 if mode == 'BF16' else torch.float16, enabled=True if '16' in mode else False):\n","                logits = model(\n","                    vision_x =  pixel_values,\n","                    lang_x =  input_ids,\n","                    attention_mask = attention_mask\n","                    )\n","                losses_1 = criterion_1(logits,label)\n","                losses_2 = criterion_2(logits,label)\n","                losses = (losses_1 + losses_2)/2\n","                \n","            lll = losses.item()\n","            epoch_loss += lll\n","            \n","            all_asp_loss = losses / accum_iter\n","            scaler.scale(all_asp_loss).backward()\n","            embed_grad = (\n","                    model.lang_encoder.get_input_embeddings().weight.grad\n","                        )\n","            zero_mask = torch.zeros_like(embed_grad)\n","            zero_mask[250002] = torch.ones_like(zero_mask[250002])\n","            model.lang_encoder.get_input_embeddings().weight.grad = (\n","                                embed_grad * zero_mask\n","            )\n","\n","            if ((i + 1) % accum_iter == 0) or (i + 1 == len(dataloader)):\n","                torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n","                scaler.step(optimizer)\n","                scaler.update()\n","                scheduler.step()\n","                optimizer.zero_grad()\n","\n","\n","            tepoch.set_postfix(loss=lll)\n","\n","        epoch_loss /= i\n","        print(f\"At EPOCH {epoch}, loss = {epoch_loss}\")\n","        print(\"======================================\")\n","\n","    return epoch_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:38.819686Z","iopub.status.busy":"2024-10-09T05:45:38.818601Z","iopub.status.idle":"2024-10-09T05:45:38.825919Z","shell.execute_reply":"2024-10-09T05:45:38.824658Z","shell.execute_reply.started":"2024-10-09T05:45:38.819593Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from sklearn.metrics import precision_recall_fscore_support\n","\n","def macro_f1(y_true, y_pred):\n","    p_macro, r_macro, f_macro, support_macro \\\n","      = precision_recall_fscore_support(y_true, y_pred, average=None,zero_division=0.0,labels = [0,1,2,3])\n","    return p_macro, r_macro, f_macro\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:58:44.004093Z","iopub.status.busy":"2024-10-09T05:58:44.003187Z","iopub.status.idle":"2024-10-09T05:58:44.017623Z","shell.execute_reply":"2024-10-09T05:58:44.016259Z","shell.execute_reply.started":"2024-10-09T05:58:44.004048Z"},"trusted":true},"outputs":[],"source":["llabels = ['not-sarcasm', 'image-sarcasm','text-sarcasm','multi-sarcasm']\n","def eval(model, criterion_1, criterion_2, dataloader,device):\n","\n","    print(\"EVAL STEP: \")\n","\n","    model.eval()\n","    epoch_loss = 0\n","    preds = []\n","    labels = []\n","    with tqdm(dataloader, unit=\"batch\") as tepoch:\n","        for i, data in enumerate(tepoch):\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","\n","            input_ids, attention_mask, pixel_values, label = data\n","            pixel_values = pixel_values.float()\n","\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            pixel_values = pixel_values.to(device)\n","            label = label.to(device)\n","\n","            with torch.no_grad():\n","                logits = model(\n","                    vision_x =  pixel_values,\n","                    lang_x =  input_ids,\n","                    attention_mask = attention_mask\n","                    )\n","                losses_1 = criterion_1(logits,label)\n","                losses_2 = criterion_2(logits,label)\n","                eval_losses = losses_1 + losses_2\n","\n","                pred = np.argmax(logits.cpu().detach(),axis = -1).to('cpu').numpy()\n","                label = label.to('cpu').numpy()\n","                \n","                preds.append(pred)\n","                labels.append(label)\n","\n","            epoch_loss += eval_losses\n","            \n","        epoch_loss /= i\n","        print(f\"Loss = {epoch_loss}\")\n","        \n","        preds = np.concatenate(preds)\n","        labels = np.concatenate(labels)\n","        precision, recall, f1_score = macro_f1(labels,preds)\n","        for idx, ll in enumerate(llabels):\n","            print(f\"{ll}:\")\n","            print(\"\\t- Precision:\", precision[idx])\n","            print(\"\\t- Recall:\", recall[idx])\n","            print(\"\\t- F1-score:\", f1_score[idx])\n","            print(\"========================\")\n","        print(\"===============NEXT STEP===============\")\n","\n","        return epoch_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:39.263391Z","iopub.status.busy":"2024-10-09T05:45:39.262300Z","iopub.status.idle":"2024-10-09T05:45:39.269232Z","shell.execute_reply":"2024-10-09T05:45:39.267832Z","shell.execute_reply.started":"2024-10-09T05:45:39.263322Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def save_model(path, model, epoch):\n","    torch.save({\n","        \"epoch\": epoch,\n","        \"model_state_dict\": model.state_dict(),\n","    },path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:39.507012Z","iopub.status.busy":"2024-10-09T05:45:39.506536Z","iopub.status.idle":"2024-10-09T05:45:39.513932Z","shell.execute_reply":"2024-10-09T05:45:39.512773Z","shell.execute_reply.started":"2024-10-09T05:45:39.506968Z"},"trusted":true},"outputs":[],"source":["def load_model(path):\n","    check_point = torch.load(path)\n","    return check_point"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:40.303487Z","iopub.status.busy":"2024-10-09T05:45:40.303041Z","iopub.status.idle":"2024-10-09T05:45:40.309197Z","shell.execute_reply":"2024-10-09T05:45:40.307979Z","shell.execute_reply.started":"2024-10-09T05:45:40.303445Z"},"trusted":true},"outputs":[],"source":["focal_criterion = FocalLossAdaptive(gamma = 3,size_average=True,device = device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T05:45:41.141656Z","iopub.status.busy":"2024-10-09T05:45:41.141198Z","iopub.status.idle":"2024-10-09T05:57:04.257805Z","shell.execute_reply":"2024-10-09T05:57:04.256478Z","shell.execute_reply.started":"2024-10-09T05:45:41.141593Z"},"trusted":true},"outputs":[],"source":["best_loss = float('inf')\n","count = 0\n","for epoch in range(NUM_EPOCHS):\n","    idx = epoch // 4\n","    betas = [0, 0.9999]\n","    effective_num = 1.0 - np.power(betas[idx], counts)\n","    per_cls_weights = (1.0 - betas[idx]) / np.array(effective_num)\n","    per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(counts)\n","    per_cls_weights = torch.FloatTensor(per_cls_weights).to(device)\n","\n","    ldam_criterion = LDAMLoss(counts,max_m = 0.3, s = 30, weight=per_cls_weights)\n","\n","    train_one_epoch(model, focal_criterion, ldam_criterion, train_loader, epoch, device,scaler)\n","    eval_loss = eval(model, focal_criterion, ldam_criterion,dev_loader,device)\n","    if epoch >= NUM_EPOCHS - 3:\n","        save_model(f'model_{epoch}.pth',model, epoch)\n"]},{"cell_type":"markdown","metadata":{},"source":["# MAKE RESULT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.627254Z","iopub.status.idle":"2024-10-09T05:40:35.627651Z","shell.execute_reply":"2024-10-09T05:40:35.627460Z","shell.execute_reply.started":"2024-10-09T05:40:35.627441Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["test_data = pd.read_json('kaggle/input/dev-dsc2024/vimmsd-public-test.json',orient = 'index').reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.629197Z","iopub.status.idle":"2024-10-09T05:40:35.629580Z","shell.execute_reply":"2024-10-09T05:40:35.629407Z","shell.execute_reply.started":"2024-10-09T05:40:35.629387Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["normalize_class = TextNormalize()\n","test_data['caption'] = test_data['caption'].apply(lambda x: normalize_class.normalize(text_normalize(convert_unicode(x))))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.630687Z","iopub.status.idle":"2024-10-09T05:40:35.631079Z","shell.execute_reply":"2024-10-09T05:40:35.630904Z","shell.execute_reply.started":"2024-10-09T05:40:35.630884Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_PATH)\n","tokenizer.add_special_tokens(\n","        {\"additional_special_tokens\": [\"<image>\"]}\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["# PHASE 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.633145Z","iopub.status.idle":"2024-10-09T05:40:35.633540Z","shell.execute_reply":"2024-10-09T05:40:35.633367Z","shell.execute_reply.started":"2024-10-09T05:40:35.633348Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class TestDS_1(torch.utils.data.Dataset):\n","    def __init__(self, data, tokenizer, img_folder, vision_processor, max_len):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.img_folder = img_folder\n","        self.vision_processor = vision_processor\n","        self.labels = ['not-sarcasm','sarcasm']\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","    \n","    def __getitem__(self, idx):\n","        text = self.data.loc[idx,'caption']\n","        \n","        index = self.data.loc[idx,'index']\n","        img_name = self.data.loc[idx,'image']\n","        img_file = os.path.join(self.img_folder,img_name)\n","        \n","        tokenizer_seq_dict = self.tokenizer(f\"<image> {text}\", max_length=self.max_len,truncation=True,padding='max_length', return_length=True, return_tensors='pt')\n","        input_ids = tokenizer_seq_dict['input_ids'][0]\n","        attention_mask = torch.zeros(input_ids.size())\n","\n","        image = Image.open(img_file)\n","        pixel_values = self.vision_processor(images=image, return_tensors=\"pt\")['pixel_values'].unsqueeze(1)\n","        \n","        return input_ids, attention_mask, pixel_values, index   "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.635217Z","iopub.status.idle":"2024-10-09T05:40:35.635616Z","shell.execute_reply":"2024-10-09T05:40:35.635430Z","shell.execute_reply.started":"2024-10-09T05:40:35.635411Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["vision_processor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.637462Z","iopub.status.idle":"2024-10-09T05:40:35.638009Z","shell.execute_reply":"2024-10-09T05:40:35.637760Z","shell.execute_reply.started":"2024-10-09T05:40:35.637732Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["test_set = TestDS_1(test_data, tokenizer, '/kaggle/input/dev-dsc2024/public-test-images/dev-images', vision_processor, max_len = 150)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.639365Z","iopub.status.idle":"2024-10-09T05:40:35.639938Z","shell.execute_reply":"2024-10-09T05:40:35.639686Z","shell.execute_reply.started":"2024-10-09T05:40:35.639658Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["test_loader = torch.utils.data.DataLoader(test_set, batch_size = 4, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.641074Z","iopub.status.idle":"2024-10-09T05:40:35.641599Z","shell.execute_reply":"2024-10-09T05:40:35.641346Z","shell.execute_reply.started":"2024-10-09T05:40:35.641319Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["lang_encoder = AutoModel.from_pretrained('susnato/ernie-m-base_pytorch')\n","vision_encoder = CLIPVisionModel.from_pretrained('openai/clip-vit-large-patch14')\n","\n","extend_instance(lang_encoder, FlamingoLMMixin)\n","encoder_layers_attr_name  = _infer_encoder_layers_attr_name(lang_encoder)\n","lang_encoder.set_encoder_layers_attr_name(encoder_layers_attr_name)\n","lang_encoder.resize_token_embeddings(len(tokenizer))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.643668Z","iopub.status.idle":"2024-10-09T05:40:35.644189Z","shell.execute_reply":"2024-10-09T05:40:35.643943Z","shell.execute_reply.started":"2024-10-09T05:40:35.643916Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["model_1 = Flamingo(\n","    vision_encoder,\n","    lang_encoder,\n","    cross_attn_every_n_layers=1,\n","    vis_dim = 1024,\n","    media_token_id = tokenizer.convert_tokens_to_ids('<image>'),\n","    n_classes = 1,\n","    gradient_checkpointing=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.646111Z","iopub.status.idle":"2024-10-09T05:40:35.646612Z","shell.execute_reply":"2024-10-09T05:40:35.646371Z","shell.execute_reply.started":"2024-10-09T05:40:35.646346Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["model_1.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.648267Z","iopub.status.idle":"2024-10-09T05:40:35.648812Z","shell.execute_reply":"2024-10-09T05:40:35.648546Z","shell.execute_reply.started":"2024-10-09T05:40:35.648520Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["checkpoint = load_model('phase1_7.pth')\n","model_1.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.650393Z","iopub.status.idle":"2024-10-09T05:40:35.650801Z","shell.execute_reply":"2024-10-09T05:40:35.650609Z","shell.execute_reply.started":"2024-10-09T05:40:35.650589Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["labels = test_set.labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.652621Z","iopub.status.idle":"2024-10-09T05:40:35.653023Z","shell.execute_reply":"2024-10-09T05:40:35.652854Z","shell.execute_reply.started":"2024-10-09T05:40:35.652834Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["results = {}\n","for data in tqdm(test_loader):\n","    \n","    input_ids, attention_mask, pixel_values, indexes = data\n","    pixel_values = pixel_values.float()\n","\n","    input_ids = input_ids.to(device)\n","    attention_mask = attention_mask.to(device)\n","    pixel_values = pixel_values.to(device)\n","\n","    with torch.no_grad():\n","        logits = model_1(\n","            vision_x =  pixel_values,\n","            lang_x =  input_ids,\n","            )\n","        pred = (torch.nn.functional.sigmoid(logits) >= 0.6).to(torch.int).cpu().detach().numpy().reshape(-1)\n","        \n","    for idx, pred in zip(indexes,pred):\n","        results[idx.item()] = labels[pred]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.659830Z","iopub.status.idle":"2024-10-09T05:40:35.660252Z","shell.execute_reply":"2024-10-09T05:40:35.660068Z","shell.execute_reply.started":"2024-10-09T05:40:35.660047Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.661271Z","iopub.status.idle":"2024-10-09T05:40:35.661712Z","shell.execute_reply":"2024-10-09T05:40:35.661493Z","shell.execute_reply.started":"2024-10-09T05:40:35.661472Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["final_results = {\n","    'results': results,\n","    'phase': 'dev'\n","}\n","len(final_results['results'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.664236Z","iopub.status.idle":"2024-10-09T05:40:35.664682Z","shell.execute_reply":"2024-10-09T05:40:35.664468Z","shell.execute_reply.started":"2024-10-09T05:40:35.664448Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["with open('phase_1_results.json', 'w',encoding = 'utf-8') as f:\n","    json.dump(final_results, f, ensure_ascii = False, indent = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# PHASE 2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.665960Z","iopub.status.idle":"2024-10-09T05:40:35.666356Z","shell.execute_reply":"2024-10-09T05:40:35.666180Z","shell.execute_reply.started":"2024-10-09T05:40:35.666160Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class TestDS(torch.utils.data.Dataset):\n","    def __init__(self, data, tokenizer, img_folder, vision_processor, max_len):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.img_folder = img_folder\n","        self.vision_processor = vision_processor\n","        self.labels = ['image-sarcasm','text-sarcasm','multi-sarcasm']\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","    \n","    def __getitem__(self, idx):\n","        text = self.data.loc[idx,'caption']\n","        \n","        index = self.data.loc[idx,'index']\n","        img_name = self.data.loc[idx,'image']\n","        img_file = os.path.join(self.img_folder,img_name)\n","        \n","        tokenizer_seq_dict = self.tokenizer(f\"<image> {text}\", max_length=self.max_len,truncation=True,padding='max_length', return_length=True, return_tensors='pt')\n","        input_ids = tokenizer_seq_dict['input_ids'][0]\n","        attention_mask = torch.zeros(input_ids.size())\n","\n","        image = read_image(img_file).permute(1,2,0)[:,:,:3]\n","        pixel_values = self.vision_processor(images=image, return_tensors=\"pt\")['pixel_values'].unsqueeze(1)\n","        \n","        return input_ids, attention_mask, pixel_values, index   "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.667965Z","iopub.status.idle":"2024-10-09T05:40:35.668367Z","shell.execute_reply":"2024-10-09T05:40:35.668177Z","shell.execute_reply.started":"2024-10-09T05:40:35.668153Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["test_data_2 = pd.read_json('phase_1_results.json').reset_index()\n","test_data_2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.669493Z","iopub.status.idle":"2024-10-09T05:40:35.669897Z","shell.execute_reply":"2024-10-09T05:40:35.669722Z","shell.execute_reply.started":"2024-10-09T05:40:35.669703Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["test_data_2 = test_data_2[test_data_2['results'] == 'sarcasm']\n","test_data_2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.671299Z","iopub.status.idle":"2024-10-09T05:40:35.671828Z","shell.execute_reply":"2024-10-09T05:40:35.671561Z","shell.execute_reply.started":"2024-10-09T05:40:35.671535Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["test_data_2 = test_data[test_data['index'].isin(test_data_2['index'].values.tolist())].reset_index()\n","test_data_2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.673482Z","iopub.status.idle":"2024-10-09T05:40:35.674038Z","shell.execute_reply":"2024-10-09T05:40:35.673788Z","shell.execute_reply.started":"2024-10-09T05:40:35.673761Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["test_set = TestDS(test_data_2, tokenizer, '/kaggle/input/dev-dsc2024/public-test-images/dev-images', vision_processor, max_len = 200)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.675613Z","iopub.status.idle":"2024-10-09T05:40:35.676184Z","shell.execute_reply":"2024-10-09T05:40:35.675927Z","shell.execute_reply.started":"2024-10-09T05:40:35.675898Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["test_set[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.677537Z","iopub.status.idle":"2024-10-09T05:40:35.678090Z","shell.execute_reply":"2024-10-09T05:40:35.677843Z","shell.execute_reply.started":"2024-10-09T05:40:35.677816Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["test_loader = torch.utils.data.DataLoader(test_set, batch_size = 4, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.680096Z","iopub.status.idle":"2024-10-09T05:40:35.680655Z","shell.execute_reply":"2024-10-09T05:40:35.680382Z","shell.execute_reply.started":"2024-10-09T05:40:35.680353Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["VISION_PRETRAINED_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.681897Z","iopub.status.idle":"2024-10-09T05:40:35.682314Z","shell.execute_reply":"2024-10-09T05:40:35.682131Z","shell.execute_reply.started":"2024-10-09T05:40:35.682110Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["lang_encoder = AutoModel.from_pretrained(PRETRAINED_PATH)\n","vision_encoder = AutoModel.from_pretrained(VISION_PRETRAINED_PATH)\n","\n","extend_instance(lang_encoder, FlamingoLMMixin)\n","encoder_layers_attr_name  = _infer_encoder_layers_attr_name(lang_encoder)\n","lang_encoder.set_encoder_layers_attr_name(encoder_layers_attr_name)\n","lang_encoder.resize_token_embeddings(len(tokenizer))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.684471Z","iopub.status.idle":"2024-10-09T05:40:35.685022Z","shell.execute_reply":"2024-10-09T05:40:35.684776Z","shell.execute_reply.started":"2024-10-09T05:40:35.684749Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["model = Flamingo(\n","    vision_encoder,\n","    lang_encoder,\n","    cross_attn_every_n_layers=1,\n","    vis_dim = 1024,\n","    media_token_id = tokenizer.convert_tokens_to_ids('<image>'),\n","    gradient_checkpointing=False,\n","    n_classes = 3\n",")\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.686662Z","iopub.status.idle":"2024-10-09T05:40:35.687183Z","shell.execute_reply":"2024-10-09T05:40:35.686935Z","shell.execute_reply.started":"2024-10-09T05:40:35.686908Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["checkpoint = load_model('model_4.pth')\n","model.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.689400Z","iopub.status.idle":"2024-10-09T05:40:35.689957Z","shell.execute_reply":"2024-10-09T05:40:35.689700Z","shell.execute_reply.started":"2024-10-09T05:40:35.689673Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["labels = test_set.labels\n","labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.691979Z","iopub.status.idle":"2024-10-09T05:40:35.692519Z","shell.execute_reply":"2024-10-09T05:40:35.692251Z","shell.execute_reply.started":"2024-10-09T05:40:35.692224Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["results = {}\n","for data in tqdm(test_loader):\n","    \n","    input_ids, attention_mask, pixel_values, indexes = data\n","    pixel_values = pixel_values.float()\n","\n","    input_ids = input_ids.to(device)\n","    attention_mask = attention_mask.to(device)\n","    pixel_values = pixel_values.to(device)\n","\n","    with torch.no_grad():\n","        logits = model(\n","            vision_x =  pixel_values,\n","            lang_x =  input_ids,\n","            )\n","        pred = np.argmax(logits.cpu().detach(),axis = -1).to('cpu').numpy()\n","    \n","    for idx, pred in zip(indexes,pred):\n","        results[idx.item()] = labels[pred]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.694385Z","iopub.status.idle":"2024-10-09T05:40:35.694973Z","shell.execute_reply":"2024-10-09T05:40:35.694704Z","shell.execute_reply.started":"2024-10-09T05:40:35.694674Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["final_results = {\n","    'results': results,\n","    'phase': 'dev'\n","}\n","len(final_results['results'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.697164Z","iopub.status.idle":"2024-10-09T05:40:35.697721Z","shell.execute_reply":"2024-10-09T05:40:35.697442Z","shell.execute_reply.started":"2024-10-09T05:40:35.697414Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["final_results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.698773Z","iopub.status.idle":"2024-10-09T05:40:35.699174Z","shell.execute_reply":"2024-10-09T05:40:35.698997Z","shell.execute_reply.started":"2024-10-09T05:40:35.698976Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["with open('phase_2_results.json', 'w',encoding = 'utf-8') as f:\n","    json.dump(final_results, f, ensure_ascii = False, indent = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.701440Z","iopub.status.idle":"2024-10-09T05:40:35.701892Z","shell.execute_reply":"2024-10-09T05:40:35.701708Z","shell.execute_reply.started":"2024-10-09T05:40:35.701686Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["with open('phase_1_results.json', 'r',encoding = 'utf-8') as f:\n","    phase_1_results = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.703053Z","iopub.status.idle":"2024-10-09T05:40:35.703451Z","shell.execute_reply":"2024-10-09T05:40:35.703273Z","shell.execute_reply.started":"2024-10-09T05:40:35.703253Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["with open('phase_2_results.json', 'r',encoding = 'utf-8') as f:\n","    phase_2_results = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.704883Z","iopub.status.idle":"2024-10-09T05:40:35.705280Z","shell.execute_reply":"2024-10-09T05:40:35.705104Z","shell.execute_reply.started":"2024-10-09T05:40:35.705084Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["final = phase_1_results.copy()\n","len(final['results'].items())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.708911Z","iopub.status.idle":"2024-10-09T05:40:35.709287Z","shell.execute_reply":"2024-10-09T05:40:35.709116Z","shell.execute_reply.started":"2024-10-09T05:40:35.709097Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["for k,v in phase_2_results['results'].items():\n","    final['results'][k] = v"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.711293Z","iopub.status.idle":"2024-10-09T05:40:35.711754Z","shell.execute_reply":"2024-10-09T05:40:35.711533Z","shell.execute_reply.started":"2024-10-09T05:40:35.711511Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["len(final['results'].items())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-09T05:40:35.713124Z","iopub.status.idle":"2024-10-09T05:40:35.713596Z","shell.execute_reply":"2024-10-09T05:40:35.713402Z","shell.execute_reply.started":"2024-10-09T05:40:35.713380Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["with open('results.json', 'w',encoding = 'utf-8') as f:\n","    json.dump(final, f, ensure_ascii = False, indent = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5828704,"isSourceIdPinned":true,"sourceId":9565924,"sourceType":"datasetVersion"},{"datasetId":5828698,"isSourceIdPinned":true,"sourceId":9565944,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
